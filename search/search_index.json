{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ME","text":""},{"location":"#hello-world-","title":"HELLO WORLD \ud83d\udc30","text":"<p>It's @27rabbitlt speaking, a master student at ETH Zurich, majoring at Theorectical Computer Science. Totally new to open source communities, but willing to make as much contribution as I can. Also I'll try my best to spend my whole life to become a mathematician.</p> <p>The communities I'm currently focusing on:</p> <p> </p>"},{"location":"#interests","title":"INTERESTS","text":"<p>I have a wide range of interests. Combinatorics, number theory, system &amp; architecture, cryptography, etc.. </p>"},{"location":"#languages--skills","title":"LANGUAGES &amp; SKILLS","text":""},{"location":"#rabbit-hole","title":"RABBIT HOLE","text":""},{"location":"posts/ALGO/Dilworth/","title":"Dilworth Theorem","text":"<p>In high school many OIer must have heard of a classic problem [NOIP1999 \u63d0\u9ad8\u7ec4] \u5bfc\u5f39\u62e6\u622a.</p> <p>The problem asks us to calculate the least number of disjoint monotone decreasing subsequences that form a partition of the original sequence.</p> <p>The answer is surprisingly easy: we only need to calculate the longeset monotone increasing subsequence and that's exactly the answer.</p> <p>This could be proved by Greedy but it's more straightward if we use Dilworth Theorem.</p> <p>Dilworth Theorem</p> <p>For a partial order set, the largest antichain has the same size as the smallest chain decomposition. Here antichain is a subset no two of which are comparable to each other, chain decomposition is a partition of the set by chain.</p> <p>This theorem could be proved by induction.</p> <p>It's easy to see that for any partial order set \\(S\\), we can divide it into three parts: \\(S = A \\cup D(A) \\cup U(A)\\), where \\(A\\) is the largest antichain, \\(D(A) = \\{x | \\exists a \\in A \\;\\; x \\le a \\}\\), \\(U(A) = \\{x | \\exists a \\in A \\;\\; a \\le x \\}\\).</p> <p>Then if there exists a largest antichain \\(A\\) s.t. \\(D(A), U(A)\\) are both not empty, we can apply induction hypothesis for \\(A \\cup D(A)\\) and \\(A \\cup U(A)\\) seperately (\\(A\\) is the largest antichain of \\(S\\) then it's also the largest antichain of \\(A \\cup D(A)\\)) and combine the chains we get.</p> <p>If any largest antichain \\(A\\) has either empty \\(D(A)\\) or empty \\(U(A)\\) (which means every \\(A\\) is either set of maximals or set of minimals) then we can firstly pick an arbitrary maximal element \\(y\\) and then pick an minimal element \\(x\\) such that \\(x \\le y\\) (this is always possible because we only need \"walk\" down along the chain). Obviously the path from \\(x\\) to \\(y\\): \\((x, \\cdots, y)\\) forms a chian. Then any largest antichain would contain either \\(x\\) or \\(y\\) (if neither of them is contained, this largest antichain can include one of them and becomes larger). Thus we can remove \\((x, \\cdots, y)\\), largest antichain's size must decrease by \\(1\\) and we apply induction and finally and the chain \\((x, \\cdots, y)\\) back.</p> <p>This theorem can solve this interesting problem from lecture Graph Theory.</p> <p>For a sequence of \\(kl + 1\\) numbers, we can either get a (non-strictly) monotone increasing subsequence of length \\(k + 1\\) or a montone decreasing subsequence of length \\(l + 1\\).</p> <p>This is because suppose any monotone increasing subsequence has length less than or equal to \\(k\\), then to divide the whole sequence, we need at least \\(l + 1\\) monotone increasing subseq. By Dilworth Theorem (monotone inc subseq is a chain), there has to be a antichain, i.e. a monotone dec. subseq., of size at least \\(l + 1\\).</p>"},{"location":"posts/ALGO/MST_survey/","title":"MST Naive Survey","text":""},{"location":"posts/ALGO/MST_survey/#0-abstract","title":"0 Abstract","text":"<p>This is a very simple and naive survey on M(Minimum) S(Spanning) T(Tree) problem. This problem is a widely researched and extremely important combinatorial problem, which has significant influences on all kinds of fields such as computer architecture allocation, transportation, express logistics, to name but a few. </p> <p>Also, a MST can also provide bounds (lower bound and 2-approximation upper bound if given triangle inequality) for Travelling Salesman Problem. Here I will introduce some classical algorithms and several modern and asymptotically better algorithms as well as parallel algorithms.</p>"},{"location":"posts/ALGO/MST_survey/#1-introduction","title":"1 Introduction","text":"<p>In a network composed of nodes and arcs with costs, a spanning tree is a acyclic subgraph connecting all the nodes together. A minimum spanning tree is the spanning tree with minimum cost on the network. That is it is the spanning tree with the least sum of the costs of all the edges. </p> <p>Finding the minimum spanning tree is useful in several different applications. Perhaps the most direct application is designing physical systems like road system. For example, consider isolated villages that need to be connected with roads. We need to ensure that each village has at least one way to go to any other villages. Obviously, we also want the total construction cost to be minimum, and this forms a MST problem. </p> <p>We can easily come up with other ideas and situations needing the aid of MST. Another application of the minimum spanning tree problem is cluster analysis. Suppose that we have a network and want to split the network into \\(k\\) different clusters such that the total cost of all the clusters is minimized. We can take the minimum spanning tree and delete the \\(k-1\\) arcs with the highest cost. The result is a forest of \\(k\\) trees with minimal cost. </p> <p>We can see that the above two examples respectively correspond to adding edges and deleting edges. We will then show that both adding and deleting edges are important with respect to MST problem. They are important in finding a MST and using MST to solve a practical problem.</p>"},{"location":"posts/ALGO/MST_survey/#2-basic-principle","title":"2 Basic Principle","text":"<p>[!Definition] Fundamental Cutset For a graph  \\(G\\)  two distinct vertices say  \\(i\\)  and  \\(j\\) , let  \\(X\\) be any subset of vertices that contains \\(i\\) but not  \\(j\\) and let  \\(\\bar{X}\\) be its complement, then set  \\(X\\) is a  \\(cut\\) and the set of edges  \\((i,j), i\\in X,j\\in \\bar{X}, (i,j)\\in E\\) is called a   \\(cutset\\) .  The removal of the arcs in a   \\(cutset\\) leaves a disconnected subgraph of  \\(G\\) . Corresponding to each edge  \\(e\\) of a spanning tree  \\(T\\) of a connected graph  \\(G\\) , there is a unique cutset called the <code>fundamental cutset</code> of the tree  \\(T\\) with respect to edge  \\(e\\) .</p> <p>We will then show that this definition has much to do with adding edges.</p> <p>[!Definition] Fundamental Circle Given a graph  \\(G\\) , a spanning tree  \\(T\\) and a co-tree edge   \\(e=(i, j)\\in E - T\\) , the unique cycle in  \\(G\\)  consisting of the edge  \\(e\\) and the edges of the unique chain in  \\(T\\) between  \\(i\\) and  \\(j\\) is called  a <code>fundamental cycle</code> of  \\(G\\) relative to  \\(T\\) with respect to  \\(e\\) .</p> <p>We will then show that this definition has much to do with deleting edges.</p> <p>[!Theorem] Fundamental Cut Optimality A spanning tree  \\(T\\) in a weighted graph is a MST if and only if every edge in the tree is a minimum weight edge in the<code>fundamental cutset</code>defined by that edge.  </p> <p>Proof:</p> <p>the weight of edge  \\((i,j)\\) is noted as  \\(c_{ij}\\) .</p> <p>1' MST==&gt;Cut Opt: This can be proved by contradiction. Suppose  \\(T^*\\) is a minimum spanning tree violating Cut Opt, then there are two edges   \\(c_{ij}\\in T\\) and  \\(c_{kl}\\in cut_{ij}\\)  such that  \\(c_{ij}&gt;c_{kl}\\) . Then replacing   \\(c_{ik}\\) with  \\(c_{kl}\\) would yield a lower cost spanning tree.</p> <p>2' Cut Opt==&gt;MST: Contradiction again. Suppose  \\(T^\u2217\\) satisfies the cut optimality conditions, but  \\(T^\u2217\\) is not a minimum spanning tree. That is, there exists a tree  \\(T^\u2032\u2260T^\u2217\\) that is a minimum spanning tree. Then, since  \\(T^\u2217\\) is not a minimum spanning tree, it must have an edge  \\((i,j)\\) not contained in  \\(T^\u2032\\) . Deleting edge  \\((i,j)\\) from  \\(T^\u2217\\)  creates a cut  \\([S,\\bar{S}]\\) . Adding one edge to a tree will form and only form one circle. Assume we add  \\((i,j)\\) to   \\(T'\\) then we will get a circle  \\(C\\) . Since  \\(i\\in S, j\\in \\bar{S}\\) , there must be an edge  \\((k,l)\\) in circle  \\(C\\) such that  \\(k\\in S, l\\in \\bar{S}\\) . According to Cut Opt we know that  \\(c_{ij}\\leq c_{kl}\\)  , and also we have  \\(c_{kl}\\leq c_{ij}\\) because  \\(T'\\) is a MST, which means that if  \\(c_{ij}&lt;c_{kl}\\) then we can replace  \\((k,l)\\) with  \\((i,j)\\) yielding a less cost tree. Having the both inequalities, we know that  \\(c_{ij}=c_{kl}\\) . Then we just replace  \\((k,l)\\) freely with  \\((i,j)\\) making a new MST (the total cost doesn't change, so it's still a MST) . Following this process, we can gradually change  \\(T'\\) into  \\(T^*\\) , which means that  \\(T^*\\) is already a MST. </p> <p>[Theorem 2] Fundamental Circle Optimality A spanning tree  \\(T\\) in a graph  \\(G\\) is a MST if and only if every edge  \\(e\\in E-T\\) is a maximum weight edge in the unique<code>fundamental cycle</code>defined by that edge.</p> <p>Proof:</p> <p>1' MST==&gt;Circle Opt: This can also be proved by contradiction. Assume that there exists a MST  \\(T^*\\) violating Fundamental Circle Optimality, which means that there is an edge  \\(e\\in E-T\\) weighing less than one edge  \\(e'\\) in the <code>fundamental cycle</code>defined by  \\(e\\) . Then we could delete edge  \\(e'\\) and add edge  \\(e\\) , then we can get a new tree  \\(T'\\) with less cost. This conflictst with the assumption that  \\(T^*\\) is a MST.</p> <p>2' Circle Opt==&gt;MST: This can be proved with the help of Theorem 1. It's obvious that if a tree doesn't hold Cut Opt, then it will not hold Circle Opt either. So we know that: Circle Opt==&gt;Cut Opt. Then we have: Circle Opt==&gt;Cut Opt==&gt;MST using Theorem 1. This is my personal proof, and here is an another much more elegant proof in <sup>1</sup>.</p> <p>Blue Rule: Select a cutset that does not contain a blue edge. Among the uncoloured edges in the cutset, select one of minimum weight and colour it blue.</p> <p>Red Rule: Select a simple cycle containing no red edges. Among the uncoloured edges on the cycle, select one of maximum weight and colour it red.  </p> <p>These two rules are proposed in Tarjan's book<sup>3</sup>, and elaborately illustrated by tutorial slides<sup>2</sup>. Since we already proved these two optimality conditions, these two rules can be immediately derived.</p> <p>Both red rules and blue rules can be used for the construction of MST, but in practice, we usually use blue rule. The most well known algorithms including Prim, Kruskal and Sollin's algorithm are all using blue rule.</p>"},{"location":"posts/ALGO/MST_survey/#3-details-of-classical-algorithm","title":"3 Details of Classical Algorithm","text":""},{"location":"posts/ALGO/MST_survey/#31-sollins-boruvkas-algorithm","title":"3.1 Sollin's (Boruvka's) Algorithm","text":"<p>More commonly known as Boruvka's algorithm, Sollin's algorithm was the first algorithm for the minimum spanning tree problem. Borukva first published his algorithm in 1926 which he designed as a way of constructing an efficient electricity network in Moravia, a region of the Czech Republic. He published two papers in the same year with this algorithm. One was 22 pages and has been viewed as unnecessarily complicated. The other was a one page paper that has been regarded as the key paper showing Boruvka's understanding and knowledge of the problem.</p> <p>It was then independently rediscovered in 1938 by French mathematician Gustave Choquet, and finally rediscovered in 1962 by Georges Sollin. </p> <p>We have introduced this algorithm in our lectures, so here I will not elaborately illustrate it. Each time we choose an edge with lowest cost from all edges incident to a component. We then add these edges into the graph and form a new graph with less components. It's obvious that this algorithm is using blue rules to add edges.</p> <p>The time complexity is  \\(O(e\\log n)\\) .</p>"},{"location":"posts/ALGO/MST_survey/#32-prim-algorithm","title":"3.2 Prim Algorithm","text":"<p>This algorithm is so classical that almost everyone will be taught with this algo when learning MST for the first time. It use an arbitrary starting vertex  \\(s\\) and apply the color step  \\(n-1\\) times. The color step is that: Let  \\(T\\) be the blue tree containing the vertex  \\(s\\) . </p> <p>Select a minimum weight edge incident to  \\(T\\) and colour it blue.   </p> <p>There are many ways to implement Prim algorithm, with different time complexity. The difference lies in the way we select the minimum-weight edge. If we simply go through all the edges, then the time complexity would be  \\(O(n^2)\\) . If we use binary heap and adjacent list, the time complexity would be  \\(O(e\\log n)\\) . If we use Fibonacci heap, the time complexity would be  \\(O(e+n\\log n)\\) . If we use d-heap, which is a general case of binary heap and an implementation of priority queue, the time complexity would be  \\(O(nd\\log_dn+m\\log_dn)\\) . If we properly choose  \\(d\\) , we can always get a satisfying algorithm for both dense graph and sparse graph (if dense, use a large d; if sparse, use a small d).</p>"},{"location":"posts/ALGO/MST_survey/#33-kruskal-algorithm","title":"3.3 Kruskal Algorithm","text":"<p>This algorithm is easy to implement and very suitable for competitive programming contests, so many competitors are very familiar with this algorithm. And its algorithm process can be a proof to many properties. The time complexity is  \\(O(e\\log e)=O(e\\log n)\\) . This algorithm is also using blue rule, each time picking up the minimum-weight edge. If the edges are already sorted for us, then the time complexity could be  \\(O(e\\alpha (e,n))\\) , where  \\(\\alpha(a,b)\\) is the inverse Ackermann's function. This is because we are using union-and-find. </p> <p>To be honest, I didn't understand the proof of the time complextiy of union-and-find, here I'm just using the result provided by Wikipedia.</p>"},{"location":"posts/ALGO/MST_survey/#4-not-that-well-known-algorithm","title":"4 Not That Well-Known Algorithm","text":"<p>We learned two not that well-known algorithms in this class: Yao algorithm and Cheriton-Tarjan algorithm. They are adapted from Sollin's algorithm, using smarter way to select the minimum-weight edge. </p>"},{"location":"posts/ALGO/MST_survey/#41-yao-algorithm","title":"4.1 Yao Algorithm","text":"<p>This is an algorithm proposed by Qizhi Yao. It is on the basis of another algorithm finding the median with deterministic linear time<sup>8</sup>. Assume that we already know how to find the median in linear time, then we can refine Sollin's algorithm by grouping edges. In Sollin's, we need to go through every edge to find the one with minimum weight. However, we know that some edges with large weight should not be considered in the early stages.</p> <p>Consider dividing all the edges into  \\(k\\) groups, then we each time only need to go through the edges within the group. Then the time complexity is  \\(O(dividing+\\frac{n}{k}\\log n)\\) . Then what is the time complexity of dividing edges into  \\(k\\) groups such that edges in  \\(i+1\\)th group have larger weight than these in  \\(i\\)th group? We each time find the median and divide the set into two parts, and then recurssively go down in these two smaller subset. It's obvious that the dividing time is  \\(O(n\\log k)\\) .  Then using simple calculus knowledge we can that when  \\(k=\\log n\\) the formula  \\(O(n\\log k+\\frac{n}{k}\\log n)\\) gets its minimum:  \\(O(n\\log \\log k)\\) .</p>"},{"location":"posts/ALGO/MST_survey/#42-cheriton-tarjan-algorithm","title":"4.2 Cheriton-Tarjan Algorithm","text":"<p>The full algorithm<sup>5</sup> is very complex, here is the key point: A heap is kept for each blue tree. Each heap holds the edges with at least one endpoint in the tree and which are candidates for becoming blue. Similar to Kruskal's algorithm, it grows a spanning forest, beginning with a forest of   \\(n\\) components each consisting of a single node. Since, every component    \\(T_u\\) must eventually be connected to another component, this algorithm keeps a separate heap   \\(PQ_u\\) for each component   \\(T_u\\) , so, that initially   \\(n\\) smaller heaps are used. Initially,   \\(PQ_u\\) will contain only   \\(DEG(u)\\) edges, since   \\(T_u\\) consists only of vertex   \\(u\\) . When   \\(T_u\\) and   \\(T_v\\)  are merged,   \\(PQ_u\\) and   \\(PQ_v\\) must also be merged. This requires a modification of the data structures, since heaps cannot be merged efficiently. This is essentially because merging heaps reduces to building a new heap. It's difficult to work out the precise time complexity, I still need some time to understand the original paper. Here I just use the result provided by the paper, that the time complexity is   \\(O(e\\log \\log n)\\) .</p> <p>In Cheriton and Tarjan's paper, they also proposed an algorithm for planar graph with time complexity   \\(O(n)\\) , and an algorithm for sparse graph with time complexity   \\(O(e)\\) . </p>"},{"location":"posts/ALGO/MST_survey/#5-randomized-algorithm","title":"5 Randomized Algorithm","text":""},{"location":"posts/ALGO/MST_survey/#51-tarjans-expected-linear-time-algorithm","title":"5.1 Tarjan's Expected Linear Time Algorithm","text":"<p>It was developed by David Karger, Philip Klein, and Robert Tarjan. So it is also known as Karger's algorithm. The algorithm relies on techniques from Boruvka's algorithm along with an algorithm for verifying a minimum spanning tree in linear time. It combines the design paradigms of divide and conquer algorithms, greedy algorithms, and randomized algorithms to achieve expected linear performance. The key insight to the algorithm is a random sampling step which partitions a graph into two subgraphs by randomly selecting edges to include in each subgraph. The algorithm recursively finds the minimum spanning forest of the first subproblem and uses the solution in conjunction with a linear time verification algorithm to discard edges in the graph that cannot be in the minimum spanning tree. A procedure taken from Bor\u016fvka's algorithm is also used to reduce the size of the graph at each recursion. </p> <p>In more details, there are three steps in total: Boruvka step, random sampling step, and verification step.</p> <ul> <li>Boruvka Step: Given a graph  \\(G\\) , apply the Boruvka algorithm to carry out one coloring step only. And then contract the graph  \\(G\\) and get  \\(G'\\) .</li> <li>Random sampling step: Given a contracted graph  \\(G\\) , choose a subgraph  \\(H\\) by selecting each edge</li> </ul> <p>in  \\(G\\) independently with a probability   \\(1/2\\) .</p> <ul> <li>Verification step: Given any minimum spanning forest  \\(F\\) of  \\(H\\) , find the   \\(F_{heavy}\\)  edges and delete them from  \\(G\\) to reduce the graph further. Here   \\(F_{heavy}\\)  means that the weight of an edge   \\(w(u,v)\\)  is larger than the weight of the path from   \\(u\\)  to   \\(v\\)  in a forest   \\(F\\) . It's obvious that all the  \\(F_{heavy}\\) edges will not appear in the final MST, so we can delete them.</li> </ul> <p>The complete algorithm process is shown below:</p> <ol> <li>Given   \\(G(V, E)\\) , apply two successive Boruvka steps to the graph to contract  \\(G\\) .</li> <li>Apply the random sampling step to the contracted graph to select  \\(H\\) .</li> <li>Apply the algorithm recursively producing a minimum spanning forest  \\(F\\) of the   \\(H\\)  formed in step 1</li> <li>Given   \\(F\\)  of   \\(H\\) , apply the verification step to the subgraph  \\(H\\) , which was chosen, and obtain</li> </ol> <p>a graph  \\(G\\) which is reduced further.</p> <ol> <li>Apply the algorithm recursively to  \\(G\\) to compute the minimum spanning forest  \\(F\\) of  \\(G\\) .</li> <li>Return those edges contracted in step 1 together with the edges of  \\(F\\) .</li> </ol> <p>The time complexity relies on two properties:</p> <p>Property 1 Let  \\(H\\) be a subgraph obtained from  \\(G\\) by including each edge independently with probability  \\(p\\) , and let  \\(F\\) be the minimum spanning forest of  \\(H\\) . The expected number of  \\(F\\) edges in  \\(G\\) is at mos  \\(n/p\\) where  \\(n\\) is the number of vertices.</p> <p>Property 2 We can do the verification step in deterministic linear time with the algorithm<sup>9</sup> proposed by V. King.</p> <p>Both properties need further research and more time, I will get it through soon. The expected running time is  \\(O(m)\\) and it runs in   \\(O(m)\\)  time with probability   \\(1-exp(-\\Omega (m))\\) . In the worst case, it will be equivalent to Boruvka's algorithm, so the worst time complexity is   \\(O(e\\log n)\\) .</p>"},{"location":"posts/ALGO/MST_survey/#6-parallel-algorithms","title":"6 Parallel Algorithms","text":"<p>The performance comparison of these sequencial algorithms is basically of no use, because in reality, we will never use one processer to handle a large-scale problem. And this is also the reason why parallel algorithms are getting more and more popular. All the classical algorithms illustrated above can be adapted into parallel version.</p>"},{"location":"posts/ALGO/MST_survey/#61-prim-parallel-version","title":"6.1 Prim Parallel Version","text":"<p>Prim algorithm has the least parallelism possiblity. Because each time we need to update the distance vecter, which can't be paralleled, since each element can't be changed by two processes. What we can do is that: we can use paralleled priority queue with insert time complexity of  \\(O(1)\\) .</p>"},{"location":"posts/ALGO/MST_survey/#62-kruskal-parallel-version","title":"6.2 Kruskal Parallel Version","text":"<p>There are two ways to parallelize the Kruskal algorithm. The first is to parallelize the sorting stage. We know that in Kruskal algorithm, we need to sort all the edges by their weight, which could be parallelized. We can sort  \\(n\\) elements in   \\(O(\\log n)\\)  time with   \\(n\\)  processors. So the total time would be   \\(O(m\\alpha(n))\\) . The second way is to parallelize an adaptation of Kruskal algorithm, which is named Filter-Kruskal algorithm[10]. </p>"},{"location":"posts/ALGO/MST_survey/#63-sollin-parallel-version","title":"6.3 Sollin Parallel Version","text":"<p>In Sollin's process, we have three stages. The first stage is finding the lightest edge, which can be parallelized; the second stage is to go through each subgraph, which can be parallelized; the third stage is to contract the graph, which can be parallelized. The respective time complexity is   \\(O(\\frac{m}{p}+\\log n+\\log p), O(\\frac{n}{p}+\\log n), O(\\frac{m}{p}+\\log n)\\) .</p>"},{"location":"posts/ALGO/MST_survey/#7-other-applicatioins-and-properties","title":"7 Other Applicatioins and Properties","text":"<p>There are so many different variations about MST, coming up with different graceful properties. We can impose some edges, which means that these edges must be in MST, and then construct the MST; We can add a new vertex and the incident edges and find the new MST based on the previous MST, which is one of our homework problems; We can delete one vertex and find the new MST; We can change the weight of one edge and find the MST; We can get linear time approxiamations on MST, to name but a few. Here I will introduce some simple properties:</p> <ol> <li>Given a graph with its MST, and a new vertex with its incident edges, we can find the new MST in  \\(O(n)\\) time. This is based the observation: the new MST will only contain the newly added edges and the edges in the original MST.</li> <li>Given a graph with its MST, and a deleted vertex, we can also ensure that the new MST will consist all the edges in the previous MST except for these deleted ones.</li> <li>Add a constant to the weight of every vertex will not change the MST.</li> <li>Change the weight of an edge, there are four cases in total. If we decrease the edge in MST, remains the same; If we increase the edge not in MST, remains the same; If we increase the edge in MST, we just need   \\(O(m)\\)  time to find the new edge connecting the two components; If we decrease the edge not in MST, we just need to find the circle in  \\(MST\\cup edge\\) , if the decreased weight is smaller than any edge in that circle, we can do a substitution.</li> <li>There might be many MSTs in a graph, but all the MSTs have the same edges-weight-set, which means that: if we only focus on the weight of the edges in MST, then the sorted array of the weights of the edges in any MST is the same.</li> </ol>"},{"location":"posts/ALGO/MST_survey/#8-reference","title":"8 Reference","text":"<ol> <li> <p>Slides about MST and cut optimality and path optimality conditions https://copland.udel.edu/~amotong/teaching/algorithm%20design/lectures/(Lec%2012)%20Greedy%20Strategy%20-%20Minimum%20Spanning%20Tree%20-%20Optimality%20Condition.pdf \u21a9</p> </li> <li> <p>Slides about MST and blue rule and red rule https://www.cs.princeton.edu/~wayne/cs423/lectures/mst.pdf \u21a9</p> </li> <li> <p>Tarjan RE. Data structures and network algorithms. In CBMS-NFS Regional Conference Series in Applied Mathematics. Philadelphia: Society for Industrial and Applied Mathematics, 1983. p. 44. https://www.amazon.com/Structures-Algorithms-CBMS-NSF-Conference-Mathematics/dp/0898711878 (This is a book, so it's the link of the book on Amazon)\u00a0\u21a9</p> </li> <li> <p>Johnson DB. Priority queues with update and \"finding minimum spanning trees. Information Processing Letters 1975;4:53}7. https://www.sciencedirect.com/science/article/abs/pii/0020019075900010  ZJU didn't subscribe to this journel, pity.\u00a0\u21a9</p> </li> <li> <p>FINDING MINIMUM SPANNING TREES, DAVID CHERITON AND ROBERT ENDRE TARJAN https://epubs.siam.org/doi/pdf/10.1137/0205051 \u21a9</p> </li> <li> <p>Improved Multiple Constant Multiplication Using a Minimum Spanning Tree https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1399088 \u21a9</p> </li> <li> <p>Imposing edges in Minimum Spanning Tree https://arxiv.org/pdf/1912.09360.pdf \u21a9</p> </li> <li> <p>Linear Time Median Finding: https://rcoh.me/posts/linear-time-median-finding/ \u21a9</p> </li> <li> <p>A Simpler Minimum Spanning Tree Verification Algorithm  https://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/A%20Simpler%20Minimum%20Spanning.pdf \u21a9</p> </li> <li> <p>The Filter-Kruskal Minimum Spanning Tree Algorithm http://algo2.iti.kit.edu/documents/fkruskal.pdf \u21a9</p> </li> </ol>"},{"location":"posts/ALGO/NetworkFlow/","title":"Network Flow","text":"<p>Suppose at first the graph has only intergral capacity.</p> <p>The Ford-Folkerson algorithm runs in this intuitive way:</p> <p>For currenet \\(f-\\)flow (if it's the very first step, then the flow is just empty), we try to augment it by finding a path from \\(s\\) to \\(t\\) in residual graph.</p> <p>If we can find such a path, then we add this path to the current \\(f-\\)flow and we get a new flow with at least 1 more value.</p> <p>And the other direction is always true: if we can't find an augmentation path, then it means \\(s\\) and \\(t\\) are already seperated into two connected component in residual graph, thus we can get a \\(s-t\\) cut \\(C = \\text{component}(s)\\).</p> <p>Now we want to prove this cut has the same value of the flow.</p> <p>We know that the value of a flow:</p> \\[ \\nu(f) = f(\\delta^+(C)) - f(\\delta^-(C)) \\] <p>While since \\(C\\) is a connected component in residual graph, so \\(u_r(\\delta^+(C)) = 0\\), which means \\(f(\\delta^+(C)) = u(\\delta^+(C))\\).</p> <p>Again, for a similar reason, \\(u_r(\\delta^-(C)^R) = 0\\), which means \\(f(\\delta^-(C)) = 0\\).</p> <p>So in conclusion,</p> \\[\\begin{align*} \\nu(f) &amp;= f(\\delta^+(C)) - f(\\delta^-(C)) \\\\        &amp;= u(\\delta^+(C)) - 0 \\\\        &amp;= u(\\delta^+(C)) \\\\        &amp;= \\text{cut}(C) \\end{align*}\\] <p>This indeed proves the Ford-Folkerson algorithm's correctness.</p> <p>If the algorithm terminates, then it finds a maximum flow; if it didn't terminate yet, then we can at least improve the flow by 1.</p> <p>So the time complexity bound of this algorithm is \\(O(|E| f)\\) where \\(|E|\\) is the number of edges and \\(f\\) is the value of maximum flow.</p> <p>A variation of FF algorithm is the Edmonds-Karp algorithm with time complexity bound \\(O(|V||E|^2)\\). Another variation is Dinic/Dinitz algorithm, which is published earlier with time complexity of \\(O(|V|^2|E|)\\).</p> <p>The idea of EK algorithm is also simple. We still implement the FF algorithm but each time we pick the shortest augment path (short in the sense of number of edges). We can always find such path using BFS.</p> <p>Denote the edges in BFS tree (a tree formed by the edges used in BFS procedure) as \\(E_L\\).</p> <p>It's obvious that each iteration of augmenting cost \\(O(|E|)\\) time so now we need to determine the upper bound of the number of iterations.</p> <p>We denote the edges in the BFS tree of \\(G_f\\) to be \\(E_L\\).</p> <p>We notice that using BFS to find augment path, we always have at least one bottleneck edge that will get saturated. This edge will be removed and the reverse will be added to the graph.</p> <p>Say edge \\(e = (u, v)\\) is a bottleneck edge with \\(u \\in L_i, v \\in L_{i+1}\\) where \\(L_i = \\{x | \\text{dist}(s, x) = i\\}\\). Then removing \\(e\\) and adding \\(e^R\\) will never decrease the distance between \\(s\\) and \\(t\\); and the level of \\(u\\) increase by at least 2 or the edge \\(e\\) is completely removed from the BFS tree, the level of \\(v\\) doesn't decrease. If an edge is removed from the BFS tree, it cannot be added back (why? think about this).</p> <p>This means an edge can at most appear \\(\\frac{n}{2}\\) times because the BFS tree is at most \\(n\\) levels deep. We have \\(2|E|\\) edges including the reversed ones, so the number of iterations would be $O(|V||E|), leading a total runtime of \\(O(|V||E|^2)\\).</p> <p>Another variation Dinic algorithm also implements FF algorithm but in yet another way finding a new augment path.</p> <p>It's indeed not only a path but actually a blocking flow.</p> <p>In Dinic algorithm, each iteration we find a blocking flow and add it to current flow.</p> <p>A blocking flow is definied in this way:</p> <p>Definition</p> <p>A blocking flow in \\(G_f\\) is a flow \\(f'\\) such that every shortest path (short in the sense of number of edges) consists of at least one saturated edge in \\(f'\\). Or in other words, there isn't a path from \\(s\\) to \\(t\\) only using the edges \\(E_L \\setminus \\{e \\;|\\; f'(e) = u_f(e)\\}\\).</p> <p>Blocking flow isn't necessarily a max flow.</p> <p>We firstly prove in this way we only need \\(O(n)\\) iterations.</p> <p>Like how we reason in EK algorithm, we would like to show each iteraion the distance in \\(G_L = (V, E_L)\\) between \\(s\\) and \\(t\\) increase at least by one.</p> <p>Note that every shortest path between \\(s\\) and \\(t\\) contains a saturated edge (because of the definition of blocking flow). So suppose before the iteration the flow is \\(f\\), and during iteration we find the blocking flow \\(f'\\), and the graph \\(G_L\\) is constructed under \\(G_f\\) and \\(G'_L\\) is constructed under \\(G_{f + f'}\\).</p> <p>Now if we have a shortest path \\(p'\\) from \\(s\\) to \\(t\\) in \\(G'_L\\) with the same distance as the shorteset path \\(p\\) from \\(s\\) to \\(t\\) in \\(G_L\\).</p> <p>If \\(p'\\) consists of a reversed edge (reversed in \\(G'_L\\) and un-reversed in \\(G_L\\)), then there exists a shorter path in \\(G_L\\), leading to contradiction; if \\(p'\\) doesn't contain any reversed edge, then all edges also appear in \\(G_L\\). However by definition of blocking flow, every shortest path contains at least one saturated edge.</p> <p>So we proved by contradition that the distance on \\(G_L\\) increase by at least 1 after each iteration.</p> <p>The distance can increase no more than \\(n\\) times since the upper bound of the distance is \\(n\\).</p> <p>Then it comes to the question: how to find such blocking flow?</p> <p>We can give a simple algorithm which does DFS on \\(G_L\\) repeatedly until no DFS path can be found.</p> <p>Each time we run DFS from \\(s\\) to find a path to \\(t\\). During DFS, if we notice the subtree of \\(v\\) didn't return a valid path to \\(t\\) then we remove the edges to \\(v\\); and after find such a path, we remove the edges that are saturated.</p> <p>The pseudocode looks like this:</p> <pre><code>f = 0;\nH = E_L;\n\nwhile (true) {\n       P = dfs(s, t, H);\n       if (P.empty()) {\n              return f;\n       } else {\n              f' = the flow saturates the path P;\n              f = f + f';\n       }\n}\n\nPath dfs(node u, node t, EdgeSet H) {\n       for ((u, v) in H) {\n              P = dfs(v, t, H);\n              if (P.empty()) {\n                     H.remove((u, v));\n              } else {\n                     return (u, v) + P;\n              }\n       }\n       return emptyPath;\n}\n</code></pre> <p>Then what's the runtime bound for one iteration?</p> <p>Consider that each edge can be only delted for once, so it takes at most \\(O(m)\\) time; each edge can be saturated only once, so the <code>while()</code> loop can have at most \\(m\\) iterations; and despite of the time of removing edges in <code>dfs</code> (since we already took them into consideration), the runtime of each <code>dfs()</code> is $O(n). So the total runtime would be \\(O(m + nm) = O(nm)\\).</p> <p>Now we are done, we have at most \\(O(n)\\) iterations and each iteration costs \\(O(nm)\\), so the time complexity of Dinic is \\(O(n^2m)\\).</p> <p>In special cases we can have better bounds for the number of iterations.</p> <p>If it's a unit capacity graph then we have at most \\(\\min(m^{1/2}, n^{2/3})\\) iterations.</p> <p>We can prove the two upper bounds one by one.</p> <p>For the first one, consider after \\(k\\) iterations we have a flow \\(f\\) and a residual graph \\(G_f\\). Suppose the maximal flow on \\(G_f\\) is \\(f'\\), which means \\(f + f'\\) is the max flow for graph \\(G\\).</p> <p>Since it's already after \\(k\\) iterations, the min distance between \\(s\\) and \\(t\\) is at least \\(k\\).</p> <p>Then the capacities of all edges in \\(f'\\) is at least \\(\\nu(f) \\cdot k\\) (consider path decomposition). However we have in total \\(m\\) edges so \\(\\nu(f) \\cdot k \\le m\\), thus \\(\\nu(f) \\le \\frac{m}{k}\\). So we can improve the current flow by at most \\(\\frac{m}{k}\\).</p> <p>We know that each iteration increase the flow by at least 1 so the remaining iterations are at most \\(\\frac{m}{k}\\).</p> <p>So the number of iterations is bounded by \\(k + \\frac{m}{k}\\) which minimize at \\(k = \\sqrt{m}\\).</p> <p>For the second bound, consider the BFS tree.</p> <p>There are at least \\(k\\) levels in BFS tree, and \\(n-1\\) vertices in total (ignore the source \\(s\\)). Ignore the levels deeper than \\(k\\). (Here \\(k\\) is the distance between \\(s\\) and \\(t\\))</p> <p>There are more than \\(k/2\\) levels with less than \\(2n/k\\) vertices, otherwise the sum exceeds \\(n-1\\). </p> <p>So at least two such levels are adjacent. There are at most \\((2n/k)^2\\) edges connected between these two levels, so we can form a \\(s-t\\) cut with value at most \\((2n/k)^2\\). Again, we know that each iteration increase the flow by at least 1 so the remaining iterations are at most \\(\\frac{4n^2}{k^2}\\).</p> <p>Now the number of iterations is bounded by \\(k + \\frac{4n^2}{k^2}\\) which minimize at \\(k=2n^{2/3}\\) with min value \\(O(n^{2/3})\\).</p> <p>And for such graph, the blocking flow can be found in \\(O(m)\\) time.</p> <p>Try to prove the upper bound of iterations for bipartite graph matching flow: \\(O(\\sqrt{n})\\).</p>"},{"location":"posts/ALGO/RandomizedIncrementalAlgo/","title":"Randomized Inceremental Algorithm","text":"<p>\u968f\u673a\u589e\u91cf\u6cd5\u5728\u5b66\u7ade\u8d5b\u7684\u65f6\u5019\u542c\u5b66\u957f\u8bb2\u8fc7\uff0c\u53ef\u60dc\u5f53\u65f6\u6839\u672c\u6ca1\u542c\u61c2\u3002\u53ea\u77e5\u9053\u5b83\u5f88\u795e\u5947\uff0c\u770b\u8d77\u6765\u4e09\u5c42\u5faa\u73af\u4f46\u5b9e\u9645\u4e0a\u671f\u671b\u590d\u6742\u5ea6\u7ebf\u6027\uff1b\u770b\u8d77\u6765\u9053\u7406\u5f88\u7b80\u5355\uff0c\u4f46\u5374\u80fd\u89e3\u51b3\u611f\u89c9\u4e0a\u5f88\u590d\u6742\u7684\u95ee\u9898\u3002</p> <p>\u5728\u7ade\u8d5b\u4e2d\u6b64\u7c7b\u7b97\u6cd5\u53ea\u8003\u5bdf\u8fc7\u6700\u5c0f\u5706\u8986\u76d6\u95ee\u9898\u3002\u5b9e\u9645\u4e0a\u8fd8\u6709\u4e00\u4e9b\u5176\u4ed6\u95ee\u9898\u4e5f\u53ef\u4ee5\u7528\u7c7b\u4f3c\u7684\u601d\u60f3\u89e3\u51b3\uff08refer to https://www.cs.rpi.edu/~cutler/classes/computationalgeometry/F23/lectures/07_randomized_incremental_construction.pdf\uff09\u3002</p> <p>\u6211\u4eec\u5728\u8fd9\u91cc\u5148\u4ecb\u7ecd\u6700\u5c0f\u5706\u8986\u76d6\uff0c\u540e\u9762\u518d\u65bd\u5de5\u5176\u4ed6\u7684\u7b97\u6cd5\u3002</p> <p>\u6700\u5c0f\u5706\u8986\u76d6\u95ee\u9898</p> <p>\u7ed9\u5b9a\u5e73\u9762\u4e0a \\(n\\) \u4e2a\u70b9\uff0c\u6c42\u534a\u5f84\u6700\u5c0f\u7684\u5706\u4f7f\u5f97\u6240\u6709\u7684\u70b9\u90fd\u5728\u5706\u4e0a\u6216\u5706\u5185\u3002</p> <p>\u8fd9\u4e2a\u95ee\u9898\u662f\u6bd4\u8f83\u7279\u6b8a\u7684\u9009\u5740\u95ee\u9898\uff08\u8003\u8651 \\(R^2\\) \u4e0a\u7684\u6b27\u6c0f\u8ddd\u79bb\u7684\u53ea\u6709\u4e00\u4e2a\u8bbe\u65bd\u7684\u9009\u5740\u95ee\u9898\uff09\uff0c\u53ef\u4ee5\u5f88\u81ea\u7136\u7684\u63a8\u5e7f\u5230\u66f4\u9ad8\u7ef4\u7684\u6700\u5c0f*\u7403*\u8986\u76d6\u3002</p> <p>\u6211\u4eec\u5728\u7ade\u8d5b\u4e2d\u7528\u5230\u7684\u7b97\u6cd5\u662f Welzl \u7b97\u6cd5 \uff08https://www.cs.rpi.edu/~cutler/classes/computationalgeometry/F23/lectures/07_randomized_incremental_construction.pdf\uff09\u539f\u6587\u5199\u7684\u975e\u5e38\u597d\uff0c\u6709\u5174\u8da3\u7684\u8bdd\u53ef\u4ee5\u770b\u770b\u3002\u540c\u65f6\u8fd9\u4e2a\u95ee\u9898\u672c\u8eab\u4e5f\u6709\u786e\u5b9a\u6027\u7ebf\u6027\u65f6\u95f4\u7684\u7b97\u6cd5\uff0c\u4f46\u662f\u5e38\u6570\u5f88\u5927\uff08refer to https://en.wikipedia.org/wiki/Smallest-circle_problem\uff09\u3002\u4e24\u4e2a\u7b97\u6cd5\u90fd\u6709\u6765\u81ea\u7ebf\u6027\u89c4\u5212\u7684 intuition\u3002</p> <p>\u9996\u5148\u6211\u4eec\u9700\u8981\u4e24\u4e2a\u770b\u8d77\u6765\u5f88\u663e\u7136\u7684\u5f15\u7406\uff1a</p> <p>\u5f15\u7406</p> <p>\u5305\u542b\u4e09\u4e2a\u4e0d\u5171\u7ebf\u7684\u70b9\u7684\u6700\u5c0f\u534a\u5f84\u7684\u5706\u662f\u7531\u8fd9\u4e09\u4e2a\u70b9\u5916\u63a5\u6240\u786e\u5b9a\u7684\u5706\u3002</p> <p>\u7528\u6270\u52a8\u7684\u65b9\u6cd5\u5f88\u7b80\u5355\u5373\u53ef\u8bc1\u660e\u3002</p> <p>\u5f15\u7406</p> <p>\u6700\u5c0f\u8986\u76d6\u5706\u6709\u4e14\u4ec5\u6709\u4e00\u4e2a\u3002</p> <p>\u5b58\u5728\u6700\u5c0f\u8986\u76d6\u5706\u662f\u663e\u7136\u7684\uff0c\u552f\u4e00\u6027\u53ef\u4ee5\u901a\u8fc7\u53cd\u8bc1\u6cd5\u8bc1\u660e\u3002\u5982\u679c\u6709\u4e24\u4e2a\u6700\u5c0f\u534a\u5f84\u8986\u76d6\u5706\uff0c\u90a3\u4e48\u70b9\u96c6\u4e00\u5b9a\u90fd\u5728\u4e24\u4e2a\u5706\u7684\u4ea4\u96c6\u4e2d\u3002\u660e\u663e\u53ef\u4ee5\u6784\u9020\u4e00\u4e2a\u534a\u5f84\u66f4\u5c0f\u7684\u5706\u8986\u76d6\u8fd9\u4e2a\u4ea4\u96c6\uff0c\u4e0e\u6700\u5c0f\u534a\u5f84\u7684\u5047\u8bbe\u77db\u76fe\u3002</p> <p>\u800c\u540e\u6211\u4eec\u4ecb\u7ecd\u7b97\u6cd5\u6d41\u7a0b\u3002</p> <p>\u7b97\u6cd5\u672c\u8eab\u6781\u5ea6\u7b80\u5355\uff0c\u7528\u9012\u5f52\u6765\u63cf\u8ff0\u7b80\u6d01\u660e\u4e86\uff1a</p> <pre><code>algorithm Welzl\n    input: Finite sets P and R of points in the plane |R| \u2264 3.\n    output: Minimal disk enclosing P with R on the boundary.\n\n    if P is empty or |R| = 3 then\n        return trivial(R)\n    choose p in P (randomly and uniformly)\n    D := welzl(P \u2212 {p}, R)\n    if p is in D then\n        return D\n\n    return welzl(P \u2212 {p}, R \u222a {p})\n</code></pre> <p>\u4f2a\u4ee3\u7801\u4e2d <code>P</code> \u4ee3\u8868\u9700\u8981\u88ab\u8986\u76d6\u7684\u70b9\u96c6\u3002<code>R</code> \u4ee3\u8868\u989d\u5916\u7684\u7ea6\u675f\uff0c\u6211\u4eec\u8981\u6c42 <code>R</code> \u4e2d\u7684\u5143\u7d20\u5fc5\u987b\u5728\u5706\u7684\u8fb9\u754c\u4e0a\u3002\u6240\u4ee5\u7b97\u6cd5\u6c42\u7684\u662f\u6240\u6709\u8986\u76d6 <code>P</code> \u4e14\u8fb9\u754c\u5305\u542b <code>R</code> \u7684\u5706\u4e2d\u534a\u5f84\u6700\u5c0f\u7684\u90a3\u4e00\u4e2a\u3002</p> <p>\u7b97\u6cd5\u601d\u8def\u662f\uff1a</p> <p>\u7ed9\u5b9a\u8f93\u5165 \\(P\\) \u4e3a\u9700\u8981\u88ab\u8986\u76d6\u7684\u70b9\u96c6\uff0c\\(R\\) \u4e3a\u5df2\u77e5\u5fc5\u987b\u5728\u5706\u4e0a\u7684\u70b9\u6784\u6210\u7684\u70b9\u96c6\u3002\u6bcf\u6b21\u968f\u673a\u6311\u9009\u4e00\u4e2a\u70b9 \\(p\\) \u6392\u9664\u5728\u5916\uff0c\u9012\u5f52\u7684\u6c42\u53d6\u5269\u4e0b\u7684 \\(n-1\\) \u4e2a\u7684\u70b9\u7684\u6700\u5c0f\u5706\u8986\u76d6\uff0c\u8bb0\u4e3a \\(md(P \\setminus \\{p\\})\\)\u3002</p> <ul> <li>\u5982\u679c\u88ab\u6392\u9664\u7684\u70b9 \\(p\\) \u5df2\u7ecf\u5305\u542b\u5728 \\(md(P \\setminus \\{p\\})\\)\uff0c\u90a3\u4e48\u76f4\u63a5\u8fd4\u56de \\(md(P \\setminus \\{p\\})\\)\uff1b</li> <li>\u5982\u679c\u4e0d\u5305\u542b\uff0c\u8bf4\u660e \\(p\\) \u4e00\u5b9a\u5728 \\(md(P)\\) \u7684\u5706\u5468\u4e0a\uff0c\u90a3\u4e48\u6211\u4eec\u4ee4 \\(R = R \\cup \\{p\\}\\). \u7136\u540e\u9012\u5f52\u6267\u884c \\(md(P \\setminus \\{p\\}, R)\\).</li> </ul> <p>\u5982\u679c \\(R\\) \u5df2\u7ecf\u5305\u542b\u4e09\u4e2a\u5143\u7d20\uff0c\u90a3\u4e48\u5706\u53ef\u4ee5\u88ab\u76f4\u63a5\u786e\u5b9a\u3002</p> <p>\u7136\u540e\u6211\u4eec\u5206\u6790\u671f\u671b\u65f6\u95f4\u590d\u6742\u5ea6\u3002</p> \\[\\begin{align} E(T(n, j)) &amp;= 1 + E(T(n - 1, j)) + \\text{Pr} \\left\\{ p \\notin md(P \\setminus \\{p\\}, R) \\right\\} \\cdot E(T(n - 1, j + 1)) \\\\ E(T(n, 3)) &amp;= 0 \\end{align}\\] <p>\u8003\u8651\u6bcf\u4e2a\u70b9\u90fd\u662f\u968f\u673a\u9009\u62e9\u7684\uff0c\\(n\\) \u4e2a\u70b9\u4e2d\u53ea\u9700\u8981 \\(3\\) \u4e2a\u70b9\u5c31\u53ef\u4ee5\u786e\u5b9a\u6700\u5c0f\u8986\u76d6\u5706\uff0c\u800c\u6211\u4eec\u8003\u8651\u5269\u4e0b\u7684 \\(n - 3\\) \u4e2a\u70b9\uff0c\u4ed6\u4eec\u5982\u679c\u662f\u88ab\u968f\u673a\u53bb\u6389\u7684\u90a3\u4e2a\u70b9\uff0c\u4e00\u5b9a\u4f1a\u88ab\u5305\u542b\u5728\u5269\u4e0b\u7684\u70b9\u7684\u6700\u5c0f\u8986\u76d6\u5706\u91cc\uff0c\u6240\u4ee5 \\(\\text{Pr}\\left\\{p \\notin md(P \\setminus \\{p\\}, R)\\right\\} \\le \\frac{3}{n}\\)\u3002\u6240\u4ee5\u7b80\u5355\u6570\u5b66\u5f52\u7eb3\u6cd5\u5373\u53ef\u8bc1\u660e \\(E(T(n, j)) = O(n)\\)</p>"},{"location":"posts/ALGO/nearest_pair/","title":"Plane Nearest Point Pair","text":"<p>It's a popular problem to solve: given \\(n\\) points on a plane, how to find the minimum distance between two points?</p> <p>If you are familiar with Delaunay triangulation, then you can simply calculate the Delaunay triangulation first and then iterate over every edges (the total number of edges in Delaunay Tri. is linear to the number of points).</p> <p>A classical and straightforward algorithm is divide and conquer.</p> <p>We first sort all points by x-coordinate. Then we divide all points into two even halves \\(S_1, S_2\\) and call the recursive procedure.</p> <p>Suppose we already get the min distance \\(h_1, h_2\\) for \\(S_1, S_2\\). Let \\(h = \\min(h_1, h_2)\\). Then clearly now we only need to consider distance between a point in \\(S_1\\) and a point in \\(S_2\\), and both of them lie in a stripe of width \\(h\\) from the pivot element.</p> <p></p> <p>Suppose we have some magics to sort the green points in y-coordinate within linear time, then we can simply use a sliding window to tract for each point in green set, which points can be candidate of nearest pair.</p> <p>Another concern would be: what if the size of sliding windows is too large? </p> <p>That's indeed not the problem because we can prove that there will be at most 6 points in the sliding windows otherwise the min dist in \\(S_1, S_2\\) wouldn't be \\(h\\).</p> <p>Then how can we sort the points in y-coordinate?</p> <p>Easy, we can let the recursive procedure to simply return a y-axis sorted array and for each pass we do a merge them together.</p>"},{"location":"posts/ALGO/nearest_pair/#randomized-algorithm","title":"Randomized Algorithm","text":"<p>There is also an interesting interative algorithm works in expected \\(O(n)\\) time with randomization.</p> <p>Suppose we already know the min dist for first \\(i\\) points is \\(h\\). Then we divide the whole plane into meshes of size \\(h\\), and for each point we determine which grid it's in and store this information with a hashmap. Each time a point \\(v\\) comes, we find the resident grid of \\(v\\) and all candidates for new min dist must live in the surrounding grids. If no new min dist appear, then nothing happens, the algo goes on; if new min dist found, then we divide the plane into new meshes of size updated min dist \\(h'\\).</p> <p>For \\(i\\) points, the probability that min dist involves \\(i\\)-th point is \\(O(\\frac{1}{i})\\), while the time cost to re-construct the new meshes is \\(O(i)\\), so the expected time cost for each iteration is \\(O(1)\\), which sum up to \\(O(n)\\).</p>"},{"location":"posts/ALGO/scanning_line/","title":"Scanning Line","text":"<p>Scanning line is an algorithm that can be used to solve problems like: calc the area of several probably overlapping rectangles.</p> <p>Here is a perfect illustration:</p> <p></p> <p>As the figure shows, we divide the area into several sub-rectangles by imagining a scanning line moving from bottom to top.</p> <p>Clearly, we need to manintain at each time point, how wide can the rectangle be. This can be done with a value segment tree.</p> <p>What interface should the value segment tree provide?</p> <ol> <li>Range +1/-1</li> <li>Calculate number of all non-zero position.</li> </ol> <p>Since we don't need general range query, we don't have to maintain tag pushdown. Also, since we always do a \\(+1\\) to an interval before we do the \\(-1\\), so we don't need to worry that the tag would be decreased to negative numbers.</p> <p>In this case, segment tree is more like a data structure that divide the entire intervals into several sub-intervals and use a node to represent each interval: $[0, n], [0, n/2], [n/2, n], [0, n/4], \\cdots $.</p> <p>Even though we have lots of sub-intervals, there are only linear number in total.</p> <p>The wonderful properpy is that for any interval \\([l, r]\\), we can divide it into no more than \\(O(\\log (r - l))\\) sub-intervals or i.e. nodes in segment tree.  </p> <p>In fact, similarly we can also divide the range \\([0, n]\\) into \\(\\sqrt{n}\\) sub-intervals. The idea is basically the same. s</p>"},{"location":"posts/ALGO/segment_tree/","title":"Segment Tree","text":"<p>Basic segment tree is quite easy to understand. </p>"},{"location":"posts/ALGO/segment_tree/#zkw-segment-tree","title":"ZKW Segment Tree","text":"<p>ZKW segment tree views the tree as a heap-like structure.</p> <p>We always assume there are \\(n = 2^p\\) for simplicity.</p> <p>In segment tree, the leaf node corresponding to \\(a[i]\\) is \\(i + n\\), where \\(n\\) is the size of array \\(a\\). Thus it's quite straightforward to modify/query on a single position.</p> <p>What about range query?</p> <p>Suppose we want to query interval \\([l, r]\\), then we set two sentinels \\(s_l, s_r\\) to be \\(l-1\\) and \\(r + 1\\).</p> <p>Move upward in segment tree from \\(s_l\\) and \\(s_r\\) and check:</p> <ul> <li>if \\(s_l\\) is the left child, then take into account the value of the right child (the sibling of \\(s_l\\));</li> <li>if \\(s_r\\) is the right child, then take into account the value of the left child (the sibling of \\(s_r\\));</li> </ul> <p>Here moving upward in segment tree means:</p> <pre><code>s_l = s_l / 2;\ns_r = s_r / 2;\n</code></pre> <p>If we further consider range modify, we still need a tag. But unlike classical segment tree, we don't need to pushdown these tags. During query, we move upward, and at this time we naturally add up all the tags along the way up to root.</p> <p>So the implementation could look like this:</p> <pre><code>void update_add(int l, int r, ll k) {\n\n  l = P + l - 1;\n  r = P + r + 1; // \u54e8\u5175\u4f4d\u7f6e\n  int siz = 1;   // \u8bb0\u5f55\u5f53\u524d\u5b50\u6811\u5927\u5c0f\n\n  while (l ^ 1 ^ r) { // \u5f53l\u4e0er\u4e92\u4e3a\u5144\u5f1f\u65f6\uff0c\u53ea\u6709\u6700\u540e\u4e00\u4f4d\u4e0d\u540c\n    if (~l &amp; 1)\n      tr[l ^ 1] += siz * k, sum[l ^ 1] += k;\n    if (r &amp; 1)\n      tr[r ^ 1] += siz * k, sum[r ^ 1] += k;\n    // \u7c7b\u4f3c\u9012\u5f52\u7ebf\u6bb5\u6811 tr[p] += tag[p]*(r-l+1)\n    l &gt;&gt;= 1;\n    r &gt;&gt;= 1;\n    siz &lt;&lt;= 1;\n    // \u6bcf\u6b21\u5411\u4e0a\u8d70\u65f6\u5b50\u6811\u5927\u5c0f\u90fd\u4f1a\u589e\u52a0\u4e00\u500d\n    tr[l] = tr[l &lt;&lt; 1] + tr[l &lt;&lt; 1 | 1] + sum[l] * siz; // \u7ef4\u62a4\u7236\u5b50\u5173\u7cfb\n    tr[r] = tr[r &lt;&lt; 1] + tr[r &lt;&lt; 1 | 1] + sum[r] * siz;\n  }\n  for (l &gt;&gt;= 1, siz &lt;&lt;= 1; l; l &gt;&gt;= 1, siz &lt;&lt;= 1)\n    tr[l] = tr[l &lt;&lt; 1] + tr[l &lt;&lt; 1 | 1] + sum[l] * siz; // \u66f4\u65b0\u4e0a\u4f20\u81f3\u6839\u8282\u70b9\n}\n\nll query_sum(int l, int r) {\n  l = l + P - 1;\n  r = r + P + 1;\n  ll res = 0;\n  int sizl = 0, sizr = 0, siz = 1; // \u5206\u522b\u7ef4\u62a4\u5de6\u53f3\u4e24\u4fa7\u5b50\u6811\u5927\u5c0f\n\n  while (l ^ 1 ^ r) {\n    if (~l &amp; 1)\n      res += tr[l ^ 1], sizl += siz; // \u66f4\u65b0\u7b54\u6848\u53ca\u5b50\u6811\u5927\u5c0f\n    if (r &amp; 2)\n      res += tr[r ^ 1], sizr += siz;\n    l &gt;&gt;= 1;\n    r &gt;&gt;= 1;\n    siz &lt;&lt;= 1;\n\n    res += sum[l] * sizl + sum[r] * sizr;\n    // \u5373\u4f7f\u5f53\u524d\u8282\u70b9\u6240\u5b58\u7684\u533a\u95f4\u548c\u4e0d\u9700\u8981\u7528\uff0c\u4f46\u56e0\u4e3a\u5176\u662f\u4e24\u4e2a\u54e8\u5175\u7684\u7236\u4eb2\u8282\u70b9\uff0c\u4e14 tag\n    // \u4e0d\u4f1a\u4e0b\u4f20\uff0c \u6240\u4ee5\u5176 tag \u4f1a\u5bf9\u7b54\u6848\u6709\u8d21\u732e\uff0c\u6240\u4ee5\u9700\u8981\u52a0\u4e0a tag \u7684\u8d21\u732e\n  }\n  for (l &gt;&gt;= 1, sizl += sizr; l; l &gt;&gt;= 1)\n    res += sum[l] * sizl; // \u7d2f\u52a0\u81f3\u6839\u8282\u70b9\n  return res;\n}\n</code></pre>"},{"location":"posts/ALGO/Competitive_Contest/DSU_On_Tree/","title":"DSU on Tree (Heuristic Merge on Tree)","text":""},{"location":"posts/ALGO/Competitive_Contest/DSU_On_Tree/#0-usage","title":"0 Usage","text":"<p>It could be used to solve some offline tree problem. Usually the time complexity would be \\(O(n\\logn)\\).</p>"},{"location":"posts/ALGO/Competitive_Contest/DSU_On_Tree/#1-idea","title":"1 Idea","text":"<p>Although it's called heuristic in Chinese, this algorithm can be rigorously proved to be efficient.</p> <p>The idea is quite simple: go through light child twice. </p> <p>Basically, tree-based algorithms always contain two parts: go through the child subtree; and merge the results of child subtrees.</p> <p>If a node always go through ever child subtree exactly once, then time complexity would be \\(O(n)\\); while merging results of child subtrees could be costy. DSU on Tree solves this kind of problem by seperate DFS child subtree and merging. If merging is a seperate task, then we can simply go through every node without maintaining anyting for child subtree.</p> <p>However if we sepearte these two tasks then we need one more pass for some child subtree, how could we control the cost? </p> <p>We call a child heavy if it's the one with the largest child subtree, otherwise we call it light. For each node we only sepearate the tasks for light children, which means we will have one more pass for each light child. </p> <p>Compared with classical DFS algo, the only difference is that light child would be gone through one more time, so each node \\(v\\) would be went through \\(l + 1\\) times, where \\(l\\) is the number of light child along the path from \\(v\\) to root of tree.</p> <p>There can't be more than \\(\\logn\\) light children in a path starting from root.</p> <p>Another conflict that might occur in common tree traversal algo is that if each needs an array to store the result, then each time we go into a child node, we either clear the array or create a new one for child. If each child has an array that's also catastrophic.</p> <p>While using DSU on Tree it's enough to have only one array, since each time we can manually clean the array.</p> <p>In summary we divide tasks into two parts:</p> <ol> <li>update array (+-1)</li> <li>[optional] update parent answer</li> </ol> <p>And DFS has an additional argument <code>keep</code> indicating whether to keep the array.</p> <p>Firstly we run non-keep DFS on light children.</p> <p>Then we run keep DFS on heavy child.</p> <p>Then for each light child we update answer and update array.</p> <p>Finally depending on current DFS is keep or non-keep, keep/remove current array value. Removal can be done also recursively.</p>"},{"location":"posts/ALGO/Competitive_Contest/KMP/","title":"KMP Algorithm","text":""},{"location":"posts/ALGO/Competitive_Contest/KMP/#what-is-kmp","title":"What is KMP?","text":""},{"location":"posts/ALGO/Competitive_Contest/Minimize_The_Number_of_Swaps_to_Sort_List/","title":"Minimize_The_Number_of_Swaps_to_Sort_List","text":""},{"location":"posts/ALGO/Competitive_Contest/Minimize_The_Number_of_Swaps_to_Sort_List/#1-intro","title":"1 INTRO","text":"<p>Consider there is a list \\(a[0] ... a[n]\\) containing \\(n\\) elements which needs to be sorted. Assume we can swap arbitraty two elements in one operations. What is the minimum number of operations we need to take to get this list sorted?</p>"},{"location":"posts/ALGO/Competitive_Contest/Minimize_The_Number_of_Swaps_to_Sort_List/#2-thinking","title":"2 THINKING","text":"<p>We can start from some simple cases. </p> <ul> <li>If we only have two elements in total, then we can easily sort them in at most one operation. </li> <li>If we have three elements in total, then there are multiple cases:<ul> <li>If the list is 3, 2, 1, then we only need one operation.</li> <li>If the list is 2, 3, 1, then we need two operations.</li> </ul> </li> <li>If we have four elements in total, the situation is slightly more complicated:<ul> <li>If the list is 2, 1, 4, 3, then we only need two operations.</li> <li>If the list is 2, 3, 4, 1, then we need three operations.</li> </ul> </li> </ul> <p>Seems that there is an obscure rule coming out. Let's compare the two cases where \\(n=4\\). In the first case, though each element is not in the correct plcae, we can solve two incorrectness by one operation because \\(a\\) is in the place where \\(b\\) should be, while \\(b\\) is in the place where \\(a\\) should be. However, in the second case, the four elements are in a ring, which means that each element is in the place where the next element should be, just like a circle. In such a circle with 4 elements, we need to take 3 operations; with 5 elements, we need 4 operations, which can be easily validated. So we can come up with a guess:</p> <p>For a circle with n elements, we need n - 1 operations to sort it.</p>"},{"location":"posts/ALGO/Competitive_Contest/Minimize_The_Number_of_Swaps_to_Sort_List/#3-proof","title":"3 PROOF","text":"<p>Actually, it's easier to prove a even stronger theorem:</p> <p>For a \\(n\\) elements list with \\(k\\) circles, we need \\(n-k\\) operations to sort it.</p> <p>Let's consider an extreme situation first. If a \\(n\\) elements list has \\(n\\) circles, then it is already sorted (we take a self-loop as a circle). So our target is actually to create as many circles as possible. Then how many circles can we create using one operation? At most one.  If we think of it as a directed graph where vertex represents for element, and directed edge \\((u, v)\\) represents for element \\(u\\) should be in the position which is currently occupied by element \\(v\\). In this graph, each vertex \\(u\\) has exactly one out-edge, of which the other endpoint is denoted by \\(out(u)\\). An operation in the list  swapping \\(u\\) and \\(v\\) could be seen as a swap of \\(out(u)\\) and \\(out(v)\\) in the graph, which means:</p> \\[ \\begin{aligned} out'(u) = out(v)\\\\ out'(v) = out(u) \\end{aligned} \\] <p>If \\(u\\) and \\(v\\) belongs to the same circle in the graph, then we will create a new circle. If \\(u\\) and \\(v\\) belongs to different circles, then we will eliminate one circle.  So, we could at most create one circle using one operation. Now that we have \\(k\\) circles initially, we still need \\(n-k\\) operations to get \\(n-k\\) more circles, and \\(n\\) circles in total. And eventually \\(n\\) circles in the graph means the list is sorted.</p>"},{"location":"posts/ALGO/Competitive_Contest/Sieve%20of%20Euler/","title":"Sieve of Euler","text":"<p>The essence of sieve of euler is that we divide each composite number into two parts: the first one the least prime divisor and the quotient.  Formally: $$ C = p_1 \\cdot \\Pi_{i=2}^{k} \\; p_i^{q_i} $$ But how can we do this? Easy. First, note that we use every number \\(x\\) to sieve its multiples, mo matter it is a composite number or prime numer; we multiply it with different prime numbers to sieve other numbers. Second, each time when \\(x\\) is divisible by the prime number, we break the <code>for</code> loop.</p> <p>This is the code: <pre><code>void prime_sieve(int n) {\n    for (int i = 2; i &lt;= n; i++) {\n        if (!noprime[i]) {\n            prime[cnt++] = i;\n        }\n        for (int j = 0; j &lt; cnt; j++) {\n            noprime[i * prime[j]] = 1;\n            if (i % prime[j] == 0) {\n                break;\n            }\n        }\n    }\n}\n</code></pre></p>"},{"location":"posts/ALGO/Competitive_Contest/Strongly%20Connected%20Component/","title":"Strongly Connected Component","text":""},{"location":"posts/ALGO/Competitive_Contest/Strongly%20Connected%20Component/#1-dfs-tree","title":"1 DFS Tree","text":"<p>First, we introduce <code>DFS tree</code> so as to help us understand the basic structure of strongly connected component.</p> <p>![[Pasted image 20230911122608.png]]</p> <p>As is shown in this graph, there are four different types of edges, naming:</p> <ol> <li>Tree edge: the black one, indicating the edge in the DFS process.</li> <li>back edge: the red one, indicating that the destination node is an ancester.</li> <li>cross edge: the blue one, indicating that the destination node is not in the stack</li> <li>forward edge: the green one, indicating that the destination node is in the stack but not an acester.</li> </ol> <p>Here we used a stack, but that's not exactly the same as the DFS stack, which is quite similar though. We will explain this stack later on.</p>"},{"location":"posts/ALGO/Competitive_Contest/Strongly%20Connected%20Component/#2-intution","title":"2 Intution","text":"<p>Since we want to find out the strongly connected component, which is a seemingly tough problem, we can try to find a representative in this component instead. Let's think about it: which vertex is the most special in a component so as to become a representative? The one with the lowest depth in the DFS tree (we can actually easily prove that there will only be one vertex with the lowest depth, left for readers).</p> <p>WLOG, we call that vertex father vertex in the component. Our task transfered to finding the father vertex because once we find it, we can determine that all the vertices still in the stack belong to the corresponding component of that father vertex.</p> <p>Then, let's think about the property of father vertex, i.e. how to determine whether a vertex is a father vertex or not. Since father vertex has the lowest depth by definition,  if we can matintain the lowest depth that a vertex could reach (just a rough definition for temporary usage), then a vertex with its own depth the same as the depth that it could reach would be a father vertex.</p> <p>Now let's define all the stuff rigorously. </p>"},{"location":"posts/ALGO/Competitive_Contest/Strongly%20Connected%20Component/#3-definition","title":"3 Definition","text":"<p>[!tip] </p>"},{"location":"posts/ALGO/Competitive_Contest/CF/cf1672H/","title":"\u9898\u89e3 CF1672H","text":"<p>\u611f\u89c9\u8fd9\u9053\u9898\u5f88\u6709\u8da3\uff0c\u8bb0\u5f55\u4e00\u4e0b\u3002</p> <p>\u770b\u5b98\u65b9\u7b54\u6848\u6ca1\u592a\u770b\u61c2\uff0c\u6839\u636e\u7075\u795e\u7684\u89e3\u7b54\u60f3\u660e\u767d\u4e86\u600e\u4e48\u8bc1\u660e\u3002</p> <p>\u9996\u5148\u53ef\u4ee5\u8bc1\u660e\u6bcf\u6b21\u79fb\u9664\u7684\u90fd\u662f\u4e00\u4e2a\u5b8c\u6574\u7684 alternative substring\u3002\u79fb\u9664\u4e0d\u5b8c\u6574\u7684\u6ca1\u6709\u4efb\u4f55\u597d\u5904\u3002</p> <p>\u9996\u5148\u6211\u4eec\u8003\u8651\u6240\u6709\u7684 00\uff0c11 \u7684\u4e2a\u6570\uff08\u5141\u8bb8\u91cd\u53e0\uff09\u3002\u6bd4\u5982 000 \u91cc\u6709\u4e24\u4e2a 00\u3002</p> <p>\u5982\u679c\u573a\u4e0a\u6709 k \u4e2a 00\uff0c0 \u4e2a 11\uff0c\u90a3\u4e48\u5f88\u663e\u7136\u9700\u8981 k+1 \u6b21\u64cd\u4f5c\u3002 \u5982\u679c\u573a\u4e0a\u6709 k \u4e2a 00\uff0ct \u4e2a 11\uff0c\u90a3\u4e48\u6211\u4eec\u8003\u8651\uff1a</p> <ol> <li>\u5982\u679c\u79fb\u9664\u7684\u662f\u4e00\u4e2a\u5076\u6570\u957f\u5ea6\u7684 alt string\uff0c\u90a3\u4e48\u79fb\u51fa\u540e 00 \u548c 11 \u5404\u51cf\u5c11 1 \u4e2a\u3002</li> <li>\u5982\u679c\u79fb\u9664\u7684\u662f\u4e00\u4e2a\u5947\u6570\u957f\u5ea6\u7684 alt string\uff0c\u90a3\u4e48\u79fb\u51fa\u540e 00 \u6216 11 \u51cf\u5c11 1 \u4e2a\uff08\u5176\u5b9e\u662f\u51cf\u5c11 2 \u4e2a\uff0c\u4f46\u662f\u524d\u540e\u53c8\u62fc\u51fa 1 \u4e2a\uff09</li> <li>\u5982\u679c 00 \u548c 11 \u7684\u4e2a\u6570\u90fd\u4e0d\u4e3a 0\uff0c\u90a3\u4e48\u4e00\u5b9a\u5b58\u5728\u4e00\u4e2a\u5076\u6570\u957f\u5ea6\u7684 alt string\u3002\u8fd9\u662f\u56e0\u4e3a\u8003\u8651\u4efb\u610f\u4e24\u4e2a 00 11\uff0c\u5b83\u4eec\u4e2d\u95f4\u5fc5\u987b\u6709\u81f3\u5c11\u4e00\u4e2a\u5076\u6570\u957f\u5ea6\u7684 alt string\uff0c\u5426\u5219\u4e0d\u53ef\u80fd\u4ece 0 \u53d8\u6210 1 \uff08\u5947\u6570\u957f\u5ea6\u7684 alt string \u9996\u5c3e\u662f\u4e00\u6837\u7684\uff09</li> </ol> <p>\u57fa\u4e8e\u4ee5\u4e0a\u4e09\u70b9\u6211\u4eec\u4e0d\u96be\u53d1\u73b0\uff0c\u7b54\u6848\u5e94\u8be5\u662f max(<code>#00</code>, <code>#11</code>) + 1\u3002</p>"},{"location":"posts/About/","title":"About","text":""},{"location":"posts/About/#1-who-i-am","title":"1 Who I AM","text":"<p>Teng Liu (Tony Newton), bachelor's degree @ Zhejiang University, currently a master student @ ETHz.</p>"},{"location":"posts/About/#2-why-creating-this-dumb-site","title":"2 Why Creating This Dumb Site","text":"<p>Record, Review, Recycle</p>"},{"location":"posts/About/#3-why-in-english","title":"3 Why in English","text":"<p>Force me to write something in English T_T</p>"},{"location":"posts/About/Bachelor%20Courses%20Description/","title":"Bachelor Courses Description","text":"<p>This is all the courses that I have taken as undergraduate at Zhejiang University.</p> <p> Compulsory Courses </p>"},{"location":"posts/About/Bachelor%20Courses%20Description/#linear-algebraa","title":"Linear Algebra(A)","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 3.5</li> <li> <p>Description:</p> <p>This course is a degree program for undergraduate students of Zhejiang University whose majority is in the field of engineering and science, etc. It includes the theories of solving linear equations, the basic theory of matrix, linear space and quadratic form, and an introduction to the theory of linear transformation, which are very useful in science and technology.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#colledge-english-iv","title":"Colledge English IV","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 3</li> <li> <p>Description:</p> <p>This course aims at developing English skills for college students in listening, speaking, reading, writing, and translating. It is also intended to cultivate students\u2019 comprehensive ability to acquire knowledge, to implement positive thinking, and to use English in a proficient way. Student-centered and theme-based teaching methods such as reading, listening, viewing, group discussions, presentations and other classroom activities will be adopted. The themes are relevant and interesting to the students so as to meet their intellectual, emotional and individual needs.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#colledge-english-v","title":"Colledge English V","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 3</li> <li> <p>Description:</p> <p>College English V is an advanced course in the college English curriculum for students at Zhejiang University. It aims at developing students\u2019 ability to use English in a well-rounded way, especially in speaking and writing, so that in their future studies and careers as well as social interactions they will be able to communicate effectively, and at the same time enhance their ability to study independently and improve their general cultural awareness so as to fulfill the needs of China\u2019s social development and international exchanges. A combination of multimedia- and classroom-based teaching models will be applied, so that English language teaching and learning will be geared towards students\u2019 individualized and autonomous learning.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#calculusa-iii","title":"Calculus(A) I/II","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 2</li> <li>Credit: 5</li> <li> <p>Description:</p> <p>Calculus is a mathematical subject, which studies the functions, applies the method of limits (i.e. limit process like infinitesimals and infinite approximation) to analyze and deal with issues, with 96 class hours. The teaching content includes: function limits and continuity, differential calculus of one variable functions and its appliances, integral calculus of one variable functions and its appliances. The Methods of discussion and case study will be used in this course, and the capacity of quick-primary policy analysis will be emphasized.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#probability-and-mathematical-statistics","title":"Probability and Mathematical Statistics","text":"<ol> <li>Hours Per Class: 1.5</li> <li>Classes Per Week: 2</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>This is an important basic course for undergraduate students. The main theme of the subject is the study of the quantitative patterns of \"random phenomena\", including events and probability, random variables and their distributions, the numeric characters of random variables, the law of large numbers, and the central limit theorem, statistical quantities and sampling distribution, the point estimation and interval estimation of parameters, the hypothesis testing of parameters and the fitting testing of probability distribution, variance analysis and regression analysis.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#university-physicsa-i","title":"University Physics(A) I","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>University Physics A1 includes the following contents: Newton's mechanics on calculus level, rotational motion of rigid body, theory of relativity; oscillations and wave motion, thermodynamics; foundation of electrostatics. The learning of the course enables students to gain a comprehensive understanding of the principles of the moving of objects, giving an initial training of students\u2019 methods of thinking and capabilities to investigate problems scientifically. It cans also lay students a solid physical foundation for further learning (for Polytechnic postgraduates) and for learning new theories, new knowledge and new technologies.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#university-physicsa-ii","title":"University Physics(A) II","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>University Physics A2 includes the following contents: electromagnetism, optics; the foundation of quantum physics; selected lecture of the advance front in physics. The learning of the course enables students to gain a comprehensive understanding of the principles of the moving of objects, giving an initial training of students\u2019 methods of thinking and capabilities to investigate problems scientifically. It cans also lay students a solid physical foundation for further learning (for Polytechnic postgraduates) and for learning new theories, new knowledge and new technologies.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#discrete-mathematics-and-application","title":"Discrete Mathematics and Application","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>Discrete mathematics is a subject to study discrete objects and their relationship, which is different from continuous objects based on real numbers. The research contents of discrete mathematics mainly includes integer, graph theory, set theory, mathematical logic, combinatorial theory, group theory, algorithm and other subjects with very continuous variables. In this sense, discrete mathematics mainly studies the mathematical branch of object properties on countable sets, and its research object is the mathematical model of discrete quantitative relationship and discrete structure.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#introduction-to-computing-systems","title":"Introduction to Computing Systems","text":"<ol> <li>Hours Per Class: 2.5</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>This is the first course in computing for students of computer engineering and electrical engineering. The objective is to provide a strong foundation that a serious student can build on in later courses across the spectrum of computer science and engineering. The idea is that a more complete understanding of the fundamentals early in your education will help you acquire a deeper understanding of more advanced topics later, whether that topic is in computer architecture, operating systems, data base, networks, algorithm design, software engineering, or whatever. I call the approach \"motivated\" bottom-up. That is, after providing some overview of why a new concept is important, we attempt to tie that new concept to what you already understand. Starting with the transistor as a switch, we build logic gates, then more complex logic structures, then gated latches, culminating in an implementation of memory. From there, we study the computer's instruction cycle, and then a particular computer, the LC-3 (for Little Computer 3). We got it wrong the first couple of times! The LC-3 captures the important structures of a modern computer, while keeping it simple enough to allow full understanding. The first programming assignment is in the machine language of the LC-3. From there, we move up to Assembly Language, and learn how an assembler works. The remaining programming assignments are in LC-3 Assembly Language. We cover good programming style and practice, and teach debugging from the gitgo. An LC-3 Simulator allows the student to debug his/her own programs. Input (via the keyboard) and output (via the monitor) both use physical device registers. System service routines, written in LC-3 Assembly Language are used to perform I/O functions. They are invoked by user programs by the TRAP instruction and corresponding trap vector. Subroutine calls and returns complete the LC-3 instruction set.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#fundamentals-of-c-programming","title":"Fundamentals of C Programming","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 2</li> <li>Credit: 3</li> <li> <p>Description:</p> <p>This course will mainly introduce to the students the C programming language and its programming technology, and explain the basic algorithms of solving the problems. This course aims to make the students know the components of the high level programming language, master the basic processes and skills of programming, possess the basic abilities of programming in high level programming languages. The main contents of this course include: the data types and expressions, the basic flow-controlling of the programs, the functions and modular programming, the data array and file applications, the basic alogrithms, etc. </p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#lectures-on-programming","title":"Lectures on Programming","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 1</li> <li>Credit: 2</li> <li> <p>Description:</p> <p>Based on the course \u201cFundamentals of C Programming\u201d, this course will introduce five special topics on C Programming. Through study and systematic practices of these topics, students\u2019 engineering thinking is trained, and their data organization ability, structured programming ability and problem-solving ability are further improved. The four special topics include: modular programming, advanced pointers, linked list, graphic programming foundation and advanced file topic.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#fundamentals-of-data-structures","title":"Fundamentals of Data Structures","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 1</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>This course investigates the definitions, implementations, and functions related to non-numerical data objects. The content of this course consists of the basic methods for time-space complexity analysis; fundamental data structures for stack, queue, list, tree and graph; implementations and analysis of sorting and searching. Students are supposed to learn how to organize data, to represent problems in a computer, to select the optimal data structure and algorithm for a specific problem, and hence improve their ability of programming.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#advanced-data-structure--algorithm-analysis","title":"Advanced Data Structure &amp; Algorithm Analysis","text":"<ol> <li>Hours Per Class: 2.5</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>This course is based on the fundamentals of data structures.  It continues to investigate the definitions, implementations, and algorithms related to non-numerical data objects.  The content of this course consists of two parts: the first part is for advanced data structures, such as the variations of binary search trees and the inverted file index for searching big data sets, and various optimizations of the priority queues and the amortized analysis; the second part is for classical algorithms, such as divide and conquer, dynamic programming, greedy, back tracking, together with approximation methods, local search, and randomized algorithms.  Parallel algorithms and external sorting will be introduced as well.  Students are supposed to learn how to solve complicated problems with advanced programming skills, and how to give the performance a mathematical analysis, and hence build a firm foundation for studying further theories in computer science.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#digital-logic-design","title":"Digital Logic Design","text":"<ol> <li>Hours Per Class: 2.5</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>The aim of the course is to introduce basic theory and design methods for digital logic. The course covers number representation, digital codes, boolean algebra and logic minimization techniques, sources of delay in combinational circuits and effect on circuit performance, survey of common combinational circuit components, sequential circuit design and analysis; timing analysis of sequential circuits, concept of programmable logic devices and memories. The language used in the course can be English.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#computer-organization","title":"Computer Organization","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 2</li> <li>Credit: 4.5</li> <li> <p>Description:</p> <p>The main purpose of this course is a systematic study of computer organization. The emphasis is the analysis of the operating principles of the computer components and the concepts, design and implementation technology of the software and hardware interface. Through the analysis and design of CPU to make the students deep understand the computer system and the process how hardware support software\u2019s execution. And the students can also master how to solve the problems of computer systems. This course emphasizes on the required software/hardware interfaces and the modern computer organization from the perspective of the programmers and optimization including the introduction to computer organization, machine language, assembly language, analysis and design of arithmetic/logic operation functions and units, principles of computer organization, design and implementation of computer system, principles of memory structure, principles of I/O interfaces and peripheral devices, the bus, the basic interactive modes of polling/interrupt/DMA, etc..</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#computer-architecture","title":"Computer Architecture","text":"<ol> <li>Hours Per Class: 2.5</li> <li>Classes Per Week: 2</li> <li>Credit: 3.5</li> <li> <p>Description:</p> <p>This course is one of the most important professional courses in computer science that systemically introduce the fundamental concepts and design approches of computer architecture from the view of the whole computer system. The topics cover fundamental concepts, task of computer design, quantitative principles, and performance evaluation; instruction set architecture and characteristics of CISC and RISC machines; basic concepts for pipelining, causes and resolutions for pipeline hazards; memory hierarchy, improvement of cache performance; I/O storages; and basic concepts for multiprocessors. At the same time, the students are required to master the hardware design approches and skillfully use hardware design toolkits. In the lab we will introduce how to gradually implement the pipelined CPU supporting 31 MIPS instructions in Xilinx ISE environment using Verilog, and verify its correctness on FPGA board.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#database-system","title":"Database System","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>Database is the cornerstone of modern information society. The course introduces the principles and techniques of database systems, including the relational data model, relational database standard language SQL, database design and entity-relationship data model, relational formalization, database application programming, object-relational model, XML\uff0cphysical storage, index, query processing and optimization, concurrency control and discovery. Students are expected to have fully understanding of the basic concepts of database systems and DBMS implementation techniques\uff0cand be equipped with the ability to administrate DBMS and develop database applications.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#object-oriented-programming","title":"Object-Oriented Programming","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>Students will grasp the concepts and method of object-oriented programming through the study of principle and practice. They will be required to build good programming style which accord with modern software design. Students will grasp C++ language skillfully, basically know how it runs, be able to do object-oriented programming using C++ skillfully. Totally, this course will guide students to have the ability of software analysis, design and coding, as well as the ability of cooperate developing.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#operating-system","title":"Operating System","text":"<ol> <li>Hours Per Class: 4</li> <li>Classes Per Week: 2</li> <li>Credit: 5</li> <li> <p>Description:</p> <p>Operating system isprofessional core course of both theoretical and practical.  This course is to help students understand the role of operating system in a computer system, and learn how to apply the basic concepts, methods, algorithms and technologies of operating system when managing the resources of software and hardware.  The content of this course consists of introduction, process management, memory management, file system management, I/O system, and some selected advanced topics.  Students are supposed to Computational Thinking ability\uff0calgorithm analysis and design ability, large scale software design and implementation  ability , and computer software and hardware system  cognition\u3001 analysis\u3001design and application ability</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#theory-of-computation","title":"Theory of Computation","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 2</li> <li> <p>Description:</p> <p>This is a theoretical course designed for graduated students. There are 32 academic hours in total. This course covers the theory of automata and formal languages, computability by Turing machines and recursive functions, uncomputability. This term is devoted to establishing a foundation for the formal study of computation. This foundation consists of tools from mathematics such as set theory, logic, and graph theory, and concepts from theoretical computer science, such as formal languages, abstract machines.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#compiler-principle","title":"Compiler Principle","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 2</li> <li>Credit: 4</li> <li> <p>Description:</p> <p>This course investigates principles, techniques and tools to design and implement a compiler. The content of this course consists of preprocessor, scanner, parser, symbol table. Code generator, semantic analyzer and target code optimizer. Students are assumed to learn the following fundamentals: regular expression, automation, context-free grammars and etc, and they are supposed to grasp the lexical analyzer basing automation, the parser of top-down parsing, bottom-up parsing and the syntax-directed translation.  This course enables students to achieve strong  abilities for developing the computer system software.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#software-engineering","title":"Software Engineering","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>\"Software Engineering\" guides the students to understand the significance of some fundamental concepts of software engineering. The frame works are introduced, such as software process models, software engineering methods and tools, and software management. Both conventional methods and object-oriented methods are discussed. Projects are assigned to help students experience the life-cycle of a software during practice, including requirement analysis, system design, component-level design, coding, testing, maintenance, and team work.  Students will learnt to use conventional tools such as data flow diagrams, data dictionary, entity-relation diagrams, and system hierarchy; as well as object-oriented tools such as use-cases, even trace diagrams, state transition diagrams and CRC cards.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#computer-networks","title":"Computer Networks","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 3</li> <li>Credit: 4.5</li> <li> <p>Description:</p> <p>The main task of this course is to study the basic theory and professional knowledge related to computer networks. From the perspective of network architecture, network principles and related fundamental concepts and methods in digital communication, internetworking and advanced protocols are introduced in this course. Some significant functions and relative protocols in network architecture together with recent developments of network and their technologies are also introduced. Students can get familiar with fundamental computer network principles and basic knowledge of the field through the course learning. And a thorough understanding of commonly used network and network security technology can be obtained with intimate knowledge of network architecture, functional principles and various of network protocols (especially that of TCP/IP).</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#communication-skills-in-information-technology","title":"Communication Skills in Information Technology","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 2</li> <li> <p>Description:</p> <p>Being able to communicate effectively with computers is an important skill to people in the field of information technology.  On the other hand, just as important is being able to communicate effectively with technical and non-technical colleagues. This course is aimed to train their communication skills when the students have to face people in the field of information technology.  With a series of case studies and practices, the students are supposed to learn the principles and practical skills on how to express their own opinions, communicate with their teammates, search for references, read effectively, write technical documents, and give presentations.  With this course we hope to improve the information organization and communication skills of the students in the field of information technology.</p> </li> </ol> <p> Elective Courses </p>"},{"location":"posts/About/Bachelor%20Courses%20Description/#introduction-to-artificial-intelligence","title":"Introduction to Artificial Intelligence","text":"<ol> <li>Hours Per Class: 3</li> <li>Classes Per Week: 1</li> <li>Credit: 3.5</li> <li> <p>Description:</p> <p>In this course, we will introduce some basic concepts, history and the current status of artificial intelligence, but focus on the fundamental theory, method and important algorithms of AI and machine learning. From this course, students are supposed to grasp the AI\u2019s fundamental connotations, and further get a deep insight into the AI\u2019s key components from an interdisciplinary perspective, resulting in the in-depth understanding of numerous key points such as supervised/unsupervised learning, linear/nonlinear manifold learning, probabilistic topic modeling, deep neural networks, reinforcement learning, etc.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#introduction-to-applied-operations-research","title":"Introduction to Applied Operations Research","text":"<ol> <li>Hours Per Class: 4</li> <li>Classes Per Week: 1</li> <li>Credit: 3.5</li> <li> <p>Description:</p> <p>This course aims at decision models and solving methods for optimization problems under various constraints. The topics cover linear programming, nonlinear programming, integer programming, combinatorial optimization and algorithmic game theory. Students will learn from the course the fundamental theory and methods of operations research, and improve their abilities in solving and analyzing optimization problems.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#algorithm-design--analysis","title":"Algorithm Design &amp; Analysis","text":"<ol> <li>Hours Per Class: 1.5</li> <li>Classes Per Week 2</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>Algorithm design and analysis is a fundamental  course in computer science, which mainly talks about the theoretical foundation of  algorithms. This course covers sorting and selection, max-flow, matching, computational bioinformatics, machine learning, randomized algorithms, computational complexity and inapproximability, and cake cutting protocols. .</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#image-analysis-and-artistic-processing","title":"Image Analysis and Artistic Processing","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>This course will introduce some basic concepts and some advanced image processing such as image editing and synthesis. At the same time, this course will introduce the current status, most excited advance and unresolved challenge. From this course, students should know global visual features (such as color, texture and shape) and local visual features (such as SIFT and visual words), similarity computation, image signal processing (convolution, filtering and coding), high dimension reduction, image blending and morph, artistic styles rendering.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#great-ideas-in-computer-science","title":"Great Ideas in Computer Science","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 0</li> <li>Credit: 2</li> <li> <p>Description:</p> <p>This course will take a philosophical and historical perspective on the development of computer science, especially since 1966 Turing award winner\u2019s life, scientific thinking, and outstanding achievements and so on. The concrete content includes: the history of some research fields in computer science: tracking development in some research fields in computer science, revealing its scientific thought; The masters\u2019 thought on computer science: introducing some great computer scientists\u2019 life, and achievements, expounding the basic exploration and discovery in computer science, and sharing the future views. The improvement of classic problem in computer science: developing the solution to some classic problems, leading the students to carry on the deep reading, revealing the idea and its influence of the future. The course can make the students to understand the history of computer science, great scientific event and so on, and to understand the essence behind the computer science, to dedicate to their scientific research.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#numerical-analysis","title":"Numerical Analysis","text":"<ol> <li>Hours Per Class: 2.5</li> <li>Classes Per Week: 1</li> <li>Credit: 2.5</li> <li> <p>Description:</p> <p>Problems solving by numerical methods has now become one of the fundamental means of research in the physical sciences and engineering, and also in social sciences and humanities.  This course is an introduction to numerical computation employed so widely in industry and research.  We will discuss errors in numerical computation, roots of nonlinear equations, systems of linear equations, algebraic eigenvalue problems, function approximations by polynomial interpolation and cubic spline interpolations, quadratures, and numerical differentiation.  We will explain how, why, and when these methods can be expected to work efficiently.  Students will learn from lectures and a series of projects the basic theories and how to program for solving practical problems.  This course is to provide a firm basis for future study of scientific computing, to steady a solid foundation in mathematics, and to train the students how to conduct scientific research.</p> </li> </ol>"},{"location":"posts/About/Bachelor%20Courses%20Description/#wireless-network-application","title":"Wireless Network Application","text":"<ol> <li>Hours Per Class: 2</li> <li>Classes Per Week: 1</li> <li>Credit: 1.5</li> <li> <p>Description:</p> <p>This is a general education course for the undergraduate students. It utilizes the innovation teaching patterns of online intelligent study and offline exploratory experiments, proceeds from the wireless network application demands in the daily life, and through selected study and practice of the newest network entry online curricula from the world famous Cisco Networking Academy, in the wireless and wired computer network experiment environments, by easy-to-understand remote demo teaching methods, it makes the different discipline students of liberal arts, science, engineering, agricultural, and medical to quickly and basically grasp many kinds of practical techniques of network access and network communications using wireless and wired networks, which mainly include: configuration and application of the common wireless and wired network equipment such as intelligent wireless routers and wireless APs, configuration and application of many common network services, wireless networking technique application, common problem-solving techniques related to the network security and the network troubleshooting. This is a very skilled general education course, which is experiment-centered and theory-assisted. It emphasizes the training of the hands-on abilities and the capability of solving practical problems. The teaching methods of in-class lecture and in-lab experiment are used in this course. The course assessment methods consist of experiment inspection, lab reports, course project design, and open-book online exam.</p> </li> </ol>"},{"location":"posts/About/Master%20Courses/","title":"Master Courses","text":"<p>Here are the courses I've taken in ETH Zurich:</p>"},{"location":"posts/About/Master%20Courses/#algebraic-method-in-combinatorics","title":"Algebraic Method in Combinatorics","text":"<p>Focusing on algebraic way to interpret and solve combinatoric problems. There are some problems and topics particularly interesting, e.g.: odd-town even-town problem, finite field Kakeya set problem, convex geometry, spectral graph theory, and Chavelly Warning theorem, Combinatorial Nullstellenz theorem, and their applications.</p> <p>\u72e0\u72e0\u590d\u4e60\u4e86\u4e00\u4e0b\u4ee3\u6570\uff0c\u4e86\u89e3\u5230\u4e86\u4e00\u4e9b\u66f4*\u73b0\u4ee3*\u4e00\u4e9b\u7684\u7ec4\u5408\u95ee\u9898\uff0c\u975e\u5e38\u6709\u610f\u601d\u7684\u8bfe\u3002</p>"},{"location":"posts/About/Master%20Courses/#zero-knowledge-proof","title":"Zero Knowledge Proof","text":"<p>\u7b14\u8bb0\u5728 crypto \u9875\u9762\uff0c\u6682\u65f6\u8fd8\u6bd4\u8f83\u51cc\u4e71\uff0c\u4e2d\u82f1\u6df7\u6742\u3002\u8fd1\u671f\u4f1a\u4fee\u6539\u597d\u6216\u8005\u5f55\u4e00\u5957\u89c6\u9891\u3002</p>"},{"location":"posts/About/Master%20Courses/#information-theory","title":"Information Theory","text":"<p>\u7ecf\u5e38\u4f1a\u7528\u5230\u4fe1\u606f\u8bba\u4e2d\u7684\u4e00\u4e9b\u5b9a\u4e49\uff0c\u603b\u7b97\u7cfb\u7edf\u5b66\u4e60\u4e86\u4e00\u4e9b\u57fa\u7840\u77e5\u8bc6\u3002\u4e3b\u8981\u4ecb\u7ecd\u4e86\u71b5\u3001\u6761\u4ef6\u71b5\u3001\u4e92\u4fe1\u606f\u7b49\u7b49\u6982\u5ff5\u7684\u5b9a\u4e49\u548c\u4fe1\u9053\u7f16\u7801\u5b9a\u7406\u3002\u540e\u534a\u7a0b\u57fa\u672c\u5168\u5728\u4ecb\u7ecd\u5b9a\u7406\u7684\u8bc1\u660e\u3001\u5e94\u7528\u3001\u62d3\u5e7f\u3002</p>"},{"location":"posts/CPP/","title":"CPP","text":"<p>Trying to be a C++ lawyer.</p>"},{"location":"posts/CPP/CRTP/","title":"CRTP","text":""},{"location":"posts/CPP/CRTP/#what-is-crtp","title":"What is CRTP","text":""},{"location":"posts/CPP/CRTP/#_1","title":"CRTP","text":""},{"location":"posts/CPP/Concurrency/","title":"C++ Concurrency in Practice","text":""},{"location":"posts/CPP/Concurrency/#chapter-3","title":"Chapter 3","text":""},{"location":"posts/CPP/Concurrency/#31-race-condition","title":"3.1 Race Condition","text":"<p>A race condition is a situation when the ordering of execution of different threads affects the final outcome of the program.</p>"},{"location":"posts/CPP/Further%20Read/","title":"Further Read","text":"<p>Linker Stuff dynamic link x86 guide</p>"},{"location":"posts/CPP/memory_order/","title":"Memory Order","text":"<p>Let's talk about memory order.</p> <p>I've learnt about this for the fourth time and each time I thought I fully understood everything and wouldn't forget anything. While it turns out that this is not the case.</p> <p>Firstly, in case in the future I still forgot everything and this blog doesn't help, here is the blogs that I read, I should be able to recover from these articles:</p> <ul> <li>https://preshing.com/20120930/weak-vs-strong-memory-models/</li> </ul> <p>Actually this is a series of articles all about memory models.</p> <ul> <li>https://www.cl.cam.ac.uk/~pes20/weakmemory/</li> </ul> <p>Some tools, papers.</p> <ul> <li>https://github.com/27rabbitlt/memory_order_test_demo</li> </ul> <p>Simple test demos.</p> <p>Alright then, let's officially start.</p>"},{"location":"posts/CPP/memory_order/#1-what-is-memory-order","title":"1 What is memory order?","text":"<p>Memory order, by its name, is the order of access to memory by CPU. </p> <p>Let's firstly talk about the case where there is only one CPU.</p> <p>To increase throughput and reduce latency, it's understandable to re-order instructions. It can be done by compilers and the CPU.</p> <p>For the compiler side, a piece of code written in high-level language serves the functionality to help human-beings understand and maintain the code, so the order of statements are meant to be human-readable instead of maximize performance. Thus compiler will try to swap some statements and improve performace.</p> <p>For the CPU side, compiler would never have full access of every details of CPU so it's still up to CPU to re-re-order the instructions. Note that memory re-order is not just caused by instruction re-order, it can involve read-write re-order done by memory.</p> <p>No matter what ordering compiler and CPU are taking, the basic rule is that: The result should be the same. While, at least the CPU that performs re-ordering believes that the result should be the same if no memory address is shared with other CPUs.</p> <p>The truth is, in practice we tend to have multiple threads which share some specific meomory with each other. Now the problem arises: what if the re-ordering done by CPU A affects the result of CPU B?</p> <p>Here is a very simple demo:</p> P1 P2 X = 1 if ready_X: read X ready_X = true <p>Assume before P1 sets X to be 1, the value of X is just garbage that we don't know.</p> <p>Then for P1, if there is only one CPU, then the result won't change if we swap the instructions <code>X = 1</code> an <code>ready_X = true</code>. It's impossible for P1 to know that somewhere on another CPU there is a piece of code <code>if ready_X: read X</code>.</p> <p>So it's possible that P1 swaped <code>X = 1</code> an <code>ready_X = true</code> and P2 sees <code>ready_X</code> is true but get garbage value from <code>X</code>.</p>"},{"location":"posts/CPP/memory_order/#memory-barrier","title":"Memory Barrier","text":"<p>Now memory re-ordering seems to be a bad thing, it might cause the wrong result!</p> <p>However, if we stick to sequential-order, i.e. we don't allow any kind of reordering and the whole program runs as if each thread is executed sequentially, we also cannot accept the performance.</p> <p>Here is where memory barrier comes in.</p> <p>Memory barrier prevent compiler and CPU to do certain types of re-ordering so as to make sure the result is still correct and meanwhile we can have as less restrictions for CPU as possible to get best performance.</p> <p>Typically, there are four kinds of re-ordering: LoadLoad, LoadStore, StoreLoad, StoreStore. So in the same way, we can have four types of barriers. Notice that these four types are just conceptual, they do not really exist. In practice, barriers are often some combinations of these four types.</p> <p>We always see something like acquire-release in C++ atomics. Acquire can be roughly described as the combination of LoadLoad and LoadStore barrier; and Release can be described as the combination of LoadStore and StoreStore barrier.</p>"},{"location":"posts/CPP/memory_order/#memory-model-of-cpu","title":"Memory Model of CPU","text":"<p>Granted, there are 4 types of re-ordering, but not all CPUs allow all 4 re-ordering. The restrictions of re-ordering is called the memory model of CPU.</p> <p>We discuss models from the weakest to the strongest.</p> <p>A classical example of weak memory model is: https://www.cs.umd.edu/~pugh/java/memoryModel/AlphaReordering.html. </p> <p>Weakest memory model allows all kinds of re-ordering.</p> <p>ARM has stronger model, but still classified as weak-order model.</p> <p>It guarantees order with data dependencies. For example, the expression <code>A-&gt;b</code>, it's guaranteed that <code>b</code> is at least as new as <code>A</code>.</p> <p>x86 is a strong-order model. None of LoadLoad, LoadStore, or StoreStore are allowed, which means basically instructions on x86 themselves are using acquire-release semantic naturally.</p> <p>But, StoreLoad re-ordering is still possible: https://github.com/27rabbitlt/memory_order_test_demo/blob/main/test_arm_store_load_reorder.cpp</p> <p>Another interesting demo that shows some re-ordering won't happen on x86 but will on ARM is: https://github.com/27rabbitlt/memory_order_test_demo/blob/main/test_arm_store_load_reorder.cpp</p> <p>Let's look at this snippet:</p> <p><pre><code>void increase1000000() {\n    int count = 0;\n    while (count &lt; 10000000) {\n        int expected = 0;\n        if (flag.compare_exchange_strong(expected, 1, std::memory_order_relaxed)) {\n            sharedVar++;\n            flag.store(0, std::memory_order_relaxed);\n            count++;\n        }\n    }\n}\n</code></pre> When two threads executing this function at the same time on ARM platform, Apple Silcon for example, it's possible that the final result of <code>sharedVar</code> is not 2000000.</p> <p>Here <code>sharedVar</code> is a shared variable and <code>flag</code> is an atomic integer.</p> <p>This piece of code intends to achieve the same functionality as a mutex lock without using any kind of lock. So here flag is like a mutex variable, and it is partially true that at the same time there will be only one thread to be able to get in this <code>if</code> statement. </p> <p>Wait, it's only partially true, why?</p> <p>In fact, it's possible for ARM CPU to swap the Store of <code>flag.store(0)</code> and <code>sharedVar++</code>. So it might happen, though with low probability, that <code>flag</code> has already been set to 0 but the shared variable hasn't been changed, thus the modification of shared variable might be covered.</p>"},{"location":"posts/CPP/UB/infinite_loop/","title":"Infinite Loop","text":""},{"location":"posts/CPP/UB/infinite_loop/#is-infinite-loop-undefined-behavior","title":"Is infinite loop undefined behavior?","text":"<p>I saw this interesting meme in QQ Zone:</p> <p></p> <p>I tested in Godbolt and got exactly the same result:</p> <p></p> <p>At the first glance, I thought this would be a bug for compiler since optimizing away a infinite loop is somewhat unacceptable and I usually use <code>while(1);</code> to construct TLE in OnlineJudge.</p> <p>However, according to standard Basics.Program execution.Multi-threaded executions and data races.Forward progress, it seems to be an UB.</p> <p></p> <p>\"the compiler may assume X about the program\" is logically equivalent to \"if the program doesn't satisfy X, the behaviour is undefined\", so we should believe that an infinite loop without doing anything at all is an UB.</p> <p>Here explains why this is UB: N1528</p>"},{"location":"posts/Crypto/Certificate_and_Sign/","title":"\u4ec0\u4e48\u662f\u8bc1\u4e66\uff1f\u4ec0\u4e48\u662f\u7b7e\u540d\uff1f","text":"<p>\u611f\u6069\uff1ahttps://www.youdzone.com/signature.html</p> <p>\u6211\u4eec\u77e5\u9053\u975e\u5bf9\u79f0\u52a0\u5bc6\u6709\u4e00\u5bf9\u516c\u94a5\u79c1\u94a5\u3002\u516c\u94a5\u52a0\u5bc6\u53ef\u4ee5\u7528\u79c1\u94a5\u89e3\u5bc6\uff1b\u79c1\u94a5\u52a0\u5bc6\u53ef\u4ee5\u7528\u516c\u94a5\u89e3\u5bc6\u3002</p> <p>\u6570\u5b57\u7b7e\u540d\u4e00\u79cd\u7528\u6765\u9a8c\u8bc1\u6587\u4ef6\u5b8c\u6574\u6027\u771f\u5b9e\u6027\u7684\u8fc7\u7a0b\u3002\u5b83\u7684\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <ol> <li>\u7b7e\u540d\u7684\u4eba\u751f\u6210\u4e00\u5bf9\u5bc6\u94a5\uff0c\u516c\u94a5\u516c\u5f00\uff0c\u79c1\u94a5\u81ea\u5df1\u62ff\u7740\u3002</li> <li>\u5f53\u4ed6\u60f3\u7b7e\u540d\u4e00\u4e2a\u6587\u4ef6\u7684\u65f6\u5019\uff0c\u7528\u81ea\u5df1\u7684\u79c1\u94a5\u52a0\u5bc6\u6587\u4ef6\u7684\u54c8\u5e0c\uff08\u4e3a\u4e86\u8ba9\u6587\u4ef6\u5c0f\u4e00\u70b9\uff0cRSA\u8fd9\u79cd\u7b97\u6cd5\u8dd1\u7684\u5f88\u6162\uff09\u3002</li> <li>\u5f53\u5916\u4eba\u68c0\u67e5\u8fd9\u4e2a\u6587\u4ef6\u548c\u7b7e\u540d\u7684\u65f6\u5019\uff0c\u5148\u7528\u516c\u94a5\u89e3\u5bc6\u7b7e\u540d\u3002\u7136\u540e\u5bf9\u6bd4\u89e3\u5bc6\u540e\u7684\u4e1c\u897f\u548c\u6587\u4ef6\u7684\u54c8\u5e0c\u662f\u5426\u76f8\u540c</li> </ol> <p>\u4ece\u4e0a\u9762\u8fd9\u4e2a\u6d41\u7a0b\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u7b7e\u540d\u53ef\u4ee5\u68c0\u67e5\uff1a</p> <ol> <li>\u7b7e\u540d\u786e\u5b9e\u662f\u8fd9\u4e2a\u516c\u94a5\u7684\u79c1\u94a5\u6301\u6709\u8005\u7b7e\u7f72\u7684\u3002\u8fd9\u662f\u7531\u52a0\u5bc6\u7b97\u6cd5\u7684\u5b89\u5168\u6027\u4fdd\u8bc1\u7684\u3002</li> <li>\u6587\u4ef6\u786e\u5b9e\u662f\u5b8c\u6574\u7684\u3002\u8fd9\u662f\u7531\u54c8\u5e0c\u4e00\u6837\u4fdd\u8bc1\u7684\u3002</li> </ol> <p>\u4f46\u662f\u5982\u679c\u4e0d\u662f\u7ebf\u4e0b\u771f\u4eba\u5bf9\u7ebf\uff0c\u6211\u5728\u7f51\u4e0a\u58f0\u79f0\u6211\u662f\u9a6c\u4e91\uff0c\u6211\u751f\u6210\u4e86\u4e00\u5bf9\u5bc6\u94a5\uff0c\u73b0\u5728\u628a\u516c\u94a5\u516c\u5e03\u3002\u4ee5\u540e\u96be\u9053\u522b\u4eba\u5c31\u8981\u76f8\u4fe1\u6211\u771f\u7684\u662f\u9a6c\u4e91\u5417\uff1f\u80af\u5b9a\u4e0d\u662f\u7684\uff0c\u56e0\u4e3a\u73b0\u5728\u7684\u95ee\u9898\u4e0d\u5728\u4e8e\u80fd\u5426\u9a8c\u8bc1\u8fd9\u4e2a\u4e1c\u897f\u662f\u7531\u79c1\u94a5\u7b7e\u7f72\u7684\uff1b\u73b0\u5728\u7684\u95ee\u9898\u662f\u6211\u600e\u4e48\u77e5\u9053\u8fd9\u4e2a\u94a5\u771f\u7684\u662f\u9a6c\u4e91\u7684\uff1f</p> <p>\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5f15\u5165\u4e00\u4e2a\u7b2c\u4e09\u65b9\u673a\u6784\u3002\u8fd9\u662f\u6ca1\u529e\u6cd5\u7684\u4e8b\u60c5\u3002\u6240\u4ee5\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u76f8\u4fe1\u4e00\u4e9b\u7b2c\u4e09\u65b9\u673a\u6784\uff0c\u7136\u540e\u7531\u5b83\u4eec\u53bb\u8c03\u67e5\u53d6\u8bc1\u8981\u7533\u8bf7\u8bc1\u4e66\u7684\u7f51\u7ad9\u662f\u4e0d\u662f\u771f\u5b9e\u7684\u3002</p>"},{"location":"posts/Crypto/Further_Read/","title":"Further_Read","text":"<p>ProofsArgsAndZK.pdf (georgetown.edu)</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/","title":"Zero Knowledge Proof","text":"<p>\u590d\u4e60\u5230\u54ea\u91cc\u5199\u5230\u54ea\u91cc\uff0c\u53ef\u80fd\u8bed\u8a00\u4e71\u98de\uff0c\u903b\u8f91\u4e0d\u901a\uff0c\u8003\u5b8c\u6162\u6162\u4fee\u590d\u3002</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#week-1-intro--basic-definition","title":"Week 1 Intro &amp; Basic Definition","text":"<p>Interactive proof \u901a\u4fd7\u7684\u8bf4\u5c31\u662f\u4e24\u4e2a\u4eba\u804a\u5929\uff0c\u4e00\u4e2a\u4eba\u8bd5\u56fe\u5411\u53e6\u4e00\u4e2a\u4eba\u8bc1\u660e\u81ea\u5df1\u77e5\u9053\u4e00\u4e2a\u4e8b\uff0c\u4f46\u662f\u53c8\u4e0d\u60f3\u900f\u9732\u5176\u4e2d\u7684\u4fe1\u606f\u3002\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u662f\u4e00\u4e2a\u4eba\u8bd5\u56fe\u5411\u53e6\u4e00\u4e2a\u4eba\u8bc1\u660e 1000000016000000063 \u662f\u4e00\u4e2a\u5408\u6570\uff0c\u4f46\u662f\u5e76\u4e0d\u60f3\u900f\u9732\u5b83\u80fd\u5206\u89e3\u6210\u54ea\u4e24\u4e2a\u6570\u3002\u8fd9\u975e\u5e38\u6709\u7528\uff0c\u5b83\u53ef\u4ee5\u8ba9\u4f60\u663e\u5f97\u5f88\u5389\u5bb3\u7684\u540c\u65f6\u4e5f\u4e0d\u4f1a\u6cc4\u9732\u4fe1\u606f\u4f7f\u5f97\u522b\u4eba\u542c\u4e86\u4e4b\u540e\u4e5f\u80fd\u663e\u5f97\u5f88\u5389\u5bb3\u3002</p> <p>\u6b63\u7ecf\u7684\u6765\u8bf4\uff0c\u4f20\u7edf\u8bc1\u660e\u53ea\u6709\u4e00\u4e2a\u53c2\u4e0e\u8005\uff0c\u800c interactive proof \u5219\u6709 prover \u548c verifier \u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff0c\u6bd4\u8f83\u597d\u7684\u523b\u753b\u4e86\u5f88\u591a\u573a\u666f\uff0c\u4f8b\u5982\u8eab\u4efd\u9a8c\u8bc1\u3001\u533a\u5757\u94fe\u9a8c\u8bc1\u7b49\u7b49\u3002prover \u548c verifier \u4e4b\u95f4\u4f1a\u8fdb\u884c\u901a\u4fe1\uff0c\u6700\u7ec8 verifier \u4f1a\u6839\u636e\u4ed6\u6240\u770b\u5230\u7684\u5185\u5bb9\u7ed9\u51fa\u6700\u7ec8\u7684\u5224\u65ad\uff0c1 \u8868\u793a\u63a5\u53d7\uff0c0 \u8868\u793a\u62d2\u7edd\uff0c\u5206\u522b\u5bf9\u5e94\u76f8\u4fe1/\u4e0d\u76f8\u4fe1\u5bf9\u65b9\u771f\u7684\u77e5\u9053\u8fd9\u4ef6\u4e8b\u3002</p> <p>\u66f4\u6b63\u7ecf\u7684\u6765\u8bf4\uff0c\u4e00\u4e2a interactive proof \u9488\u5bf9\u7684\u662f\u4e00\u4e2a language\uff0c\u7ed9\u5b9a\u4e00\u4e2a instance \u5bf9\u53cc\u65b9\u90fd\u53ef\u89c1\uff0c\u4e00\u4e2a witness \u4ec5\u5bf9 prover \u53ef\u89c1\uff0cverifier \u6839\u636e prover \u53d1\u9001\u7684\u8fc7\u5f80\u4fe1\u606f\u548c instance \u4ee5\u53ca\u81ea\u5df1\u7684\u968f\u673a\u6027\u7ed9\u51fa\u56de\u590d\uff1bprover \u6839\u636e verifier \u8fc7\u5f80\u7684\u56de\u590d\u548c instance\u3001witness \u4ee5\u53ca\u81ea\u5df1\u7684\u968f\u673a\u6027\u7ed9\u51fa\u65b0\u7684\u56de\u590d\uff0c\u5e76\u5faa\u73af\u53cd\u590d\u76f4\u5230\u6709\u9650\u6b65\u540e verifier \u8f93\u51fa 1 \u6216 0\u3002</p> <p>\u4e00\u4e2a\u597d\u7684 interactive proof \u5e94\u8be5\u6ee1\u8db3\u4e09\u4e2a\u6027\u8d28\uff1a</p> <ul> <li>completeness</li> <li>soundness</li> <li>zero-knowledge</li> </ul> <p>\u8fd9\u4e09\u4e2a\u6027\u8d28\u5206\u522b\u63cf\u8ff0\u4e86\uff1a</p> <ul> <li>completeness\uff1a\u5982\u679c prover \u771f\u7684\u77e5\u9053\u8fd9\u4e2a\u77e5\u8bc6\uff0c\u4e5f\u5373 instance \u771f\u7684\u5728 language \u91cc\uff0cverifier \u5e94\u8be5\u76f8\u4fe1\uff0c\u4e5f\u5373\u8f93\u51fa 1\u3002</li> <li>soundness\uff1a\u5982\u679c prover \u4e0d\u77e5\u9053\u8fd9\u4e2a\u77e5\u8bc6\uff0c\u4e5f\u5373 instance \u5e76\u4e0d\u5728 language \u91cc\uff0cverifier \u4e0d\u5e94\u8be5\u76f2\u76ee\u76f8\u4fe1\uff0c\u4e5f\u5373\u8f93\u51fa 0\u3002</li> <li>zero-knowledge\uff1averifier \u4e0d\u5e94\u8be5\u4ece\u8bc1\u660e\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u4efb\u4f55\u300e\u77e5\u8bc6\u300f\u3002\u60f3\u8981\u5b9a\u4e49\u4ec0\u4e48\u662f\u77e5\u8bc6\u6bd4\u8f83\u56f0\u96be\uff0c\u6211\u4eec\u7a0d\u540e\u8bb2\u89e3\u3002</li> </ul> <p>\u6b63\u7ecf\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u5bf9 completeness \u548c soundness \u7684\u5b9a\u4e49\u4e3a\uff1a</p> <p>completeness</p> <p>\\(\\forall x \\in \\mathcal{L}, \\text{Pr}_{r,s}[\\langle P(r), V(s) \\rangle (x)=1] \\ge \\frac{3}{4}\\) </p> <p>soundness</p> <p>\\(\\forall x \\notin \\mathcal{L}, \\forall P^*, \\text{Pr}_{r,s}[\\langle P^*(r), V(s) \\rangle (x)=0] \\le \\frac{1}{2}\\)</p> <p>For zero knowledge we have a brilliant idea: if verifier could produce the proof by itself then we can say there is zero knowledge gained by verifier. So what's the definition of the proof? A proof for verifier consists of all the view that verifier sees. If there is a efficient simulator \\(S\\) such that for \\(\\forall x \\in \\mathcal{L}\\), we have \\(\\{\\text{View}_{V^*}^P\\}=\\{S(V^*,x)\\}\\), then we say \\((P, V)\\) is a perfect zero-knowledge proof, here the brace means the probability distribution.</p> <p>A classical example for perfect ZKP is Graph Isomorphism ZKP.</p> <p>\u56fe\u540c\u6784\u95ee\u9898\u672c\u8eab\u663e\u7136\u662f NP \u95ee\u9898\uff0c\u4f46\u662f\u662f\u5426\u662f NP-Complete \u6682\u65f6\u8fd8\u4e0d\u77e5\u9053\uff0c\u6709\u53ef\u80fd\u662f P \u95ee\u9898\u3002\u8fd9\u91cc\u53ea\u662f\u4e3e\u4f8b\u8bf4\u660e\u4ec0\u4e48\u662f zero-knowledge\uff0c\u5b9e\u9645\u4e0a IP \u7684\u8ba1\u7b97\u80fd\u529b\u662f\u5f88\u5f3a\u7684\u3002</p> <p></p> <p>\u8fd9\u91cc Prover \u5c06\u539f\u56fe \\(G_0\\) \u6253\u4e71\u5f97\u5230 \\(H\\) \u5e76\u53d1\u9001\u7ed9 Verifier\uff0c\u7136\u540e\u7531 Verifier \u9009\u62e9\u4e00\u4e2a 01 \u968f\u673a\u6570 \\(b\\) \u53d1\u56de Prover\u3002\u5982\u679c\u4e24\u4e2a\u56fe\u771f\u7684\u540c\u6784\uff0c\u90a3\u4e48\u4e0d\u7ba1 Verifier \u7684\u968f\u673a\u6570\u662f\u4ec0\u4e48\uff0cProver \u90fd\u5e94\u8be5\u80fd\u627e\u5230\u4e00\u79cd\u6392\u5217\u4f7f\u5f97 \\(\\sigma(G_b) = H\\)\u3002\u6240\u4ee5 Prover \u628a\u8fd9\u4e2a\u6392\u5217\u53d1\u9001\u7ed9 Verifier \u53bb\u9a8c\u8bc1\u3002</p> <p>\u5982\u679c\u4e24\u4e2a\u56fe\u771f\u7684\u540c\u6784\uff0c\u663e\u7136\u8fd9\u4e2a\u8fc7\u7a0b\u4f1a\u987a\u5229\u901a\u8fc7\uff1b\u5982\u679c\u4e24\u4e2a\u56fe\u4e0d\u540c\u6784\uff0c\u90a3\u4e48 V \u6709 50% \u7684\u6982\u7387 reject\uff08\u5982\u679c\u4e0d\u540c\u6784\uff0c\u968f\u673a\u5230\u53e6\u4e00\u4e2a\u56fe P \u5c31\u6ca1\u529e\u6cd5\u53d1\u9001\u56de\u6b63\u786e\u7684\u6392\u5217\u4e86\uff09\u3002\u6240\u4ee5\u6211\u4eec\u5c31\u8bc1\u660e\u4e86\u8fd9\u4e2a\u534f\u8bae\u7684 Completeness \u548c Soundness\u3002</p> <p>Zero-knowledge \u4e5f\u662f\u5f88\u663e\u7136\u7684\uff0c\u4ece V \u7684\u89c6\u89d2\u6765\u770b\uff0c\u4ed6\u770b\u5230\u4e86 \\(H, b, \\tau\\)\uff0c\u800c\u4ed6\u81ea\u5df1\u4e5f\u53ef\u4ee5\u968f\u5373\u6253\u4e71 \\(G_0\\) \u5f97\u5230 \\(H\\)\uff0c\u7531\u4e8e\u8fd9\u662f\u4ed6\u81ea\u5df1\u6253\u4e71\u7684\uff0c\u6240\u4ee5 \\(\\tau\\) \u4e5f\u662f\u5bb9\u6613\u5f97\u5230\u7684\u3002\u6545\u800c\u5b58\u5728\u4e00\u4e2a simulator \u6a21\u62df\u51fa V \u7684\u89c6\u89d2\uff0c\u4e5f\u5c31\u8bc1\u660e V \u4ece\u8fd9\u4e2a\u8bc1\u660e\u4e2d\u5f97\u4e0d\u5230\u4efb\u4f55\u4fe1\u606f\u3002</p> <p>zero-knowledge \u7684\u5b9a\u4e49\u4e5f\u6709\u4e00\u4e9b\u53d8\u79cd\uff1a</p> <p>black-box zero-knowledge</p> <p>If exist efficient simulator \\(S\\) s.t. \\(\\forall V^*, \\forall x \\in \\mathcal{L}\\), we have \\(\\{\\text{View}_{V^*}^P\\} = \\{S_{V^*}(x)\\}\\)</p> <p>honest verifier zero-knowledge (HVZK)</p> <p>If exist simulator \\(S\\) for honest verifier \\(V\\) s.t. \\(\\{\\text{View}_{V}^P\\}=\\{S(V,x)\\}\\) </p> <p>special/semi honest verifier zero-knowledge (SHVZK)</p> <p>If exist efficient simulator \\(S\\) s.t. \\(\\forall x \\in \\mathcal{L}, s \\in \\{0,1\\}^*\\), we have \\(\\{\\text{View}_{V(s)}^P\\}=\\{S(V(s),x)\\}\\)</p> <p>Here black-box zero-knowledge is a stricter definition of zero-knowledge, and I didn't see any rationale behind this definition.</p> <p>HVZK means when constructing simulator, we can assume that V is honest, instead of malicious. Choosing HVZK instead of ZK doesn't mean that we already know there is some malicious V that could extract knowledge, sometimes it's just that we don't know how to prove ZK. </p> <p>SHVZK stands for semi or special HVZK. Special xxx under this context means it's a special form of xxx and it's sufficient for xxx. We usually use SHVZK with regard of sigma protocol because in that sense SHVZK is enough for HVZK and it's easier to analyse.</p> <p>A classical example for HVZK is Graph 3-coloring ZKP.</p> <p>The protocol operates as follows:</p> <ul> <li>P randomly permutates the coloring and commits it.</li> <li>V chooses arandom edge \\((i, j)\\) and send this choice</li> <li>P reveals to V only the color of \\(i\\) and \\(j\\)</li> <li>V checks if commitment is correct and if the colors are different.</li> </ul> <p>It's easy to construct simulator for honest verifier, using the similar tech in GI ZKP, but it's not trivial to analyse the malicious case. If a V is malicious, it might send \\((i, j)\\) while the two vertices aren't adjacent. In this case how to simulate the view of V if we don't actually know the correct coloring? So here we choose HVZK over ZK.</p> <p>Let us move on to another topic: indistinguishability.</p> <ul> <li>Perfectly indistinguishable if for any algorithm \\(D\\), parameter \\(\\lambda\\), \\(|\\text{Pr}[D(1^\\lambda, X) = 1] - \\text{Pr}[D(1^\\lambda, Y) = 1]| = 0\\)</li> <li>Statistically indistinguishable if for any algorithm \\(D\\), parameter \\(\\lambda\\), \\(|\\text{Pr}[D(1^\\lambda, X) = 1] - \\text{Pr}[D(1^\\lambda, Y) = 1]| \\le \\text{negl}(\\lambda)\\)</li> <li>Computationally indistinguishable if for any efficient algorithm \\(D\\), parameter \\(\\lambda\\), \\(|\\text{Pr}[D(1^\\lambda, X) = 1] - \\text{Pr}[D(1^\\lambda, Y) = 1]| \\le \\text{negl}(\\lambda)\\)</li> </ul> <p>These are three types of indistinguishability. The perfect one means the two distributions really have no difference.; the statistical one means you need to be very lucky to find some difference; the computational one means you need to be very lucky and work very hard to find some difference.</p> <p>Based on different indistinguishability, we can define different zero-knowledge properties: to what extent does the generated view looks like the real view.</p> <p>Accordingly, we have perfect one, statistical one, and computational one.</p> <p>Put zero-knowledge aside we have some variants of soundness.</p> <p>We call protocols whose soundness only holds against efficient provers (so we can prove soundness using cryptographic assumptions) Interactive Arguments. It's a strange name, we could just call it computationally sound proof system. The only difference is that we only require computationally soundness. In particular, perfect ZK arguments are known to exist for every language in NP, it is considered unlikely that perfect ZK proofs.</p> <p>Another variant is knowledge soundness, which assures when V accepts, then we can extract the witness from the messages sent by P. Note that normal soundness requirement only ensures V to know there exist a witness (because \\(x\\) is in the language, thus a witness exists), but V can't be sure about whether P has this witness. This would not be strong enough in some cases, like login protocol.</p> <p>Proof of Knowledge</p> <p>(adapted from https://crypto.stanford.edu/cs355/19sp/lec5.pdf, I think this definition is better than that shown in slide) An IP \\((P, V)\\) for language \\(L\\) is a proof of knowledge with knowledge error \\(\\epsilon\\), if there exists an efficient (expected polynomial running time) algorithm \\(E\\), called an extractor, s.t. for every instance \\(x\\) and every prover \\(P\\): \\(\\text{Pr}[(x, w) \\in L: w = E^P(x)] \\ge \\text{Pr}[(P,V)(x)=1] - \\epsilon\\).</p> <p>Some explanations about the definition: The probability of V accepts is \\(\\text{Pr}[(P,V)(x)=1]\\) and we can always extract a correct witness except small probability (knowledge error). It doesn't matter P really knows the witness, if we can extract witness from P's messages, P itself could extract witness from messages as well. So this is our definition of \"knows\": you knows everything that you could efficiently compute.</p> <p>Knowledge error \\(\\epsilon\\) directly implies soundness error \\(\\epsilon\\).</p> <p>Now some more definitions in order to introduce sigma-protocol.</p> <p>We define an IP \\((P, V)\\) is public coin if V's messages are exactly random bits and nothing else. In this case, V's messages are also called challenges. For example, GI ZKP is a public coin IP because the only message sent by V is a random bit; while the trivial GNI (Graph Not Isomorphism) ZKP is not a public coin because its random bit must not be leaked otherwise P could cheat V. However, GS1986 proves every language with an IP has a public coin IP, by proving public coins and private coins the same complexity class as Probabilistic, nondeterministic, polynomial time Turing machine.</p> <p>Here's another concept called Trees of transcipts. </p> <p>An \\((n_1, \\cdots, n_k)\\) tree of transcripts for a public coin IP is a set of \\(\\Pi_{i=1}^{k} n_i\\) transcipts arranged in a tree s.t.:</p> <ul> <li>Vertices correspond to P messages.</li> <li>Edges correspond to V challenges.</li> <li>Each node at depth \\(i\\) has \\(n_i\\) child edges labelled with distinct challenges.</li> <li>Each transcript corresponds to one root-to-leaf path.</li> <li>The tree is accepting if V would accept every transcript.</li> </ul> <p>Finally we can define another soundness: Special soundness.</p> <p>A public coin IP is \\((n_1, \\cdots, n_k)\\)-special sound if exist an efficient extractor \\(E\\) takes instance and a \\((n_1, \\cdots, n_k)\\)-tree of accepting transcripts and produces a witness \\(w\\) with \\((x, w) \\in \\mathcal{R}\\).</p> <p>We've already known that special xxx means it's a special form of xxx, and it's sufficient for xxx. So here special soundness implies knowledge soundness, which further implies soundness.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#theorem-attema-cramer-kohl-2021","title":"Theorem Attema, Cramer, Kohl 2021","text":"<p>Theorem Attema, Cramer, Kohl 2021</p> <p>Let \\((P,V) be (n_1, \\cdots, n_k)\\)-special sound with uniformly random V messages from set of size \\(N\\), and \\(\\Pi_{i=1}^k n_i\\) be polynomially bounded in \\(|x|\\). Then \\((P,V)\\) is knowledge sound with knowledge error: \\(k = \\frac{N^k - \\Pi_{i=1}^k(N-n_i - 1)}{N^k} \\le \\frac{\\sum_{i=1}^k(n_i - 1)}{N}\\)</p> <p>The proof could be found here: https://eprint.iacr.org/2021/307.pdf, section 3.</p> <p>The intuition for a three-step sigma-protocol is that: if the protocol is \\(k\\)-sound, then given \\(k\\) different challenges, we can extract the witness; so if some input is not in the language (which means there doesn't exist a witness for it), then there will be at most \\(k - 1\\) accepting challenges for this \\(x \\notin L\\), because otherwise we can extract a witness for \\(x\\), which contradicts \\(x \\notin L\\). Thus, knowledge error won't exceed \\(k - 1 / |C|\\) since the probability of false accepting is at most \\(k - 1 / |C|\\).</p> <p>The results could be concluded as:</p> Soundness ZK Proofs Perfect/Sta Computational Arguments Computational Perfect/Sta"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#sigma-protocol","title":"Sigma-Protocol","text":"<p>Finally we get here, sigma-protocol.</p> <p>A sigma-protocol is an 3-move, public coin IP satisfying:</p> <ul> <li>completeness with no errors</li> <li>\\(k\\)-special soundness</li> <li>SHZVK</li> </ul> <p>By definition, GI ZKP is a sigma-protocol.</p> <p>Now we introduce another important concept: commitment schemes.</p> <p>A commitment scheme is a collection of 3 PPT algos (Setup, Commit, Verify) s.t. for any parameter \\(\\lambda\\):</p> <ul> <li>Setup(\\(1^\\lambda\\)) outputs public parameters \\(pp\\) describing message space \\(M\\), randomness space \\(R\\), decommitment space \\(D\\) and commitment space \\(C\\).</li> <li>Commit(\\(pp, m \\in M, r \\leftarrow_\\$ R\\)) outputs a pair \\((c, d) \\in C \\times D\\), where \\(c\\) is the commitment, \\(d\\) is the secret de-commitment, normally \\(d\\) won't be sent to others unless Verify requires it.</li> <li>Verify(\\(pp, c \\in C, d \\in D, m \\in M\\)) outputs a bit \\(b \\in \\{0, 1\\}\\), where \\(b\\) is the verifying result, \\(0, 1\\) stands for failiure and success respectively.</li> </ul> <p>There are two important properties for commitment scheme: hiding and binding.</p> <p>Hiding means it's difficult to determine the original message only from the commitment. Given the commitment and public parameter, how much does the original message distribution differs from uniform distribution implies how difficult it is to extract original message from commitment and \\(pp\\).</p> <p>Perfectly hiding indicates the message distribution given commitment and \\(pp\\) is exactly the uniform distribution; computationally hiding indicates unless you work very hard (using unbounded time to crack) or very lucky (i.e. something with negelectable probability happens), you can't tell the difference, i.e. you can't extract message from commitment.</p> <p>Binding means it's difficult to change the message after committed. </p> <p>Perfectly binding means the original message is unique; computationally binding means it needs hard work or huge luck to find another message which could generate the same commitment.</p> <p>Again, \\(\\text{Perfect Hiding} \\&amp; \\text{Perfect Binding} = False\\).</p> Perfect Hiding Compuatationally Hiding Perfect Binding False True Computationally Binding True True <p>Here we introduce two famous commitment scheme: Elgamal Commitment and Pedersen Commitment.</p> <p></p> <p>Elgamal Commitment is based on Elgamal encryption (https://en.wikipedia.org/wiki/ElGamal_encryption), so it inherits the security assumption: Decisional Deffie-Hellman assumption (https://en.wikipedia.org/wiki/Decisional_Diffie%E2%80%93Hellman_assumption).</p> <p>It's computationally hiding and perfectly binding.</p> <p></p> <p>The setup is the same as Elgamal commitment, the difference lies in commit function. In Elgamal commitment, we need to send \\(c2 = r \\cdot h\\) so as to ensure \\(r\\) won't be changed easily afterwards. If we don't send \\(rh\\) then malicous commit-er could change \\(r\\) to another value \\(r'\\) and calculate corresponding \\(r'g\\) and then eventually open a different value to original message \\(m\\).</p> <p>In Pedersen commitment, however, we send \\(mg + rh\\), and we no longer need \\(rh\\) in this case because if we want to open another value for \\(m\\), we have to solver DLOG to get a corresponding \\(g\\).</p> <p>It's perfectly hiding and computationally binding.</p> <p>Sigma protocol together with commitment scheme could be used for NP-complete problem, as we've already seen: Graph 3-coloring problem.</p> <p>Consider composition of sigma-protocol, we have this table:</p> Preserved? Soundness ZK Sequential True True Parallel True for proofs; False for arguments False <p>Besides, we have AND OR composition of sigma-protocol, defined as:</p> <p></p> <p></p> <p>Note that in AND composition, two sigma-protocols share one challenge; in OR composition, new protocol uses simulator to generate the response of one protocol.</p> <p>In OR composition, we require \\(x, x' \\in \\mathcal{L}\\), it's strange at the first glance: why both input need to be in the language? Here, what we want is not just to prove \\(x, x'\\) in language, what we want is to prove we know the witness! Also note in OR composition, P need to know whether it's \\((x, w) \\in R\\) or \\((x', w) \\in R\\), P needs to decide its strategy based on this (decide to simulate which one and really execute which one).</p> <p>Not Only Prove in Language, Also Prove We Know Witness</p> <p>In some cases, it's enough to show verifier that input is in the language, while in other cases, it's more important to show V we actually know the witness. For example, when it comes to graph isomorphism problem, it might be enough to show input is in language (i.e. the two graphs are isomorphic); while in DLOG protocol, it's trivial to show input is in language because every pair \\(A, G\\) is in language, what's significant is that we really know the \\(s\\) s.t. \\(A = s \\cdot G\\) !!!</p> <p>Completeness, special soundness and SHVZK of AND OR composition are preserved in both cases.</p> <p>TBD: need formal definition for MPC</p> <p>Now let us introduce another concept: MPC (Multi Party Computation).</p> <p>There are multiple players want to compute a joint function of their private inputs without leaking information on the secret. Finally all players should get the output, not knowing others' secrets.</p> <p>MPC has many interesting and practical applications, such as: Sugar Beet Auction (https://en.wikipedia.org/wiki/Danish_Sugar_Beet_Auction), Hotel Number Adjacency (https://www.zhihu.com/question/397446056).</p> <p>We say an MPC protocol computes a function \\(f\\) in semi-honest model, if it satisfies:</p> <ul> <li>correctness: any player's output is correct</li> <li>t-privacy: there exists a simulator s.t. any \\(t\\) players get no information on others' secret.</li> </ul> <p>Here we are gonna use MPC in the head to construct a sigma-protocol.</p> <p>The idea is quite similar to Graph 3-coloring ZKP: the consistency of colors is the consistency of Views; the commitment of colors is the commitment of Views.</p> <p>The process is shown below:</p> <p></p> <p>P has a MPC protocol in its head. After execution of MPC in its head, P commits to Views of every player in MPC and send the commitment to V. V randomly pick two players \\(i, j\\) and ask P to open the commitment of Views of \\(i, j\\). Fianlly V checks if commitment scheme is valid, if Views of \\(i, j\\) are consistent, and if player \\(i, j\\) output \\(1\\).</p> <p>Analysis:</p> <ul> <li>Completeness relies on correctness of MPC protocol.</li> <li>\\(\\binom{n}{2}\\)-special soundness is obvious since we can open every pair of Views, we can assure global consistency based on local consistency.</li> <li>SHVZK comes from efficient simulator of MPC protocol (t-privacy)</li> </ul> <p>Now we introduce another concept: Fiat-Shamir Heuristic.</p> <p>A non-interactive protocol has many advantages, for example: low communication cost, no need to worry about malicous verifier, etc..</p> <p>In sigma-protocol, the only usage of verifiers is the generate random challenges. Why don't P generates random challenges for V and send the initial message, challenge, and the final message together, to V, just in one round? The only problem would be that V might not be convinced that P's challenge is really randomly generated instead of carefully picked? We can use a hash function to generate \\(c\\) based on shared input \\(x\\) and the initial message \\(a\\). We send \\(a, c, z\\) together to V and leave V to verify whether \\(z\\) is a good result and whether \\(c\\) is a valid hash of \\(a\\) and \\(x\\).</p> <p></p> <p>Actually, signature scheme works just in this way.</p> <p>TBD: picnic post-quantum signature scheme</p> <p>Now lets focus on making sigma-protocols zero-knowledge against malicious verifiers.</p> <p>The idea behind compiling sigma-protocol to fully ZK protocol is quite simple. If we let the challenge be controled by both P and V, then malicious V cannot construct specific challenge to break ZK property.</p> <p>Consider the new challenge \\(c''\\) is the sum of \\(c\\) and \\(c'\\), where \\(c\\) is provided by P and \\(c'\\) is provided by V. If P send \\(c\\) to V before V generates \\(c'\\), then V could again construct any target value; but otherwise then how could V be sure P won't change \\(c\\) afterwards? Commitment!</p> <p>The whole process is:</p> <p></p> <p>Now let's look at sigma protocols from DLOG.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#schnorr-protocol","title":"Schnorr Protocol","text":"<p>First we introduce Schnorr protocol. The idea is quite simple, P tries to prove to V that it knows the discrete log of \\(A\\) with base \\(G\\), within a finite group.</p> <p></p> <p>The completeness, ZK and special soundness is obvious. One thing to be noted is that we can use linear algebra way to prove its special soundness by inverting the transformation matrix.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#same-dlog-protocol","title":"Same DLOG Protocol","text":"<p>Similar to Schnorr protocol, we can derive a Same DLOG protocol, which intends to show two pairs of values have the same discrete log, i.e. \\(A = a \\cdot G, V = a \\cdot U\\).</p> <p></p> <p>Using this idea, we can have application for mix-networks.</p> <p>A mix-network means given some inputs \\(A_1, \\cdots, A_n\\), someone rerandomise and shuffle the input. ZKP could be used to prove the correctness of the rerandomising and shuffling operation. Proof could guarantee operator rerandomized and shuffled correctly, but could not prove it's done randomly.</p> <p>Take Elgmal ciphertexts as an example, the original message is \\((c_1, c_2) = (m + rH, rG)\\). The rerandomization is: \\((c_1, c_2) + (r'H, r'G) = (m + (r+r')H, (r+r')G)\\). So we can reduce rerandomization to having same DLOG because if \\((c'_1, c'_2)\\) is correct rerandomization result of \\((c_1, c_2)\\), then \\((c_1 - c'_1, c_2 - c'2)\\) have the same DLOG, vice versa.</p> <p>Shuffling could be modeled as OR composition of sigma-protocol, so the result could be concluded as: if we want to prove \\((c_1, c_2)\\) and \\((d_1, d_2)\\) correctly rerandomized and shuffled into \\((c'_1, c'_2)\\) and \\((d'_1, d'_2)\\), we only need this composition of sigma-protocol:</p> \\[\\begin{align} (c_1 - c'_1, c_2 - c'_2) \\text{Same DLOG} \\;\\; &amp;\\textbf{AND} \\;\\; (d_1 - d'_1, d_2 - d'_2) \\text{Same DLOG}\\\\ &amp;\\textbf{OR}\\\\ (c_1 - d'_1, c_2 - d'_2) \\text{Same DLOG} \\;\\; &amp;\\textbf{AND} \\;\\; (d_1 - c'_1, d_2 - c'_2) \\text{Same DLOG} \\end{align}\\]"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#pedersen-protocol","title":"Pedersen Protocol","text":"<p>Now move on to another topic: Pedersen protocol.</p> <p>The idea is to show P knows \"secret\" \\(a, r\\) s.t. \\(A = a \\cdot G + r \\cdot H\\), given \\(G, H\\). The language is trivial since every \\(A\\) could be writen in this form, the only problem is it's hard to really find one.</p> <p>The process is very similar to Schnorr protocol, the only difference is that there are two group element \\(G, H\\), while Schnorr only have one:</p> <p></p> <p>To model/describe/abstract the similartiy, we consider a basic abstract algebra concept homomorphism. A map \\(f\\) is a group homomorphism if \\(f(a_1) + f(a_2) = f(a_1 + a_2)\\).</p> <p>It's easy to verify Pedersen and Elgamal commitment scheme satisfies this condition.</p> <p>Now we can derive a more general protocol, Homomorphism preimage protocol:</p> <p></p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#multiplication-protocol","title":"Multiplication Protocol","text":"<p>We can get some insights about how to prove soundness of this kind of protocol.</p> <p>First we construct a matrix equation in this form: \\(X \\cdot C = ...\\), where \\(X\\) denotes the challenge matrix, consisting of challenges and their powers; \\(C\\) denotes the commit matrix, consisting of different commitments of messages, masks, etc.. The reason is that if \\(X\\) is invertible then we multiply the inversion of \\(X\\) to both sides and get openings of commits. By binding properties, there should only be one possible form w.h.p. (\\(G, H\\) are analogous to linear space bases).</p> <p>Then we need to prove the obtained opening satisfies requirements, here the requirement is \\(a_1 \\cdot a_2 = a_3\\). We substitute the equations that V checks, and get a polynomial equation containing \\(x\\) and \\(a_i\\). If the equation is of degree \\(d\\) but there are \\(d+1\\) or more challenges \\(x_1, ..., x_{d+1}\\) for this equation, then the polynomial must be identical to zero polynomial. Thus we can get a equation for \\(a_i\\) by comparing the coefficients.</p> <p>Different rows are different branches of tree, different columns are different verifier checks.</p> <p>Multiplication protocol can be used to proof that values are non-zero (i.e. value is invertible, exists another value s.t. multiplication is \\(1\\)).</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#low-degree-circuits","title":"Low Degree Circuits","text":"<p>Low degree circuits is an important protocol because of its application and the analysis method could be applied to other protocols.</p> <p>Given a circuit with N add/mul gates, the output could be represented as a polynomial, and here we only consider the case when polynomial degree is bounded by \\(d\\).</p> <p>So the relation is:</p> \\[\\begin{align} R_q = \\Bigg\\{ (\\mathbb{G}, G, H, \\{A_i\\}_{i=1}^{l+1}, p), \\{(a_i, r_i)_{i=1}^{l+1}\\} :   G, H, \\{A_i\\} \\in \\mathbb{G}, \\{a_i, r_i\\} \\in \\mathbb{Z}_p, A_i = a_i \\cdot G + r_i \\cdot H, q(a_1, \\cdots, a_l) = a_{l+1} \\Bigg\\} \\end{align}\\] <p>Obviously, multiplication protocol, Schnorr protocol are special cases of low degree circuits protocol.</p> <p>Completeness comes from Pedersen's completeness. </p> <p>SHVZK analysis is similar to multiplication protocol, all commitments are uniformly distributed except the last one could be uniquely determined. </p> <p>Normally a polynomial with degree \\(d\\) will have \\(d + 1\\) soundness, the way to prove this is to construct \\(X \\cdot C = Z \\cdot G + R \\cdot H\\), where \\(X\\) denotes matrix of challenges and their powers; \\(C\\) demotes matrix of different commitments; \\(Z\\) denotes matrix of masked value \\(z_i\\); \\(R\\) denotes matrix of randomness; \\(G, H\\) are two group elements. If \\(X\\) is invertible then we can have an opening of commitments \\(C\\), which contains witness.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#week-7-sumcheck-protocol","title":"Week 7 Sumcheck Protocol","text":""},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#1-sumcheck-protocol-itself","title":"1 Sumcheck Protocol Itself","text":"<p>\u8fd9\u4e2a protocol \u7684 instance \u662f \\(p(X_1, \\cdots, X_l)\\) over \\(\\mathbb{F}\\) \u548c \\(u \\in \\mathbb{F}\\)\uff0c\u5b50\u96c6 \\(H \\subset \\mathbb{F}\\)\u3002\u60f3\u8981\u68c0\u9a8c\u7684\u662f\u591a\u9879\u5f0f \\(p\\) \u5728 \\(H^l\\) \u4e0a\u6c42\u503c\u7136\u540e\u5168\u52a0\u8d77\u6765\u662f\u4e0d\u662f\u7b49\u4e8e \\(u\\)\u3002\u5728\u8fd9\u91cc\u5e76\u6ca1\u6709 witness\uff0c\u90a3 verifier \u5230\u5e95\u60f3\u77e5\u9053\u5565\uff1f\u4ed6\u81ea\u5df1\u5176\u5b9e\u672c\u6765\u5c31\u53ef\u4ee5\u9a8c\u8bc1\uff0c\u56e0\u4e3a\u4f60\u81ea\u5df1\u628a\u6240\u6709\u7684\u503c\u90fd\u52a0\u8d77\u6765\u7b97\u7b97\u5c31\u77e5\u9053\u4e86\u3002\u4f46\u662f\u4ed6\u5e76\u4e0d\u60f3\u82b1\u8fd9\u4e48\u591a\u65f6\u95f4\uff0c\u4ed6\u53ea\u662f\u60f3\u501f\u52a9 prover \u786e\u8ba4\u8fd9\u4ef6\u4e8b\u662f\u771f\u7684\u3002</p> <p>\u8fd9\u4e2a\u534f\u8bae\u662f\u8fd9\u6837\u5de5\u4f5c\u7684\uff1a</p> <p></p> <p>\u7b80\u5355\u6765\u8bf4\u5c31\u662f prover \u8d1f\u8d23\u628a\u540e\u9762\u7684 \\(l-1\\) \u4e2a\u53d8\u91cf\u90fd\u679a\u4e3e\u4e86\uff0c\u76f8\u5f53\u4e8e\u628a\u591a\u9879\u5f0f\u540e\u9762\u7684\u53d8\u91cf\u90fd\u6d88\u9664\u4e86\uff0c\u7559\u7ed9 verifier \u81ea\u5df1\u679a\u4e3e\u7b2c\u4e00\u4e2a\u53d8\u91cf\u6240\u6709\u53ef\u80fd\u7684\u53d6\u503c\uff0c\u7136\u540e\u68c0\u67e5\u548c\u662f\u4e0d\u662f \\(u\\)\u3002</p> <p>\u8fd9\u6837 verifier \u80af\u5b9a\u4e0d\u80fd\u8f7b\u4fe1\uff0c\u5426\u5219 soundness \u5c31\u70b8\u4e86\u3002verifier \u7ee7\u7eed\u51fa\u9898\uff0c\u90a3\u6211\u628a\u591a\u9879\u5f0f\u7684\u7b2c\u4e00\u4e2a\u503c\u56fa\u5b9a\uff08\u8fd9\u5c31\u662f\u6211\u7684\u7b2c\u4e00\u4e2a challenge\uff01\uff09\uff0c\u8fd9\u6837\u5c31\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u9879\u5f0f\u4e86\uff0c\u73b0\u5728\u538b\u529b\u56de\u5230 prover \u8fd9\u8fb9\uff0c\u4f60\u7ee7\u7eed\u9012\u5f52\u5730\u7528\u8fd9\u4e2a\u591a\u9879\u5f0f\u505a\u4e00\u4e0b sumcheck\u3002</p> <p>\u8fd9\u6837\u505a\u663e\u7136 verifier \u7684\u8ba1\u7b97\u538b\u529b\u51cf\u5c0f\u4e86\uff0c\u4e3b\u8981\u7684\u8ba1\u7b97\u7531 prover \u627f\u62c5\uff0c\u540c\u65f6 verifier \u4e5f\u80fd\u786e\u5b9a prover \u6ca1\u6709\u9a97\u4eba\u3002\u540c\u65f6\u534f\u8bae\u7684 communication cost \u4e5f\u53d8\u5c0f\u4e86\uff0c\u56e0\u4e3a\u6211\u4eec\u53ea\u4f20\u9012\u5355\u53d8\u91cf\u7684\u591a\u9879\u5f0f\u3002</p> <p>completeness \u975e\u5e38\u663e\u7136\uff0c\u5982\u679c instance \u672c\u8eab\u5c31\u5728 language \u91cc\uff0c\u6ca1\u6709\u4efb\u4f55\u53ef\u80fd verifier \u4f1a reject\u3002</p> <p>soundness \u5efa\u7acb\u5728\u300e\u57df\u4e0a\u591a\u9879\u5f0f\u7684\u6839\u7684\u6570\u91cf\u4e0d\u4f1a\u8d85\u8fc7 degree\u300f\u8fd9\u4e00\u4e8b\u5b9e\u4e0a\u3002\u5373\u4f7fprover \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5047\u7684\u591a\u9879\u5f0f\u9a97\u8fc7\u4e86\u7b2c\u4e00\u9053\u68c0\u67e5\uff08\u6c42\u548c\u4e4b\u540e\u786e\u5b9e\u7b49\u4e8e \\(u\\)\uff09\uff0cverifier \u60f3\u8981\u9a8c\u8bc1\u4f60\u63d0\u4f9b\u7684\u591a\u9879\u5f0f\u548c\u771f\u5b9e\u7684\u591a\u9879\u5f0f\u786e\u5b9e\u662f\u540c\u4e00\u4e2a\u591a\u9879\u5f0f\uff0c\u5b83\u7684\u65b9\u6cd5\u662f\u5728\u968f\u673a\u4e00\u4e2a\u70b9\u4e0a\u53d6\u503c\uff0c\u56e0\u4e3a\u968f\u673a\u4e00\u4e2a\u70b9\u4e24\u4e2a\u591a\u9879\u5f0f\u53d6\u503c\u76f8\u7b49\u7684\u6982\u7387\u5176\u5b9e\u5c31\u662f\u968f\u673a\u70b9\u662f\u4e24\u4e2a\u591a\u9879\u5f0f\u7684\u5dee\u7684\u6839\u7684\u6982\u7387\uff0c\u800c\u8fd9\u4e2a\u4e0d\u8d85\u8fc7 \\(\\frac{d}{|\\mathbb{F}|}\\)\u3002\u5982\u679c\u4e0d\u5de7\u968f\u673a\u53d6\u503c\u4e24\u4e2a\u591a\u9879\u5f0f\u7684\u503c\u5c31\u662f\u4e00\u6837\u7684\uff0c\u90a3\u4e48\u9012\u5f52\u7684\u5b50\u95ee\u9898\u5c31\u5728 language \u4e2d\uff0c\u6240\u4ee5 verifier \u5c31\u6ca1\u529e\u6cd5\u5206\u8fa8\u4e86\uff1b\u4f46\u662f\u5982\u679c\u53d6\u503c\u4e0d\u540c\uff0c\u90a3\u4e48\u9012\u5f52\u7684\u5b50\u95ee\u9898\u7684 instance \u4e5f\u4e0d\u5728 language \u91cc\uff0c\u8fd9\u6837\u5c31\u5f88\u5bb9\u6613\u4f7f\u7528\u5f52\u7eb3\u6cd5\u4e86\u3002\u6700\u7ec8\u53ef\u4ee5\u8bf4\u660e soundness error\uff08\u4e5f\u5c31\u662f\u5f53 instance \u4e0d\u5728 language \u7684\u65f6\u5019 verifier \u4f9d\u7136 accept \u7684\u6982\u7387\uff09\u4e0d\u8d85\u8fc7  \\((l-1)d/|\\mathbb{F}|\\)\u3002</p> <p>Sumcheck protocol will finally reduce the claim into a single point value of the polynomial, normally V knows the polynomial \\(p\\) in advance, so it doesn't need P to send the coefficients, which brings lots of communication cost. While in GKR protocol, V doesn't know the exact coefficients of \\(p\\), but it knows the formula form of \\(p\\), so V requires some components of \\(p\\) and then calculate the value by itself.</p> <p>Here is the parameters of sumcheck protocol:</p> Parameters Value Prover Complexity \\(O(\\|H\\|^l)\\) ops and \\(p\\)-evaluation Soundness \\(ld/\\|\\mathbb{F}\\|\\) Communication Complexity \\(O(ld)\\) Verifier Complexity \\(O(ld)\\) ops"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#2-conp-is-in-ip","title":"2 coNP Is in IP","text":"<p>coNP \u91cc\u7684\u8bed\u8a00\u6ee1\u8db3\uff1a\u5176\u8865\u8bed\u8a00\u5728 NP \u4e2d\u3002\u6240\u4ee5\u6982\u5ff5\u5176\u5b9e\u5f88\u597d\u7406\u89e3\u3002\u6bd4\u5982\u8bf4\u56fe\u540c\u6784\u663e\u7136\u662f NP \u7684\uff0c\u4f60\u7ed9\u6211\u4e24\u4e2a\u56fe\u4f5c\u4e3a instance \u548c\u4e00\u4e2a\u6620\u5c04\u4f5c\u4e3a witness\uff0c\u6211\u5f88\u7b80\u5355\u5c31\u80fd\u9a8c\u8bc1\u4f60\u8bf4\u7684\u5bf9\u4e0d\u5bf9\u3002\u4f46\u662f\u56fe\u4e0d\u540c\u6784\u5c31\u6ca1\u6709\u8fd9\u4e48\u7b80\u5355\uff0c\u4f60\u5f88\u96be\u8ba9\u6211\u786e\u4fe1\u4f60\u7ed9\u6211\u7684\u4e24\u4e2a\u56fe\u4e0d\u540c\u6784\u3002\u663e\u7136\u56fe\u4e0d\u540c\u6784\u7684\u8865\u8bed\u8a00\u5c31\u662f\u56fe\u540c\u6784\uff0c\u6240\u4ee5\u56fe\u4e0d\u540c\u6784\u662f coNP \u7684\u3002</p> <p>\u73b0\u5728\u6211\u4eec\u5e0c\u671b\u5f97\u5230\u4e0b\u56fe\u8fd9\u6837\u7684\u5173\u7cfb\uff1a ![[coNP1.png.png]] \u8fd9\u5c31\u9700\u8981\u6211\u4eec\u627e\u5230\u4e00\u4e2a coNP-Complete \u7684\u8bed\u8a00\uff0c\u7136\u540e\u8bf4\u660e\u5b83\u662f IP \u7684\u5c31\u53ef\u4ee5\u4e86\u3002</p> <p>\u6bd4\u8f83\u7ecf\u5178\u7684\u4e00\u4e2a NP-Complete \u7684\u95ee\u9898\u662f 3-SAT \u95ee\u9898\uff08\u4e00\u4e9b clause \u53d6\u4ea4\u96c6\uff0c\u95ee\u80fd\u5426\u4f7f\u5f97\u8fd9\u4e2a\u903b\u8f91\u8868\u8fbe\u5f0f\u4e3a\u771f\uff09\uff0c\u5b83\u7684\u8865\u95ee\u9898 3-UNSAT \u95ee\u9898\u5c31\u662f\u4e00\u4e2a coNP-Complete \u95ee\u9898\u3002</p> <p>\u6311\u9009\u8fd9\u4e2a\u903b\u8f91\u8868\u8fbe\u5f0f\u76f8\u5173\u7684\u95ee\u9898\u662f\u6709\u7528\u610f\u7684\uff0c\u56e0\u4e3a\u903b\u8f91\u8868\u8fbe\u5f0f\u5f88\u5bb9\u6613\u7528\u52a0\u51cf\u4e58\u6cd5\u62d3\u5c55\u5230\u57df \\(\\mathbb{F}\\) \u4e0a\u8fdb\u800c\u53d8\u6210\u4e00\u4e2a sumcheck \u95ee\u9898\u3002</p> <p>\u6bd4\u5982 \\(x \\wedge y\\) \u53ef\u4ee5\u53d8\u6210 \\(x \\cdot y\\)\uff0c\\(x \\vee y\\) \u53ef\u4ee5\u53d8\u6210 \\(1 - (1 - x)(1 - y)\\)\u3002\u8fd9\u6837\u7684\u8bdd\u6211\u4eec\u5c31\u53ef\u4ee5\u628a\u4e00\u4e2a\u903b\u8f91\u8868\u8fbe\u5f0f\u53d8\u6210\u4e00\u4e2a\u591a\u9879\u5f0f\uff0c\u800c\u4e14\u591a\u9879\u5f0f\u7684\u9636\u4e0d\u8d85\u8fc7 \\(3m\\)\uff0c\u5176\u4e2d \\(m\\) \u662f clause \u7684\u4e2a\u6570\u3002</p> <p>\u5de7\u5999\u7684\u662f\uff0c\u5982\u679c\u4e00\u4e2a\u903b\u8f91\u8868\u8fbe\u5f0f\u662f\u4e0d\u53ef\u80fd\u6ee1\u8db3\u7684\uff08UNSAT\uff09\uff0c\u5c31\u610f\u5473\u7740\u5b83\u5bf9\u5e94\u7684\u591a\u9879\u5f0f\u5728 \\(\\{0,1\\}^l\\) \u4e0a\u6c42\u548c\u4f9d\u7136\u662f 0\u3002\u8fd9\u6837\u7684\u8bdd\u5c31\u53d8\u6210\u4e86\u4e00\u4e2a sumcheck \u95ee\u9898\u3002</p> <p>\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u57df\uff0c\u6240\u4ee5\u5c31\u627e \\(\\mathbb{Z}_p\\) \u5373\u53ef\uff0c\u53ea\u8981 \\(2^l &lt; p &lt; 2^{l+1}\\)\u3002</p> <p>\u6700\u7ec8\u7684 protocol \u5982\u4e0b\uff1a ![[coNP2.png.png]]</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#3-gkr-protocol","title":"3 GKR Protocol","text":"<p>GKR is named by initials of three authors' names </p> <p>\u8fd9\u4e2a\u534f\u8bae\u4e00\u822c\u8868\u8ff0\u4e3a\u4e00\u4e2a\u8fd0\u7b97\u95e8\u9635\u5217\u6c42\u503c\u95ee\u9898\uff1a\u7ed9\u5b9a\u4e00\u7cfb\u5217 add \u95e8\u548c mul \u95e8\uff0c\u5206\u522b\u5bf9\u5e94\u52a0\u6cd5\u548c\u4e58\u6cd5\uff1b\u95e8\u662f\u5c42\u7ea7\u6392\u5217\u7684\uff0c\u7b2c \\(i\\) \u5c42\u95e8\u7684\u8f93\u5165\u6765\u81ea\u4e24\u4e2a \\(i+1\\) \u5c42\u7684\u95e8\uff0c\u8f93\u51fa\u8f93\u9001\u5230\u7b2c \\(i-1\\) \u5c42\u7684\u4e00\u4e2a\u95e8\u3002\u6700\u7ec8 prover \u60f3\u8981\u8bc1\u660e\u67d0\u4e2a\u8f93\u5165 \\(\\vec{x}\\) \u5f97\u5230\u7684\u8f93\u51fa\u4e3a \\(\\vec{y}\\)\u3002</p> <p>\u8fd9\u4e2a\u95ee\u9898\u4e5f\u662f\u6ca1\u6709 witness \u7684\uff0cverifier \u81ea\u8eab\u5c31\u53ef\u4ee5\u68c0\u9a8c\u8f93\u51fa\u662f\u4e0d\u662f\u5bf9\u7684\uff0c\u53ea\u9700\u8981\u81ea\u884c\u5e26\u5165\u8fd0\u7b97\u5373\u53ef\u3002\u8fd9\u91cc\u6211\u4eec\u60f3\u8981\u5b9e\u73b0\u7684\u662f prover \u53ef\u4ee5\u6bd4\u8f83\u9ad8\u6548\u7684\u8fd0\u884c\u540c\u65f6 verifier \u53ef\u4ee5\u8d85\u7ea7\u9ad8\u6548\u7684\u8fd0\u884c\uff08\u8f93\u5165\u89c4\u6a21\u7684\u5bf9\u6570\u7ea7\u522b\uff09\u3002\u8fd9\u4e2a\u542c\u8d77\u6765\u975e\u5e38\u4e0d\u53ef\u601d\u8bae\uff0c\u56e0\u4e3a\u4e00\u4e2a\u4eba\u5c45\u7136\u8fde\u95ee\u9898\u672c\u8eab\u90fd\u6ca1\u770b\u5168\u5c31\u80fd\u4ece\u522b\u4eba\u90a3\u91cc\u9a8c\u8bc1\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002Just like the PCP theorem states, it's possible to verify a proof of \\(n\\) bits without seeing all \\(n\\) bits but only \\(O(logn)\\) bits, with high probability to be correct.</p> <p>\u9996\u5148\u6211\u4eec\u4ecb\u7ecd\u4e00\u4e2a\u5f15\u7406\uff1a</p> <p>Schwarz Zippel Lemma</p> <p>\\(p \\in \\mathbb{F}[X_1, \\cdots X_l],\\;\\; \\mathcal{S} \\subset \\mathbb{F}\\)\uff0c\u90a3\u4e48\u968f\u673a\u9009\u4e00\u4e2a \\(\\vec{x}\\) \u4f7f\u5f97\u591a\u9879\u5f0f\u53d6\u503c\u4e3a 0 \u7684\u6982\u7387 \\(\\text{Pr} \\le \\frac{\\deg p}{|\\mathcal{S}|}\\)\u3002</p> <p>To utilize sumcheck protocol, we need to transform wire value function into summation form. To achieve this, let function \\(add\\) and \\(mul\\) be two indicator functions, which means if there is an add gate with input index \\(\\vec{i}, \\vec{j}\\) and output index \\(\\vec{c}\\), then function \\(add(i,j,c) = 1\\), otherwise the value is \\(0\\); the idea for \\(mul\\) is the same.</p> <p>In this way, for an output \\(w_i(c)\\), we can iterate all the possible combination of input positions \\(w_{i+1}(a), w_{i+1}(b)\\), which leads to a summation form:</p> \\[ w_i(\\vec{c}) = \\sum_{\\vec{a}, \\vec{b}} add(\\vec{a}, \\vec{b}, \\vec{c}) \\cdot (w_{i+1}(\\vec{a}) + w_{i+1}(\\vec{b})) + mul(\\vec{a}, \\vec{b}, \\vec{c}) \\cdot (w_{i+1}(\\vec{a}) \\cdot w_{i+1}(\\vec{b})) \\] <p>So the sumcheck form is:</p> \\[ w_i(\\vec{c}) = \\sum_{\\vec{a}, \\vec{b}} p(\\vec{a}, \\vec{b}) \\] <p>The protocol works in three steps.</p> <p>Firstly, the final output is a vector, but all the tools that we have are about polynomial, so we construct MLE for wire value \\(w\\) and for output within the instance, \\(y\\). \\(y\\) is known to both sides, so V chooses a random point \\(r_0\\), and our first claim is: \\(w(r_0) = y(r_0)\\), where \\(w, y\\) are MLEs.</p> <p>The second step is to use sumcheck protocol to prove our claim \\(w(r_i) = y(r_i) = v_{i}\\).</p> <p>Finally sumcheck protocol will reduce the claim to a single point value of \\(p\\). Say V chooses \\(\\vec{r_a}, \\vec{r_b}\\) as random point and P need to send back \\(A = w_{i+1}(\\vec{r_a})\\) and \\(B = w_{i+1}(\\vec{r_b})\\). Then V can calculate \\(p(\\vec{r_a}, \\vec{r_b})\\) by itself.</p> <p>Then how to make sure P sends the correct value for \\(A = w_{i+1}(\\vec{r_a})\\) and \\(B = w_{i+1}(\\vec{r_b})\\)? So we naturally get another two claims about next level's wire value. But if so, one claim reduced to two claims, two claims reduced to four claims, so on and so forth, there will be too many claims. Thus we need to reduce two claims to single claim.</p> <p>The third step, reduce two claims to one claim. \\(\\vec{a}\\) and \\(\\vec{b}\\) are two points, then \\(L(t) = \\vec{a} \\cdot t + \\vec{b} \\cdot (1 - t)\\) is a segment (or line) where \\(t\\) is the only variable. P sends \\(Q(t) = w_{i+1}(Q(t))\\) to V, because \\(Q(t)\\) only has one variable and of degree \\(l_{i+1}\\) so won't cost too much for communication. V checks if \\(Q(0) == A\\) and \\(Q(1) == B\\), if so V again choose a random value \\(t\\) in \\(\\mathbb{F}\\), the reduced claim is then: \\(w_{i+1}(L(t)) == Q(t) = v_{i+1}\\). Here the RHS, \\(Q(t)\\), and \\(L(t)\\) are calculated by both P and V, so we go back to second step with one more level down: \\(w_{i+1}(r_{i+1}) = v_{i+1}\\).</p> <p>Finally and finally, protocol will reduce claim to final final claim \\(w_{D}(r_D) = v_{D}\\), and this could be easily verified since \\(w_{D}\\) should be input and it's in instance so V already knows it.</p> <p>All parameters of full GKR protocol concluded as:</p> <p></p> <p>Completeness of this kind of protocol is actually obvious. If we have to give a rigorous proof, then we only need to show protocol reduces true claim to true claims and checks pass finally. Also note to use uniqueness of MLEs to show two MLEs of the same polynomial are identical. Identical MLEs must have equality even outside subset \\(H\\), so we can safely use the point in whole finite field.</p> <p>To show soundness, the idea is similar: we prove protocol reduces false claim to false claim or check fails, with high probability.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#iop-pcp-and-r1cs","title":"IOP, PCP and R1CS","text":"<p>R1CS looks strange, the formal definition of this relation is:</p> \\[ R_{R1CS} = \\Bigg\\{  \\big((A,B,C, x), w\\big): z = x || w, Az \\circ Bz = Cz \\Bigg\\} \\] <p>Here \\(\\circ\\) means element-wise product.</p> <p>This relation is NP-Complete and can be solved by holographic IOP with polynomial queries.</p> <p>R1CS protocol's prover complexity needs carefully analysing because \\(d + 1 &gt; |H|\\).</p> <p>The parameters concluded as:</p> <p></p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#polynomial-commitment","title":"Polynomial Commitment","text":"<p>Now we still need the help of oracle, which will answer the queries of V. Since all the queries are about polynomial evaluation, we can replace oracle with polynomial commitment scheme.</p> <p>Compared to normal message commitment scheme, polynomial commitment involves another IP Eval used to prove the commited function \\(f\\)'s evaluation \\(f(y) = z\\) is correct.</p> <p>Design such protocol for arbitrary polynomial would be difficult, but for MLE polynomial, we can represent \\(f\\) as \\(N\\) coefficients corresponding to different terms. A good property of MLE is that for any input, there will only be one term to be \\(1\\), while the others being \\(0\\).</p> <p>The proof size (aka communication complexity) also matters.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#side-notes-about-complexity-class","title":"Side Notes About Complexity Class","text":"<p>(a good reference complexity zoo https://complexityzoo.net/)</p> <p>IP = PSPACE (The class of decision problems solvable by a Turing machine in polynomial space).</p> <p>IOP, PCP = NEXP (solvable by nondet. Turing machine in \\(2^{n^{O(1)}}\\) time)</p> <p>This lecture note introduces PCP theorem and approximation gaps: https://courses.cs.washington.edu/courses/cse533/05au/pcp-theorem.pdf.</p> <p>We introduce IOP because the oracle part could be replaced by some special mechanism which will be discussed later, and IOP is easier to understand and use.</p> <p>Now let's focus on a new problem Rank 1 Constraint Systems (R1CS), which is a fundamentally important problem simply solvable with IOP protocol.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#nizk","title":"NIZK","text":"<p>NIZK without CRS (Common Reference String) is boring because it's in BPP complexity class. So we will only discuss NIZK with CRS.</p> <p>A non-interactive prof system for an NP relation R consists of three efficient algorithms (K, P, V) which are:</p> <ul> <li>the CRS generator K(1^\\lambda) \\rightarrow \\sigma. K may take |x| or x as input</li> <li>the prover P(\\sigma, x, w) \\rightarrow \\pi</li> <li>the verifier V(\\sigma, x, \\pi) \\rightarrow b</li> </ul> <p>The security definition of non-interactive proofs also have three properties:</p> <ul> <li>Completeness: \\(\\forall (x, w) \\in R, \\text{Pr}[b = 1] = 1\\). So if $(x, w) in relation, V always accepts.</li> <li>Soundness: \\(\\forall x \\notin L_R, \\text{Pr}[b = 1] \\approx 0\\). So if \\(x\\) not in language, V approxiamately always rejects.</li> <li>ZK: Exists two efficient simulators \\((S_1, S_2)\\) s.t. any adversary \\(A\\) producing \\((x, w) \\in R\\), we have </li> </ul> \\[ \\{(\\sigma, \\pi): \\sigma \\leftarrow K(1^\\lambda), (x, w) \\leftarrow A(\\sigma), \\pi \\leftarrow P(\\sigma, x, w)\\} = \\{(\\sigma, \\pi): \\sigma \\leftarrow S_1(1^\\lambda), (x, w) \\leftarrow A(\\sigma), \\pi \\leftarrow S_2(\\sigma, x, w)\\} \\] <p>The notebable difference is that we now have two simuilators for ZK, the one produces CRS, and the other produces P's message. The important note is that simulator may put a trapdoor into CRS, so V's access to CRS would be manipulated by simulator. Also these are single-theorem definitions, meaning that there are no security guarantees reusing CRS for many \\(x\\).</p> <p>Also, in definition of knowledge soundness, it needs two extractors, the one outputs CRS and an extraction trapdoor \\(\\xi\\); the other outputs witness. But the CRS has to be indistinguishable from normal CRS.</p> <p>With the help of trapdoor, simulation trapdoors let us produce proofs without knowing witness, breaking soundness; extraction trapdoors let us extract witness from proof, breaking ZK. So it's a big problem that how can we trust the CRS?</p> <p>We have several approaches to mitigate the risks.</p> <p>TBD</p> <p>Now let's look at an example of NIZK protocol: Boolean Circuit Protocol.</p> <p>A boolean circuits consists of several boolean gates, WLOG, we only uses NAND gates. The instance is the circuit description, and the witness is satisfying wire values.</p> <p>Consider function \\(f(a, b, c) = (a NAND b) == c\\), it evaluates to \\(1\\) iff \\(a + b + 2c - 2 \\in \\{0, 1\\}\\). So the proof idea is that we commit to each wire value, then prove each wire value is valid (i.e. $\\in {0, 1}), then prove \\(a + b + 2c - 2 \\in \\{0, 1\\}\\) for each gates.</p> <p>Since we cannot directly send wire value to V to check, so we need a commitment scheme with NI bit proofs.</p> <p>First we need to introduce Boneh-Goh-Nissim Cryptosystem.</p> <p>BGN protocol is based on two symmetric bilinear groups \\(\\mathbb{G}, \\mathbb{G}_T\\) of order \\(n = pq\\) where \\(p,q\\) are primes, and \\(n\\) is very large.</p> <p>Public parameter samples \\(B \\in \\mathbb{N} &lt;&lt; p\\), and all the original messages should be in \\([B]\\).</p> <p>Commit scheme is given a message \\(m \\in \\{0, \\cdots, B - 1\\}\\) and output \\(C = m \\cdot G + r \\cdot H\\), where \\(r\\) is sampled in \\(\\mathbb{Z}_n\\).</p> <p>This commit scheme has two modes: binding and hiding.</p> <p>For binding mode, we have \\(G \\leftarrow \\mathbb{G}, s \\leftarrow \\mathbb{Z}_n^*, H = ps \\cdot G\\).</p> <p>For hiding mode, we have \\(G \\leftarrow \\mathbb{G}, s \\leftarrow \\mathbb{Z}_n^*, H = s \\cdot G\\)</p> <p>The subgroup hiding assumption holds if binding setup and hiding setup are computationally indistinguishable.</p> <p>If we are using hiding mode, since \\(H\\) is equivalently sampled randomly, the commitment is perfectly hiding, since \\(C = m \\cdot G + r \\cdot H\\) is uniformly distributed. And by subgroup hiding assumption, biding mode still gives computational hiding.</p> <p>If we are using binding mode, consider \\(e(C, q \\cdot G) = m \\cdot e(G, G) + qr \\cdot e(H, G) = m \\cdot e(G, G) + pqrs \\cdot e(G, G) = m \\cdot e(G, G)\\). So if there is another message \\(m'\\) breaking binding property, i.e. \\(m \\cdot G + r \\cdot H = m = m' \\cdot G + r' \\cdot H\\), then by the same reasoning we have \\(e(C, q \\cdot G) = m' \\cdot e(G, G)\\). So we have \\((m - m') e(G, G) = 0\\), which means \\(m \\equiv m'\\). Since \\(B \\lll p\\), \\(m = m'\\).</p> <p>Again, by subgroup hiding assumption, hiding mode should also give computational binding.</p> <p>Now we need to figure out how to prove the committed value is \\(0\\) or \\(1\\). If \\(C\\) is the commitment of \\(m\\), then \\(C - G\\) would be the commitment of \\(m - 1\\). If \\(m = 0, 1\\), then \\(m(m-1) = 0\\).</p> <p>Pairing is analogous to \"multiplication\", so we can think about what \\(e(C, C - G)\\) would be.</p> \\[ e(C, C - G) = e(mG + rH, (m - 1)G + rH) = m(m-1) \\cdot e(G,G) + r(2m-1) \\cdot e(G, H) + r^2 \\cdot e(H, H) = r(2m-1) \\cdot e(G, H) + r^2 \\cdot e(H, H) \\] <p>So we can let V checks if \\(e(C, C - G) == r(2m-1) \\cdot e(G, H) + r^2 \\cdot e(H, H)\\). To achieve this, we can simply let P send \\(\\pi = r(2m - 1) \\cdot G + r^2 \\cdot H\\), and let V calculates \\(e(\\pi, H)\\).</p> <p>Completeness is obvious by rules of bilinearity and symmetry of group operations.</p> <p>Soundness is assured, we will show that \\(m\\) could be unquely determined and \\(m(m-1)=0\\) if using binding setup.</p> <p>Given the commitment \\(C\\), obviously there exists \\(m_* \\in \\mathbb{Z}_n\\) s.t. \\(C = m_* \\cdot G\\). Then let \\(m = m_* \\mod p\\) and let \\(r = \\frac{m_* - m}{ps} \\mod n\\), then with simple calculation we know \\(C = m \\cdot G + r \\cdot H\\). So we have shown \\(m\\) could be uniquely determined.</p> <p>Now we need to show such \\(m\\) satisfies \\(m(m-1) = 0\\).</p> <p>From V's view, what he knows is that \\(e(C, C - G) = e(\\pi, H)\\). Note that under binding setup, \\(q \\cdot e(G', H) = 0\\), we have \\(q \\cdot e(C, C - G) = q \\cdot e(\\pi, H) = 0\\), also we know \\(q \\cdot e(C, C - G) = q m(m-1) \\cdot e(G, G) + qr(2m-1) \\cdot e(G, H) + qr^2 \\cdot e(H, H)\\), thus \\(qm(m-1) \\cdot e(G, G) = 0\\), which means \\(m(m-1) = 0\\).</p> <p>It's almost knowledge sound, because as we have shown, we can extract unique \\(m\\) from \\(C\\) if we have trapdoor \\(p\\) or \\(q\\), but it's not completely knowledge sound, since we can't extract \\(r\\).</p> <p>It's not ZK, but it has proof uniqueness (i.e. witness indistinguishable). Proof uniqueness means there only exists one possible \\(\\pi\\) for each \\(C\\). So as long as for each \\(C\\), P in our whole protocol sends the correct \\(\\pi\\), it would be impossible to tell the difference between real \\(\\pi\\)'s distribution and our P's \\(\\pi\\) distribution since there is only one possible \\(\\pi\\).</p> <p>So, with the help of bif proof protocol, we can construct our complete protocol for boolean satisfiability.</p> <p>Firstly, for each gate's output, we use bit commitment to commit to these wire values.</p> <p>Then for each gate \\((i, j) \\rightarrow k\\), we need to prove \\(m_i + m_j + 2m_k - 2 \\in \\{0, 1\\}\\), so we commit to \\(m_i + m_j + 2m_k - 2 \\in \\{0, 1\\}\\), but we don't need to really compute the commitment, we can just use \\(C_i + C_j + 2C_k - 2G\\), and this could be calculated by V as well. Then for this gate, we compute corresponding \\(\\pi_{ijk} = r_{ijk}(2m_{ijk} - 1) \\cdot G + r_{ijk}^2 \\cdot H\\), where \\(r_{ijk}\\) is \\(r_i + r_j + 2 r_k\\) and \\(m_{ijk}\\) is \\(m_i + m_j + 2m_k - 2\\).</p> <p>What V checks is that each \\(i\\) satisfies \\(e(C_i, C_i - G) = e(\\pi_i, H)\\) and for each gate the similar check \\(e(C_{ijk}, C_{ijk} - G) = e(\\pi_{ijk}, H)\\).</p> <p>Completeness and knowledge soundness follow from completeness and soundness of bit proof system.</p> <p>Under hiding setup, we can easily construct two simulators, the one produces CRS with trapdoor \\(p\\) or \\(q\\), and the other one firstly assume each wire value is \\(0\\) and generates \\(C_i, \\pi_i\\). Then for each gate \\((i, j) \\rightarrow k\\), we open the commitment \\(C_k\\) as \\(C_k = 1 \\cdot G + r' \\cdot H\\), we can do such opening because under hiding setup, it's equivocable with key \\(s\\), which means it can be opened to any messages by letting \\(r' - r = \\frac{m - m'}{s} \\mod n\\). Doing such opening let us easily calculate \\(\\pi_{ijk}\\).</p> <p>ZK because each \\(C_i\\) is uniformly distributed, and \\(\\pi, \\pi_{ijk}\\) are uniquely determined by \\(C_i\\) so it's indistinguishable.</p>"},{"location":"posts/Crypto/Zero%20Knowledge%20Proof/#r1cs-as-polynomial-divisibility","title":"R1CS as polynomial divisibility","text":"<p>Let's define Lagrange polynomials on set \\(H\\) defined for \\(w \\in H\\), by:</p> \\[ L_{w, H}(x) = \\Pi_{w' \\in H \\setminus \\{w\\}} \\frac{x - w'}{w - w'} \\] <p>This polynomial has degree \\(|H| - 1\\), and it could be understood in this way: \\(L_{w, H}(x) = (x == w)\\).</p> <p>The vanishing polynomial on \\(H\\) is defined as \\(v_H(x) = \\Pi_{w \\in H}(x - w)\\).</p> <p>It's called vanishing polynomial because if \\(f(x) = 0, \\; \\forall x \\in H\\), then we have \\(v_H(x) | f(x)\\).</p>"},{"location":"posts/MATH/","title":"MATH","text":"<p>Trying to be a mathematician.</p>"},{"location":"posts/MATH/AMC/","title":"AMC","text":"<p>\u5982\u679c\u4e0d\u8bb0\u7b14\u8bb0\u597d\u50cf\u6ca1\u6709\u52a8\u529b\u590d\u4e60\u8003\u8bd5\uff0c\u800c\u4e14\u770b\u8fc7\u4e00\u904d\u90fd\u4f1a\u5fd8\u6389\uff0c\u5c1d\u8bd5\u5199\u4e00\u70b9\u6982\u8981\u3002</p> <p>\u5f53\u6211\u4eec\u60f3\u8981\u8bc1\u660e\u6570\u91cf\u7684\u4e0b\u754c\u7684\u65f6\u5019\uff0c\u4f8b\u5982 \\(S = \\{x_1, \\cdots, x_k\\}\\) \u53ef\u4ee5\u8003\u8651\uff1a</p> <ol> <li>\u5982\u679c\u6570\u91cf\u592a\u5c11\uff0c\u4e00\u5b9a\u53ef\u4ee5\u627e\u5230\u4e00\u4e2a\u6b21\u6570\u4e0d\u8d85\u8fc7 \\(d\\) \u7684 vanishing \u975e\u5e73\u51e1\u591a\u9879\u5f0f \\(p\\)\uff0c\u4e5f\u5c31\u662f\u8bf4 \\(p(x_i) = 0\\) \u4e14 \\(p \\neq 0\\).</li> <li>\u901a\u8fc7 \\(S\\) \u7684\u7279\u6b8a\u7ed3\u6784\u63a8\u51fa\u77db\u76fe\uff0c\u4e00\u822c\u53ef\u4ee5\u5c1d\u8bd5\u8bc1\u660e\u5982\u679c \\(S\\) \u4e2d\u7684\u5143\u7d20\u53d6\u503c\u90fd\u4e3a \\(0\\)\uff0c\u90a3\u4e48\u63a8\u51fa\u8fd8\u6709\u5176\u4ed6\u66f4\u591a\u5143\u7d20\u7684\u53d6\u503c\u4e5f\u4e3a \\(0\\)\uff0c\u6700\u7ec8\u63a8\u51fa\u591a\u9879\u5f0f\u4e3a\u96f6\u591a\u9879\u5f0f\uff0c\u6216\u8005\u662f\u5f15\u53d1\u5176\u4ed6\u77db\u76fe\u3002</li> </ol> <p>\u76ee\u524d\u8fd9\u4e2a\u65b9\u6cd5\u7528\u5728\u4e24\u4e2a\u5730\u65b9\uff1a</p> <ul> <li>\u6709\u9650\u57df Kakeya Set \u4e0d\u80fd\u592a\u5c0f</li> <li>\u4e00\u6761\u7ebf\u4e0a\u7684 joint \u4e0d\u80fd\u592a\u591a</li> </ul> <p>\u7b2c\u4e8c\u4e2a\u5e94\u7528\u5e76\u4e0d\u662f\u76f4\u63a5\u8bc1\u660e\u67d0\u4e2a\u6570\u91cf\u592a\u5c11\uff0c\u800c\u662f\u8bc1\u660e\u76f8\u5bf9\u6570\u91cf\u4e0d\u80fd\u592a\u5c11\uff0c\u4e5f\u5373\u53e6\u4e00\u4e2a\u7684\u6570\u91cf\u4e0d\u80fd\u592a\u591a\u3002</p> <p>\u6211\u4eec\u4ee5 joint \u6570\u91cf\u4e3a\u4f8b\uff1a</p> <p>Joint Number</p> <p>TBD</p>"},{"location":"posts/MATH/AMC/#restricted-set-intersection","title":"Restricted Set Intersection","text":"<p>\\(L\\)-interesting \u4e0d\u8981\u6c42\u96c6\u5408\u81ea\u8eab\u4e0d\u5728 \\(L\\) \u91cc\uff0c\u4f46\u662f \\(L\\) mod \\(p\\)-interesting \u8981\u6c42\u96c6\u5408\u81ea\u8eab\u5927\u5c0f\u4e0d\u5728 \\(L\\) \u91cc\u3002\u56e0\u4e3a\u8bc1\u660e\u8fc7\u7a0b\u4e2d\u4e0d\u53d6\u6a21\u7684\u8bdd\u53ef\u4ee5\u901a\u8fc7\u6392\u5e8f\u7684\u65b9\u6cd5\u4fdd\u8bc1 \\(f_j(v_j) \\neq 0\\)\uff0c\u800c\u6a21\u610f\u4e49\u4e0b\u5927\u5c0f\u987a\u5e8f\u6ca1\u6709\u610f\u4e49\uff0c\u53d6\u6a21\u540e\u5927\u7684\u53ef\u80fd\u53d8\u5c0f\u3002</p>"},{"location":"posts/MATH/AMC/#\u6982\u7387\u6784\u9020","title":"\u6982\u7387\u6784\u9020","text":"<p>\u6709\u7684\u65f6\u5019\u6211\u4eec\u4e0d\u80fd\u7ed9\u51fa explicit \u6784\u9020\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u6982\u7387\u8bc1\u660e\u8fd9\u6837\u7684\u4e1c\u897f\u662f\u5b58\u5728\u7684\u3002</p> <ul> <li> <p>\u8003\u8651\u4e00\u4e2a\u56fe\u7684\u6700\u5927\u5272\uff08\u628a\u56fe\u5206\u6210 A, B \u4e24\u4e2a\u90e8\u5206\uff0c\u6700\u5927\u5316\u4e24\u4e2a\u90e8\u5206\u4e92\u76f8\u8fde\u8fb9\u7684\u6570\u91cf\uff09\uff0c\u5982\u679c\u6211\u4eec\u72ec\u7acb\u540c\u5206\u5e03\u7684\u628a\u6bcf\u4e2a\u8282\u70b9\u7b49\u6982\u7387\u5206\u5230 A \u6216 B\uff0c\u90a3\u4e48\u6bcf\u6761\u8fb9\u5728\u5272\u91cc\u7684\u6982\u7387\u4e5f\u662f \u00bd\uff0c\u867d\u7136\u8fb9\u4eec\u5e76\u4e0d\u662f\u72ec\u7acb\u7684\uff0c\u4f46\u662f\u6c42\u671f\u671b\u4e0d\u5f71\u54cd\uff0c\u671f\u671b\u662f\u603b\u8fb9\u6570/2\uff0c\u6240\u4ee5\u81f3\u5c11\u5b58\u5728\u4e00\u4e2a\u5206\u5272\u65b9\u6848\u4f7f\u5f97\u5272\u8fb9\u6570\u91cf\u5927\u4e8e \u00bd \u3002</p> </li> <li></li> </ul>"},{"location":"posts/MATH/AbstractAlgebra/","title":"Abstract Algebra","text":"<p>For TA purpose</p>"},{"location":"posts/MATH/AbstractAlgebra/#basic-definitions","title":"Basic Definitions","text":"<p>Some important groups:</p> <ul> <li>Symmetric group \\(S_n\\): group of permutations on \\(n\\) symbols</li> <li>Alternating group \\(A_n\\): group of even permutations on \\(n\\) symbols</li> <li>Dihedral group \\(D_n\\): group of \\(n\\)-dim polygon rotations and reflections, containing \\(2n\\) elements.</li> <li>Quaternion group \\(Q_8\\): \\(\\langle \\bar{e}, i, j, k | \\bar{e}^2 = e, i^2 = j^2 = k^2 = ijk = \\bar{e} \\rangle\\)</li> </ul>"},{"location":"posts/MATH/AbstractAlgebra/#ring-theory","title":"Ring Theory","text":"<p>The study of ring theory arose from the study of Diophantine equations. A commutative ring is a generalization of integers, we can add, subtract, multiply, but not necessarily divide. We'll get some motivation from integers' concepts, e.g. prime, co-prime, etc..</p>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/","title":"Advanced Formal Language Theory","text":""},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#2-wfsa","title":"2 WFSA","text":"<p>Important definitions on section 3.2.3</p> <ul> <li>\\(\\Pi^n(i, j)\\): paths from \\(i\\) to \\(j\\) with exactly \\(n\\) edges</li> <li>\\(\\Pi^{(n)}(i, j)\\): paths from \\(i\\) to \\(j\\) with no more than \\(n\\) edges</li> </ul>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#21-intersection","title":"2.1 Intersection","text":""},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#211-non-epsilon-transition","title":"2.1.1 Non-epsilon transition","text":"<p>It's natural to think about intersections between WFSAs since we can construct more complicated ones by intersecting basic ones. If we have two WFSA \\(A_1, A_2\\) accepting languages \\(L_1, L_2\\), then the intersection language should be \\(L_1 \\bigcap L_2\\) and the weight of a string \\(y\\) is \\((A_1 \\bigcap A_2)(y) = A_1(y) \\otimes A_2(y)\\) (we assume string weight semiring is commutative, otherwise it's hard to achieve this intersection weight). </p> <p>Naively, we can correspond new WFSA's state to every pair of states in original WFSAs and there is a new transition from \\((q_1, q_2) \\rightarrow (q_3, q_4)\\) iff there are two transitions \\(q_1 \\rightarrow q_3\\) and \\(q_2 \\rightarrow q_4\\) originally. Naturally the transition weight should be the multiplication (that's why we assume commutativity).</p> <p>The correctness of such construction is obvious and left as excercise.</p>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#212-epsilon-transition","title":"2.1.2 epsilon transition","text":"<p>We still have a tough problem to deal with: epsilon-transition. It's easy to see that if \\(q_1\\) in \\(A_1\\) can accept \\(\\epsilon\\) and transit to \\(q_3\\) then there should be a valid transition in new WFSA like: \\((q_1, q_2) \\rightarrow (q_3, q_2)\\) even if there is no \\(\\epsilon\\) transition from \\(q_2\\) to itself.</p> <p>So we augment the original WFSA by adding new transitions called \\(\\epsilon_1, \\epsilon_2\\) transitions, which explicitly indicates which automaton is transiting over \\(\\epsilon\\). We replace \\(\\epsilon\\) in \\(A_1\\) with \\(\\epsilon_2\\) and replace \\(\\epsilon\\) in \\(A_2\\) with \\(\\epsilon_1\\). Then we further add self-loops in \\(A_1\\) with \\(\\epsilon_1\\) and in \\(A_2\\) iwth \\(\\epsilon_2\\). Given the modification we can thus safely use the non-epsilon version algorithm( really? ). We call the augmented automaton \\(\\tilde{A_1}, \\tilde{A_2}\\).</p> <p>However another problem arises. Using such augmentation the new intersection would have redundant paths (think about why). To remove redundant paths, we need to filter out these unqualified transitions. For example, \\((\\epsilon_1, \\epsilon_2)\\) transition should be forbidden; also, \\((\\epsilon_2, \\epsilon_2)\\) followed by \\((\\epsilon_1, \\epsilon_1)\\) should be forbidden becuase it's equivalent to a simpler form: \\((\\epsilon_2, \\epsilon_1)\\). Such filter can be implemented by a FSA with very few states, leave this as an excercise.</p>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#22-path-sum","title":"2.2 Path Sum","text":"<p>As the name itself suggests, path sum means the summation of path weights. Using different definition of \\(\\oplus\\), path sum corresponds to different tasks:</p> <ul> <li>Shortest path in the graph (tropical semiring)</li> <li>Sum of all paths (real semiring)</li> <li>Transforming WFSA into a weighted regular language (Kleene semiring)</li> </ul>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#221-acyclic-automaton-path-sum","title":"2.2.1 Acyclic automaton path sum","text":"<p>Obviously we can use topological sort to calculate the path sum with time complexity \\(O(n + m)\\).</p>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#222-closed-semiring","title":"2.2.2 Closed Semiring","text":"<p>There might be cycles in non-acyclic graph, which means there would be infinitely many paths in automaton. Then here is the problem: how do we calculate path sum, it might not even converge?</p> <p>To make sure such sum exists, we further require closed semiring to have a new unary operation Kleene star defined as:</p> <p>Kleene star</p> <p>Kleene star operation must satisfy following two axioms: 1. \\(x^* = 1 \\oplus x \\otimes x^*\\) 2. \\(x^* = 1 \\oplus x^* \\otimes x\\)</p> <p>Another definition that impies closed is k- closed.</p> <p>k-closed semiring</p> <p>A semiring is k-closed if for any \\(x\\), we have \\(\\bigoplus_{i=1}^{k + 1} x^{\\otimes_i} = \\bigoplus_{i=1}^{k} x^{\\otimes_i}\\), where \\(x^{\\otimes_i} = x \\otimes x \\otimes \\cdots x\\)</p> <p>So we can now safely compute path sum on closed semiring or k-closed semiring.</p>"},{"location":"posts/MATH/AdvancedFormalLanguageTheory/#223-semiring-lifted-to-matrix","title":"2.2.3 Semiring Lifted to Matrix","text":"<p>It's easy to check that we can naturally define corresponding matrix addition and multiplication over a semiring, and the matrix addition multiplication again form a new semiring.</p> <p>The Kleene star and matrix exponent on matrix semiring can be interpreted as complete path sum and path sum of a specific length.</p> <p>However, note that closeness of original semiring doesn't guarantee closeness of induced matrix semiring. So we further define a commutative semiring \\(W\\) is k-closed for graph \\(G\\) if for any cycle \\(\\pi\\) in \\(G\\) it holds that:</p> \\[ \\bigoplus_{i=0}^{k+1} w(\\pi) = \\bigoplus_{i=1}^{k} w(\\pi) \\] <p>We requires the semiring to be commutative because we want weight of a cycle not to depend on the node we enter in. A}</p>"},{"location":"posts/MATH/FFT/","title":"Introduction to FFT","text":""},{"location":"posts/MATH/FFT/#essence","title":"essence","text":""},{"location":"posts/MATH/Interesting%20Puzzle%20or%20Paper/","title":"Interesting Puzzle or Paper","text":"<p>Lattice Path Counting All in One</p>"},{"location":"posts/MATH/TDA/","title":"Topological Data Analysis","text":""},{"location":"posts/MATH/TDA/#0-topology-basics","title":"0 Topology basics","text":""},{"location":"posts/MATH/TDA/#01-open-sets","title":"0.1 Open Sets","text":"<p>Topology is basically about shapes and distances. We care about how different two shapes are and how far two points/elements are. Instead of using distance directly, we use open set as our basic building block.</p> <p>The topology can be seen as how we define open sets. A topological space is a set \\(X\\) together with a subset family \\(O\\) of which each subset is called an open set. \\(O\\) needs to satisfy three properties:</p> <ul> <li>\\(\\emptyset \\in O, X \\in O\\)</li> <li>\\(\\bigcap_{i}^n O_i \\in O\\), where \\(O_i \\in O\\).</li> <li>For every \\(S \\subset O\\), \\(\\bigcup S \\in O\\).</li> </ul> <p>We can conclude that open sets have closure under finite intersection and arbitrary union. This definition is consistent with the intuition we already got in middle school: infinite intersection of open intervals \\(\\bigcap_n^\\infty I_n\\), where \\(I_n = (0-\\frac{1}{n}, 1+\\frac{1}{n})\\) is a close interval \\([0,1]\\).</p>"},{"location":"posts/MATH/TDA/#02-base","title":"0.2 Base","text":"<p>Base \\(\\mathcal{B}\\) is a set of open sets such that for any open set \\(U\\) we can find a subset \\(\\mathcal{B}'\\) of \\(\\mathcal{B}\\) satisfying \\(U = \\bigcup_{B \\in \\mathcal{B}'}B\\), where \\(\\mathcal{B}' \\subset \\mathcal{B}\\).</p> <p>So the base is a subset of \\(O\\) but can construct every open set by appropriate union.</p> <p>For our old friend Euclidean space \\(R\\), we can give a base using rational number and open balls: \\(\\mathcal{B} = \\{B(x, 1/p) | x \\in \\mathbb{Q}, p \\in \\mathbb{N}_+\\}\\)</p>"},{"location":"posts/MATH/TDA/#03-closure-boundary-interior-and-limit-point","title":"0.3 Closure, Boundary, Interior, and Limit Point","text":"<p>Closure of a set \\(A\\) is the smallest (or the intersection of all) close sets that contains \\(A\\).</p> <p>Interior of a set \\(A\\) is the biggest (or the union of all) open sets that is contained in \\(A\\).</p> <p>Boundary is closure mod interiror.</p> <p>A point \\(p\\) is limit point of a set \\(A\\) if every open set that contains \\(p\\) intersects with \\(A\\). </p> <p>Note that this definition of boundary is different from what we will see of the simplex chain. However the definition of boundary of manifold is consistent with that of simplex chain. The boundary of a \\(d\\)-dim manifold is all the points whose neighbourhood is homeomorphic to half-ball \\(H^d\\), while interiror is these whose neighbourhood is homeomorphic to an open ball \\(B^d\\).</p>"},{"location":"posts/MATH/TDA/#1-homotopy-\u540c\u4f26","title":"1 Homotopy \u540c\u4f26","text":"<p>Homotopy is actually between two maps. A homotopy connecting between maps \\(g, h: X \\rightarrow Y\\) is a map \\(H: X \\times [0,1] \\rightarrow Y\\) such that \\(H(\\cdot, 0) = g, H(\\cdot, 1) = h\\). In this case \\(g\\) and \\(h\\) are called homotopic.</p> <p>However, usually we care about the equivalence relation induced by this notion of homotopy. Two sapces \\(X, Y\\) are homotopy equivalent if there exist maps \\(g: X \\rightarrow Y, h: Y \\rightarrow X\\) such that:</p> <ul> <li>\\(h \\circ g\\) is homotopic to \\(\\text{id}_X\\), and</li> <li>\\(g \\circ h\\) is homotopic to \\(\\text{id}_Y\\).</li> </ul> <p>Where \\(\\text{id}_X\\) is the identity map from \\(X\\) to \\(X\\).</p> <p>This definition is a bit abstract thus difficult to intuitively think about. Another usefull definition is deformation retract.</p> <p>Let \\(A \\subset X\\), a deformation retract of X onto \\(A\\) is a map \\(R: X \\times [0,1] \\rightarrow X\\) such that:</p> <ul> <li>\\(R(\\cdot, 0) = \\text{id}_X\\)</li> <li>\\(R(x,1) \\in A, \\forall x \\in X\\)</li> <li>\\(R(a,t) = a, \\forall a \\in A, \\forall t \\in [0,1]\\)</li> </ul> <p>If such deformation retractt of \\(X\\) onto \\(A\\) exists, we also say that \\(A\\) is a deformation retract of \\(X\\) and we can prove that \\(X\\) and \\(A\\) are homotopy equivalent.</p> <p>Another importance fact is that \\(X, Y\\) are homotopy equivalent iff there exists a space \\(Z\\) such that \\(X, Y\\) are deformation retracts of \\(Z\\).</p> <p>Deformation Retract and Homotopy Equivalent</p> <p>\\(X, Y\\) are homotopy equivalent iff there exists a space \\(Z\\) such that \\(X, Y\\) are deformation retracts of \\(Z\\).</p>"},{"location":"posts/MATH/TDA/#2-homology-\u540c\u8c03","title":"2 Homology \u540c\u8c03","text":"<p>\u4ee3\u6570\u62d3\u6251\u8bfe\u672c\u5565\u4e5f\u770b\u4e0d\u61c2</p> <p>\u5f00\u59cb\u6570\u6d1e</p> <p>Simplicial Map</p> <p>A map \\(f: K_1 \\rightarrow K_2\\) is called simplicial if it can be described by a vertex map \\(g: V(K_1) \\rightarrow V_(K_2)\\) s.t. \\(f(\\{v_0, \\cdots, v_k\\}) = \\{g(v_0), \\cdots, g(v_k)\\}\\) and every simplex in \\(K_1\\) is mapped to another simplex in \\(K_2\\).</p> <p>Note that it doesn't require a k-dim simplex is mapped to also a k-dim simplex.</p> <p>Now we introduce a very famous theorem which has lots of interesting applications.</p> <p>Brouwer Fixed Point Theorem</p> <p>Let \\(f: \\mathbb{B}^d \\rightarrow \\mathbb{B}^d\\) be continuous, then f has a fixed point, that is, \\(\\exists x \\in \\mathbb{B}^d \\implies f(x) = x\\).</p> <p>To prove this, we need some lemmas. The first one is:</p>"},{"location":"posts/MATH/TDA/#3-persistence","title":"3 Persistence","text":"<p>For a simplicial filtration, each time when we add a new \\(p\\)-dim simplex, it has to be either a creator or a destructor. A creator means it creates a new \\(p\\)-dim cycle (hole) and a destructor means it destroys a \\(p-1\\)-dim cycle (hole) since that cycle is the boundary of added simplex.</p> <p>But we need to notice that, here either a creator or a destructor creates or destroys not only a cycle, but instead a base cycle. So the rank increases or decreases by 1, which means the number different cycles actually doubles or shrinks to a half.</p> <p>This definition agrees with the definition of betti number and persistence diagram.</p> <p>It's easy to prove that a new simplex is either a creator or a destructor. So we can pair them in this way: each time we pair a destructor with the youngest still unpaired creator within the cycle it destroys. This algorithm avoids the ambiguity of exactly which cycle is destroyed, when there are two cycles in fact merged into one cycle after adding a destructor.</p> <p>What if all the creators within the destroyed cycle  have been paired? Then for each creator \\(\\rho\\), we consider its paired destructor \\(\\tau\\), we replace \\(\\rho\\) by  \\(\\rho + \\partial \\tau\\). Then we get a new set of candidates. Repeat the previous precedure until we find an unpaired creator or if there is no more creator then the added cycle could not be a destructor (WHY?).</p> <p>Then for each pair of paired simplexes, we draw a point on the persistence diagram, with the coordinate \\((f(\\rho), f(\\tau))\\), where \\(\\rho\\) is the creator, \\(\\tau\\) is the destructor, \\(f(\\cdot)\\) is timestamp function. For technical reason, we add infinitely many points on diagonal.</p> <p>Given persistence diagrams, it's natural to think about how similar two diagrams are or can we define a distance metric for diagrams.</p> <p>Here we use bottleneck distance, which is defined as:</p> \\[ d_b(Dgm_p(F), Dgm_p(G)) = \\inf_{\\pi \\in \\Pi} \\sup_{x \\in Dgm_p(F)}  || x - \\pi(x) ||_\\infty \\] <p>We can prove if the diagrams have finite off-diagonal points then bottleneck distance is a metric.</p> <p>We want to prove the stability of simplicial filtrations.</p> <p>Stability for Simplicial Filtration Theorem</p> <p>Let \\(f, g: K \\rightarrow \\mathbb{R}\\)  be simplex-wise monotone functions. Then \\(d_b(Dgm_p(F_f), Dgm_p(F_g)) \\le |f-g|_\\infty\\).</p> <p>This theorem states that if we only change a little on the filtration level function \\(f\\), the diagram also changes a little, which implies stability.</p> <p>The idea behind the proof is that we construct a new function \\(v(x, t) = t f(x) + (1 - t) g(x)\\), then we draw diagrams for each timestamp \\(t\\), which forms a vineyard. The slope of each vine is at most \\(|| f - g ||_\\infty\\).</p> <p>We can also generalize this to any triangulable topological space, which will be proved later using stability with respect to interleaving distance.</p>"},{"location":"posts/MATH/TDA/#wasserstein-distance","title":"Wasserstein Distance","text":"<p>Bottleneck distance cannot capture the number of \"mismatching\". Say if there is a relatively deviated point, then no matter how many other points, which are closer to diagonal, are added, the bottleneck distance won't change. We always only look at the longest edge in matching.</p> <p>To avoid this problem we can use Wasserstein distance.</p> \\[  d_{W, q}(Dgm_p(F), Dgm_p(G)) = \\big[ \\inf_{\\pi \\in \\Pi} (\\sum_{x \\in Dgm_p(F)} (||x - \\pi(x)||_\\infty)^q) \\big]^{1/q} \\] <p>Note that when \\(q = \\infty\\), \\(d_{W, \\infty} = d_b\\).</p> <p>We cannot guarantee stability of Wasserstein distance. Counterexamples can be found for simplicial complex and topological spaces. But if we restrict the function to be Lipschitz, we do have stability for Wasserstein distance.</p>"},{"location":"posts/MATH/TDA/#interleaving-distance","title":"Interleaving Distance","text":"<p>Interleaving distance of two function levelset induced filtration is at most \\(||f - g||_infty\\).</p> <p>And using the theorem of stability with respect to interleaving distance, we have \\(d_b(Dgm(U), Dgm(V)) = d_I (U, V)\\). Note that here \\(U, V\\) are homology group based persistence module, but not filtration. </p> <p>It's also obvious that the interleaving distance of homology goup is no greater than the interleaving distance of corresponding interleaving distance, because we can always construct a map for homology group based on the map for its corresponding simplexes.</p> <p>So:</p> <p>\\(d_b(Dgm(H_p U), Dgm(H_p V)) = d_I (H_p U, H_p V) \\le d_I (U, V) \\le || f - g ||_\\infty\\).</p> <p>A reason why we consider interleaving distance is that previously we only pay attention to stability of one space or simplicial complex with different filtrations induced by levelset of some function. However in real application, we only have point cloud, which may vary in size. Interleaving distance still gives some promise about stability even with different sizes.</p> <p>Based on this intuition, we consider Hausdoff distance.</p>"},{"location":"posts/MATH/TDA/#5-reeb-graph-and-mapper","title":"5 Reeb Graph and Mapper","text":""},{"location":"posts/MATH/TDA/#reeb-graph","title":"Reeb Graph","text":"<p>The idea of Reeb graph is that we extract the 1-dimensional information out of the space \\(X\\) using a function \\(f: X \\rightarrow \\mathbb{R}\\).</p> <p>Reeb Graph</p> <p>\\(X\\) is a topological space, and \\(f\\) is a function from \\(X\\) to \\(\\mathbb{R}\\). Two points \\(x, y \\in X\\) are called equivalent (\\(x \\sim y\\)) iff \\(f(x) = f(y) = \\alpha\\) and \\(x, y\\) are in the same path-connected component of \\(f^{-1}(\\alpha)\\). The Reeb graph \\(R_f\\) is the quotient space \\(X / \\sim\\).</p> <p>To exclude the weird and meaningless cases, we only consider the situations where there are always connected components and homology groups of levelsets only change at finitely many critical values.</p> <p>Reeb graph by definition is a topological space (it's a quotient space), we call it a graph because it's 1-dimensional. To get a real graph we need a discretization.</p> <p>It's very natural to consider the number of \"neighbours\" that a point has. Let \\(u\\) be the number of neighbours in the direction of \\(f\\)-value increases; \\(l\\) be the number of neighbours in the direction of decreasing. Then if \\(u = l = 1\\), this point is a regular one; while in the other cases it's a critical point which should be displaced as a distinct vertex in the graph.</p>"},{"location":"posts/MATH/TDA/#from-topospace-to-mathbbr","title":"From Topospace to \\(\\mathbb{R}\\)","text":"<p>Mapper is an approximation of Reeb graph. Instead of preimage of single point, we now consider preimage of intervals.</p> <p>Again we need a function \\(f: X \\rightarrow \\mathbb{R}\\), and for an open cover \\(\\{U_\\alpha\\}\\) of \\(\\mathbb{R}\\), we consider the path-connected components of preimages of each interval \\(U_\\alpha\\) and further compute the nerve of this family, i.e. \\(f^{-1}(U_\\alpha) = \\bigcup_{\\beta} V_\\beta\\), let \\(f^*(U) = \\{V_\\beta\\}\\), and finally \\(N(f^*(U))\\).</p> <p>If we take sufficiently appropriate function \\(f\\) and sufficiently appropriate cover \\(\\{U\\}\\) then \\(N(f^*(U))\\) is isomorphic to \\(R_f\\).</p>"},{"location":"posts/MATH/TDA/#topological-mapper","title":"Topological Mapper","text":"<p>Previous definition only considers maps to \\(\\mathbb{R}\\), now we generalize to arbitary space.</p> <p>Def: well-behaved</p> <p>TBD</p> <p>Def: Mapper</p> <p>Let \\(f: X \\rightarrow Z\\) be well-behaved, and \\(U\\) be a finite open cover of \\(Z\\). Then the Mapper is defined as \\(M(U,f) = N(f^*(U))\\).</p>"},{"location":"posts/MATH/Combinatorial_Maths/Burnside%27s_Lemma/","title":"Burnside's_Lemma","text":"<p>This is a fantastic lecture note about Burnside's Lemma and Ploya Theorem: https://math.mit.edu/~apost/courses/18.204_2018/Jenny_Jin_paper.pdf</p> <p>I would write down my learning notes for myself so it would be in Chinese.</p>"},{"location":"posts/MATH/Combinatorial_Maths/Burnside%27s_Lemma/#1-\u80cc\u666f","title":"1 \u80cc\u666f","text":"<p>\u4e0d\u7ba1\u662f\u7b97\u6cd5\u9898\u91cc\u8fd8\u662f\u751f\u6d3b\u4e2d\u5176\u5b9e\u8fd9\u662f\u4e00\u4e2a\u6bd4\u8f83\u5e38\u89c1\u7684\u95ee\u9898\uff1a\u67d0\u67d0\u573a\u666f\u4e0b\u672c\u8d28\u4e0d\u540c\u7684\u67d3\u8272\u6709\u591a\u5c11\u79cd\uff1f \u65e2\u7136\u5f3a\u8c03\u4e86\u672c\u8d28\u4e0d\u540c\uff0c\u5c31\u662f\u8bf4\u5728\u4e00\u4e9b\u53d8\u6362\u4e0b\u6211\u4eec\u8ba4\u4e3a\u67d0\u4e24\u4e2a\u67d3\u8272\u65b9\u6848\u662f\u76f8\u540c\u7684\u3002\u6bd4\u5982\u4e00\u4e2a\u5f88\u81ea\u7136\u7684\u95ee\u9898\uff1a\u6211\u4eec\u60f3\u7ed9\u4e00\u4e2a\u6b63\u65b9\u4f53\u516d\u4e2a\u9762\u67d3\u4e0a\u7ea2\u84dd\u7eff\u4e09\u79cd\u989c\u8272\u3002\u5982\u679c\u6211\u4eec\u8ba4\u4e3a\u6b63\u65b9\u4f53\u6bcf\u4e2a\u9762\u90fd\u662f\u76f8\u540c\u7684\uff08\u6bd4\u5982\u9ab0\u5b50\u516d\u4e2a\u9762\u5c31\u4e0d\u540c\uff0c\u56e0\u4e3a\u5b83\u6709\u6807\u53f7\uff09\uff0c\u90a3\u4e48\u663e\u7136\u65b9\u6848\u6570\u5e76\u4e0d\u662f\u7b80\u5355\u7684 \\(6^3\\)\uff0c\u56e0\u4e3a\u6709\u5f88\u591a\u60c5\u51b5\u4e4b\u95f4\u662f\u672c\u8d28\u76f8\u540c\u7684\uff0c\u6211\u4eec\u628a\u6b63\u65b9\u4f53\u8f6c\u4e00\u8f6c\u5b83\u4eec\u5c31\u4e00\u6837\u4e86\u3002\u4e00\u901a\u601d\u8003\u4e4b\u540e\u4e0d\u96be\u53d1\u73b0\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u95ee\u9898\u3002 \u9664\u4e86\u8fd9\u4e2a\u4e4b\u5916\u8fd8\u80fd\u60f3\u5230\u5f88\u591a\u7c7b\u4f3c\u7684\u95ee\u9898\uff0c\u5b83\u4eec\u53ef\u80fd\u6709\u4e0d\u540c\u7684\u53d8\u6362\u65b9\u5f0f\uff0c\u5982\u679c\u6bcf\u6b21\u90fd\u8981\u91cd\u65b0\u53bb\u60f3\u65b9\u6cd5\u5b9e\u5728\u662f\u592a\u8822\u4e86\u3002\u6211\u4eec\u5e0c\u671b\u80fd\u591f\u62bd\u8c61\u51fa\u4e00\u5957\u4e1c\u897f\u6765\u5e2e\u52a9\u6211\u4eec\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p>"},{"location":"posts/MATH/Combinatorial_Maths/Burnside%27s_Lemma/#2-\u57fa\u672c\u5b9a\u4e49","title":"2 \u57fa\u672c\u5b9a\u4e49","text":"<p>\u9996\u5148\u6211\u4eec\u6709\u4e24\u4e2a\u96c6\u5408\uff0c\u4e00\u4e2a\u662f\u6211\u4eec\u7684\u65b9\u6848\u96c6\u5408 \\(X\\)\uff0c\u5b83\u4ee3\u8868\u7684\u662f\u6240\u6709\u7684\u67d3\u8272\u65b9\u6848\uff08\u4e0d\u8003\u8651\u662f\u5426\u672c\u8d28\u4e0d\u540c\uff09\uff1b\u53e6\u4e00\u4e2a\u662f\u6211\u4eec\u7684\u64cd\u4f5c\u96c6\u5408 \\(G\\)\uff0c\u5b83\u4ee3\u8868\u7684\u662f\u6240\u6709\u53ef\u4ee5\u65bd\u52a0\u7684\u64cd\u4f5c\uff08\u5177\u4f53\u6765\u8bf4\u53ef\u80fd\u662f\u65cb\u8f6c\u3001\u5bf9\u79f0\u7b49\uff09\u3002\u64cd\u4f5c\u5929\u7136\u5c31\u5177\u6709 \u590d\u5408 \u8fd9\u4e2a\u7279\u6027\uff0c\u4e5f\u5c31\u662f\u8bf4\u6211\u53ef\u4ee5\u5148\u8fdb\u884c \\(A\\) \u64cd\u4f5c\uff0c\u7136\u540e\u518d\u8fdb\u884c \\(B\\) \u64cd\u4f5c\u3002\u4ece\u6211\u4eec\u6734\u7d20\u7684\u60c5\u611f\u51fa\u53d1\uff0c\u663e\u7136\u8fd9\u4e2a\u64cd\u4f5c\u7684 \u590d\u5408 \u8fd0\u7b97\u5e94\u8be5\u662f\u5c01\u95ed\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\u5148\u8fdb\u884c \\(A\\) \u64cd\u4f5c\u518d\u8fdb\u884c \\(B\\) \u64cd\u4f5c\uff0c\u5e94\u8be5\u8981\u7b49\u4ef7\u4e0e \\(G\\) \u4e2d\u7684\u67d0\u4e2a\u5143\u7d20\u624d\u884c\u3002(\u540e\u9762\u6362\u6210\u82f1\u6587\u6807\u70b9\u770b\u770b\u6548\u679c) \u6211\u4eec\u5f88\u81ea\u7136\u7684\u8ba4\u4e3a\u8fd9\u4e2a\u64cd\u4f5c\u96c6\u5408 \\(G\\) \u548c\u64cd\u4f5c\u7684 \u590d\u5408 \u8fd0\u7b97\u5e94\u8be5\u6784\u6210\u4e00\u4e2a\u7fa4. \u5f53\u7136\u5e76\u4e0d\u662f\u6240\u6709\u7684\u64cd\u4f5c\u96c6\u5408\u90fd\u80fd\u6784\u6210\u7fa4. \u6bd4\u5982\u6211\u4eec\u6709\u4e00\u4e2a\u78be\u788e\u64cd\u4f5c, \u4f46\u662f\u6ca1\u6709\u590d\u539f\u64cd\u4f5c, \u90a3\u5c31\u4e0d\u80fd\u6784\u6210\u7fa4. \u6211\u4eec\u4e0b\u9762\u8ba8\u8bba\u7684\u90fd\u662f\u80fd\u591f\u6784\u6210\u7fa4\u7684\u573a\u666f.</p>"},{"location":"posts/MATH/Complexity/Ladner%20Theorem/","title":"Ladner Theorem","text":"<p>If P \\(\\neq\\) NP, then there must be a gap between NP and NPC.</p> <p> </p>"},{"location":"posts/MATH/Linear_Algebra/Chpater%203/","title":"Chpater 3","text":""},{"location":"posts/MATH/Linear_Algebra/Chpater%203/#important-theorems","title":"important theorems","text":"<ol> <li>A sum is direct sum if and only if the sum of dimensions equals to the dimension of the sum.</li> <li></li> </ol>"},{"location":"posts/MATH/MISCS/Continuous%2C%20all%20direction%20derivative%20exist%20but%20not%20differentiable/","title":"Continuous, all direction derivative exist but not differentiable","text":""},{"location":"posts/MATH/MISCS/Continuous%2C%20all%20direction%20derivative%20exist%20but%20not%20differentiable/#\u7b80\u5355\u7684\u4f8b\u5b50","title":"\u7b80\u5355\u7684\u4f8b\u5b50","text":"\\[ f(x,y) = \\left\\{                \\begin{array}{**lr**}                \\frac{2xy^3}{x^2 + y^4}, &amp; x^2 + y^2 \\ne 0 \\\\              0,                       &amp; x^2 + y^2 = 0              \\end{array}   \\right.   \\] <p>\u8fd9\u4e2a\u51fd\u6570\u663e\u7136\u662f\u8fde\u7eed\u7684 (\u975e 0 \u5904\u521d\u7b49\u6240\u4ee5\u5fc5\u7136\u8fde\u7eed, 0 \u5904\u7528\u5747\u503c\u4e0d\u7b49\u5f0f\u5bb9\u6613\u8bc1\u660e\u662f\u8fde\u7eed\u7684). \u540c\u65f6\u65b9\u5411\u5bfc\u6570\u4e5f\u662f\u5b58\u5728\u7684, \u4f46\u662f\u5b83\u4e0d\u53ef\u5fae, \u53ea\u8981\u6211\u4eec\u903c\u8fd1\u65b9\u5f0f\u53d6 \\(x = y^2\\) \u5373\u53ef. </p> <p>\u8fd9\u8bf4\u660e\u4efb\u610f\u65b9\u5411\u65b9\u5411\u5bfc\u6570\u5b58\u5728, \u5e76\u4e0d\u80fd\u8bf4\u660e\u4efb\u610f\u65b9\u5f0f\u903c\u8fd1\u5bfc\u6570\u5b58\u5728.</p>"},{"location":"posts/MISC/eride_worktips/","title":"Tips on Random Stuff Used When Working at Eride","text":"<p>Best practice of user/role management for Postgresql: https://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/#:%7E:text=Users%2C%20groups%2C%20and%20roles%20are,to%20log%20in%20by%20default.&amp;text=The%20roles%20are%20used%20only,grant%20them%20all%20the%20permissions</p> <p>Postgresql top useful commands: https://hasura.io/blog/top-psql-commands-and-flags-you-need-to-know-postgresql</p>"},{"location":"posts/MISC/latex_tips/","title":"Tips for LaTex","text":""},{"location":"posts/MISC/latex_tips/#enumerate","title":"enumerate","text":""},{"location":"posts/MISC/latex_tips/#enumerate-using-alphabet","title":"enumerate using alphabet","text":"<pre><code>\\begin{enumerate}[label=(\\alph*)]\n</code></pre>"},{"location":"posts/MISC/latex_tips/#equations","title":"equations","text":""},{"location":"posts/MISC/latex_tips/#case","title":"case","text":"<pre><code>\\begin{equation}\n  D_{it} =\n    \\begin{cases}\n      1 &amp; \\text{if bank $i$ issues ABs at time $t$}\\\\\n      2 &amp; \\text{if bank $i$ issues CBs at time $t$}\\\\\n      0 &amp; \\text{otherwise}\n    \\end{cases}       \n\\end{equation}\n</code></pre>"},{"location":"posts/MISC/latex_tips/#font","title":"font","text":"<p>Normal font: \\(\\(RQSZ\\)\\)</p> <p>Calligraphy font: \\(\\(\\mathcal{RQSZ}\\)\\)</p> <p>Fraktur font: \\(\\(\\mathfrak{RQSZ}\\)\\)</p> <p>Bold font: \\(\\(\\mathbb{RQSZ}\\)\\)</p>"},{"location":"posts/MISC/working_setting/","title":"working_setting","text":""},{"location":"posts/MISC/working_setting/#terminal","title":"Terminal","text":"<p>Windows: Windows Terminal Preview Mac: ITerm2</p>"},{"location":"posts/MISC/working_setting/#shell","title":"Shell","text":"<p>Windows: WSL + Oh-My-Bash Mac: Oh-My-Zsh Oh-My-Tmux</p>"},{"location":"posts/MISC/working_setting/#editor","title":"Editor","text":"<p>VSCode + Vim keymap</p>"},{"location":"posts/MISC/working_setting/#personal-notes","title":"Personal Notes","text":"<p>mkdocs-material: https://squidfunk.github.io/mkdocs-material/reference/code-blocks/</p> <pre><code>pip install mkdocs\npip install mkdocs-material\npip install pymdown-extensions\n\nmkdocs serve &amp;\n</code></pre>"},{"location":"posts/MISC/working_setting/#version-control--code-storage","title":"Version Control &amp; Code Storage","text":"<p>lazygit: https://github.com/jesseduffield/lazygit</p> github ssh<pre><code>echo \"IdentityFile path/to/sshkey\" &gt;&gt; ~/.ssh/config # add key\nssh -T git@github.com # test ssh connection\n</code></pre>"}]}